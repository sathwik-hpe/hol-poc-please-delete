<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI/ML Learning Hub - Master LLMs & Agentic AI</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f7fa;
        }

        .container {
            display: flex;
            min-height: 100vh;
        }

        /* Sidebar Navigation */
        .sidebar {
            width: 300px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px 20px;
            position: fixed;
            height: 100vh;
            overflow-y: auto;
            box-shadow: 2px 0 10px rgba(0,0,0,0.1);
        }

        .sidebar h1 {
            font-size: 24px;
            margin-bottom: 10px;
            font-weight: 700;
        }

        .sidebar p {
            font-size: 14px;
            opacity: 0.9;
            margin-bottom: 30px;
        }

        .nav-item {
            display: block;
            padding: 12px 15px;
            margin: 8px 0;
            color: white;
            text-decoration: none;
            border-radius: 8px;
            transition: all 0.3s ease;
            font-size: 14px;
            background: rgba(255,255,255,0.1);
        }

        .nav-item:hover {
            background: rgba(255,255,255,0.2);
            transform: translateX(5px);
        }

        .nav-item.active {
            background: rgba(255,255,255,0.3);
            font-weight: 600;
        }

        /* Main Content */
        .main-content {
            margin-left: 300px;
            flex: 1;
            padding: 40px 60px;
            max-width: 1200px;
        }

        .module-content {
            display: none;
            background: white;
            padding: 50px;
            border-radius: 15px;
            box-shadow: 0 2px 20px rgba(0,0,0,0.08);
            animation: fadeIn 0.5s ease;
        }

        .module-content.active {
            display: block;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        /* Typography */
        h1 {
            font-size: 42px;
            color: #2d3748;
            margin-bottom: 20px;
            font-weight: 800;
            line-height: 1.2;
        }

        h2 {
            font-size: 32px;
            color: #4a5568;
            margin: 40px 0 20px 0;
            font-weight: 700;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
        }

        h3 {
            font-size: 24px;
            color: #667eea;
            margin: 30px 0 15px 0;
            font-weight: 600;
        }

        p {
            margin: 15px 0;
            color: #4a5568;
            font-size: 16px;
            line-height: 1.8;
        }

        /* Lists */
        ul {
            margin: 20px 0;
            padding-left: 0;
            list-style: none;
        }

        li {
            margin: 12px 0;
            padding-left: 30px;
            position: relative;
            color: #4a5568;
            line-height: 1.8;
        }

        li:before {
            content: "‚ñ∂";
            position: absolute;
            left: 0;
            color: #667eea;
            font-size: 12px;
        }

        /* Code Blocks */
        pre {
            background: #2d3748;
            color: #e2e8f0;
            padding: 25px;
            border-radius: 10px;
            overflow-x: auto;
            margin: 25px 0;
            border-left: 4px solid #667eea;
        }

        code {
            font-family: 'Monaco', 'Menlo', 'Consolas', monospace;
            font-size: 14px;
            line-height: 1.6;
        }

        p code {
            background: #f7fafc;
            color: #667eea;
            padding: 3px 8px;
            border-radius: 4px;
            font-size: 14px;
        }

        strong {
            color: #2d3748;
            font-weight: 600;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .sidebar {
                width: 100%;
                position: relative;
                height: auto;
            }

            .main-content {
                margin-left: 0;
                padding: 20px;
            }

            .container {
                flex-direction: column;
            }
        }

        /* Progress Indicator */
        .progress-bar {
            position: fixed;
            top: 0;
            left: 300px;
            right: 0;
            height: 4px;
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            transform-origin: left;
            z-index: 1000;
        }

        /* Scroll to Top */
        .scroll-top {
            position: fixed;
            bottom: 30px;
            right: 30px;
            width: 50px;
            height: 50px;
            background: #667eea;
            color: white;
            border: none;
            border-radius: 50%;
            font-size: 24px;
            cursor: pointer;
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
            transition: all 0.3s ease;
            opacity: 0;
            pointer-events: none;
        }

        .scroll-top.visible {
            opacity: 1;
            pointer-events: all;
        }

        .scroll-top:hover {
            transform: scale(1.1);
            background: #764ba2;
        }
    </style>
</head>
<body>
    <div class="progress-bar" id="progressBar"></div>
    
    <div class="container">
        <aside class="sidebar">
            <h1>ü§ñ AI/ML Learning Hub</h1>
            <p>Master LLMs, Agents, and Production ML</p>
            <nav>
                <a href="#module-0" class="nav-item" onclick="showModule('module-0')">00 LEARNING PATH</a>
<a href="#module-1" class="nav-item" onclick="showModule('module-1')">01 LLM Fundamentals</a>
<a href="#module-2" class="nav-item" onclick="showModule('module-2')">02 Tokens Context Embeddings</a>
<a href="#module-3" class="nav-item" onclick="showModule('module-3')">03 Prompting Temperature</a>
<a href="#module-4" class="nav-item" onclick="showModule('module-4')">04 CoT vs ReAct</a>
<a href="#module-5" class="nav-item" onclick="showModule('module-5')">05 Tool Calling Function Calling</a>
<a href="#module-6" class="nav-item" onclick="showModule('module-6')">06 Agent Reasoning Loops</a>
<a href="#module-7" class="nav-item" onclick="showModule('module-7')">07 LangChain Components</a>
<a href="#module-8" class="nav-item" onclick="showModule('module-8')">08 Memory Context Management</a>
<a href="#module-9" class="nav-item" onclick="showModule('module-9')">09 Output Parsers Structured Outputs</a>
<a href="#module-10" class="nav-item" onclick="showModule('module-10')">10 RAG Vector Databases</a>
<a href="#module-11" class="nav-item" onclick="showModule('module-11')">11 Error Handling Production</a>
<a href="#module-12" class="nav-item" onclick="showModule('module-12')">12 ML System Design Best Practices</a>
<a href="#module-13" class="nav-item" onclick="showModule('module-13')">13 Hands On Exercises</a>
<a href="#module-14" class="nav-item" onclick="showModule('module-14')">INTERVIEW DEMO PREP</a>
<a href="#module-15" class="nav-item" onclick="showModule('module-15')">QUICK REFERENCE</a>
            </nav>
        </aside>

        <main class="main-content">
            
    <div class="module-content" id="module-0">
        <h1>AI/ML Learning Path for K8s Agent Project</h1>
<h2>Your Journey from Zero to Confident ML Engineer</h2>
<strong>Goal:</strong> Master AI concepts needed to build and explain a production-grade K8s troubleshooting agent
<strong>Timeline:</strong> 8-12 hours of focused study before starting project
<strong>Outcome:</strong> Be able to confidently explain every technical decision and compare alternatives
<p>---</p>
<h2>üìö <strong>Learning Modules</strong></h2>
<h3><strong>Phase 1: Core AI Fundamentals</strong> (2-3 hours)</h3>
<ul><li>Module 1: What are LLMs? (30 min)</li>
<li>Module 2: Tokens, Context Windows, Embeddings (45 min)</li>
<li>Module 3: Prompting & Temperature (45 min)</li>
<li><strong>Checkpoint:</strong> Can you explain how ChatGPT works to a 5-year old?</li>
<h3><strong>Phase 2: Agentic AI</strong> (2-3 hours)</h3>
<li>Module 4: Chain-of-Thought vs ReAct (45 min)</li>
<li>Module 5: Tool Calling & Function Calling (1 hour)</li>
<li>Module 6: Agent Reasoning Loops (45 min)</li>
<li><strong>Checkpoint:</strong> Can you draw the ReAct pattern from memory?</li>
<h3><strong>Phase 3: LangChain Framework</strong> (2 hours)</h3>
<li>Module 7: LangChain Components (45 min)</li>
<li>Module 8: Memory Types (45 min)</li>
<li>Module 9: Output Parsers (30 min)</li>
<li><strong>Checkpoint:</strong> Can you explain when to use each memory type?</li>
<h3><strong>Phase 4: Production ML</strong> (2 hours)</h3>
<li>Module 10: RAG Architecture (45 min)</li>
<li>Module 11: Error Handling & Fallbacks (45 min)</li>
<li>Module 12: ML System Design (30 min)</li>
<li><strong>Checkpoint:</strong> Can you critique a production ML system?</li>
<h3><strong>Phase 5: Hands-On Practice</strong> (2-3 hours)</h3>
<li>Exercise 1: Call an LLM API (30 min)</li>
<li>Exercise 2: Build a simple tool (30 min)</li>
<li>Exercise 3: Create a basic agent (1 hour)</li>
<li>Exercise 4: Add memory (30 min)</li>
<li><strong>Checkpoint:</strong> Can you build a simple agent from scratch?</li></ul>
<p>---</p>
<h2>üéØ <strong>How to Use This Guide</strong></h2>
<p>1. <strong>Study each module in order</strong> - concepts build on each other
2. <strong>Take notes in your own words</strong> - don't just read
3. <strong>Complete the checkpoint questions</strong> - test your understanding
4. <strong>Do ALL hands-on exercises</strong> - reading is not enough
5. <strong>Explain concepts out loud</strong> - pretend you're teaching someone</p>
<p>---</p>
<h2>üìä <strong>Progress Tracker</strong></h2>
<p>| Module | Time | Status | Self-Rating (1-5) | Notes |
|--------|------|--------|-------------------|-------|
| Module 1: LLMs | 30m | ‚¨ú | - | |
| Module 2: Tokens | 45m | ‚¨ú | - | |
| Module 3: Prompting | 45m | ‚¨ú | - | |
| Module 4: CoT vs ReAct | 45m | ‚¨ú | - | |
| Module 5: Tool Calling | 1h | ‚¨ú | - | |
| Module 6: Agent Loops | 45m | ‚¨ú | - | |
| Module 7: LangChain | 45m | ‚¨ú | - | |
| Module 8: Memory | 45m | ‚¨ú | - | |
| Module 9: Parsers | 30m | ‚¨ú | - | |
| Module 10: RAG | 45m | ‚¨ú | - | |
| Module 11: Error Handling | 45m | ‚¨ú | - | |
| Module 12: ML Systems | 30m | ‚¨ú | - | |
| Exercise 1 | 30m | ‚¨ú | - | |
| Exercise 2 | 30m | ‚¨ú | - | |
| Exercise 3 | 1h | ‚¨ú | - | |
| Exercise 4 | 30m | ‚¨ú | - | |</p>
<p>---</p>
<h2>üöÄ <strong>When You're Ready</strong></h2>
<p>After completing ALL modules and exercises, you should be able to:</p>
<p>‚úÖ Explain what an LLM is and how it generates text  
‚úÖ Describe the difference between CoT and ReAct patterns  
‚úÖ Design tools for an agent with proper schemas  
‚úÖ Choose the right memory type for your use case  
‚úÖ Parse LLM outputs safely with validation  
‚úÖ Handle errors and edge cases in production  
‚úÖ Compare alternatives and justify decisions</p>
<strong>If you can do all of the above, you're ready to build the K8s agent project with confidence!</strong>
<p>---</p>
<h2>üìù <strong>Study Tips</strong></h2>
<p>1. <strong>Don't rush</strong> - Understanding > Speed
2. <strong>Take breaks</strong> - 25 min study, 5 min break
3. <strong>Practice explaining</strong> - Teach it back to yourself
4. <strong>Ask "why"</strong> - Don't just memorize, understand
5. <strong>Connect to experience</strong> - Relate to your K8s knowledge</p>
<p>---</p>
<strong>Start with:</strong> `01_LLM_Fundamentals.md`

    </div>
    

    <div class="module-content" id="module-1">
        <h1>Module 1: What are Large Language Models (LLMs)?</h1>
<strong>Time:</strong> 30 minutes  
<strong>Goal:</strong> Understand what LLMs are, how they work at a high level, and their capabilities/limitations
<p>---</p>
<h2>ü§ñ <strong>What is an LLM?</strong></h2>
<h3><strong>Simple Definition</strong></h3>
A Large Language Model is a <strong>neural network trained on massive amounts of text</strong> that learns to <strong>predict the next word</strong> in a sequence.
<strong>Analogy:</strong> 
Think of it like super-advanced autocomplete on your phone. Type "The weather is..." and your phone suggests "nice", "bad", "sunny". An LLM does this at a much more sophisticated level.
<p>---</p>
<h2>üß† <strong>How LLMs Work (High Level)</strong></h2>
<h3><strong>1. Training Phase</strong></h3>
<pre><code class="language-text">Step 1: Collect Data
<ul><li>Scrape billions of web pages, books, code repositories</li>
<li>For GPT-4: ~13 trillion words</li>
<li>For Llama 3: ~15 trillion words</li>
<p>Step 2: Tokenization
<li>Break text into tokens (roughly words/sub-words)</li>
<li>&quot;Hello world&quot; ‚Üí [&quot;Hello&quot;, &quot; world&quot;]</li>
<li>Each token gets a number ID</li></p>
<p>Step 3: Learn Patterns
<li>Show the model: &quot;The cat sat on the ___&quot;</li>
<li>Model predicts: &quot;mat&quot; (90%), &quot;floor&quot; (5%), &quot;chair&quot; (3%)</li>
<li>Adjust model weights to improve predictions</li>
<li>Repeat trillions of times</li></p>
<p>Result: Model learns grammar, facts, reasoning patterns</code></pre></p>
<h3><strong>2. Inference Phase (Using the Model)</strong></h3>
<pre><code class="language-text">You: &quot;Why is my Kubernetes pod failing?&quot;
<p>LLM Process:
1. Convert to tokens: [&quot;Why&quot;, &quot; is&quot;, &quot; my&quot;, &quot; Kubernetes&quot;, &quot; pod&quot;, &quot; failing&quot;, &quot;?&quot;]
2. Process through neural network layers
3. Generate probability distribution for next token
4. Sample a token: &quot;There&quot; (60% probability)
5. Add to sequence: &quot;Why...failing? There&quot;
6. Repeat until natural stopping point</p>
<p>Output: &quot;There could be several reasons: port conflicts, 
        resource limits, image pull errors...&quot;</code></pre></p>
<p>---</p>
<h2>üìä <strong>Key Concepts</strong></h2>
<h3><strong>1. Tokens</strong></h3>
<pre><code class="language-text">Token ‚âà Word (but not exactly)
<p>Examples:
&quot;Hello world&quot; ‚Üí 2 tokens
&quot;Kubernetes&quot; ‚Üí 1 token  
&quot;GPT-4&quot; ‚Üí 2 tokens (G, PT-4)
&quot;API&quot; ‚Üí 1 token</p>
<p>Why it matters:
<li>LLMs have TOKEN limits, not word limits</li>
<li>GPT-4: 8K tokens ‚âà 6K words</li>
<li>Every API call costs per token</code></pre></li></p>
<h3><strong>2. Context Window</strong></h3>
<pre><code class="language-text">Context Window = Maximum tokens LLM can &quot;remember&quot; at once
<p>GPT-4: 8,192 tokens (standard) or 32,768 tokens (extended)
Llama 3: 8,192 tokens
Claude 3: 200,000 tokens</p>
<p>What fits in 8K tokens?
<li>~10 pages of text</li>
<li>~300 lines of code</li>
<li>20-30 back-and-forth messages</li></p>
<p>What happens when you exceed it?
<li>Older messages get &quot;forgotten&quot;</li>
<li>Need memory management strategies</code></pre></li></p>
<h3><strong>3. Temperature (Creativity Control)</strong></h3>
<pre><code class="language-text">Temperature = 0.0 ‚Üí Deterministic (always same answer)
Temperature = 0.7 ‚Üí Balanced (default for most uses)
Temperature = 1.0 ‚Üí Creative (more variety, more &quot;risky&quot;)
Temperature = 2.0 ‚Üí Chaotic (probably nonsense)
<p>Example: &quot;Name a fruit&quot;</p>
<p>Temp 0.0: Always says &quot;apple&quot; (most probable)
Temp 0.7: &quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot; (varies)
Temp 2.0: &quot;dragonfruit&quot;, &quot;persimmon&quot;, &quot;starfruit&quot; (unusual)</p>
<p>For your K8s agent: Use 0.0-0.3 (need consistency!)</code></pre></p>
<h3><strong>4. Parameters (Model Size)</strong></h3>
<pre><code class="language-text">Parameter = A single &quot;knob&quot; the model can adjust
<p>Model sizes:
<li>GPT-3: 175 billion parameters</li>
<li>GPT-4: ~1.7 trillion parameters (estimated)</li>
<li>Llama 3 7B: 7 billion parameters</li>
<li>Llama 3 70B: 70 billion parameters</li></p>
<p>More parameters = 
  ‚úÖ Better at complex reasoning
  ‚úÖ More knowledge
  ‚ùå Slower inference
  ‚ùå More expensive
  ‚ùå More memory needed</p>
<p>For your project:
<li>Copilot (GPT-4 based): ~1.7T params - Best accuracy</li>
<li>Llama 3 70B: Good balance</li>
<li>Llama 3 7B: Fast, good enough for many tasks</code></pre></li></p>
<p>---</p>
<h2>üéØ <strong>What LLMs Can and Cannot Do</strong></h2>
<h3><strong>‚úÖ LLMs Are Good At:</strong></h3>
<p>1. <strong>Text Generation</strong>
   - Writing code
   - Explaining concepts
   - Translating languages</p>
<p>2. <strong>Pattern Recognition</strong>
   - Finding errors in logs
   - Identifying common issues
   - Suggesting fixes based on similar problems</p>
<p>3. <strong>Reasoning (to an extent)</strong>
   - Following instructions
   - Breaking down complex tasks
   - Making logical connections</p>
<h3><strong>‚ùå LLMs Are BAD At:</strong></h3>
<p>1. <strong>Math & Calculations</strong>
   - "What is 4729 √ó 8361?" ‚Üí Often wrong
   - Solution: Give it a calculator tool!</p>
<p>2. <strong>Current Information</strong>
   - Training data has a cutoff date
   - GPT-4 knowledge ends in April 2023
   - Solution: Give it a search tool!</p>
<p>3. <strong>Deterministic Tasks</strong>
   - "List all pods in alphabetical order" ‚Üí May vary
   - Solution: Use traditional code for exact operations!</p>
<p>4. <strong>Accessing External Systems</strong>
   - Cannot directly run `kubectl get pods`
   - Solution: Give it tools to call functions!</p>
<p>---</p>
<h2>üîç <strong>LLM Limitations (Critical to Understand)</strong></h2>
<h3><strong>1. Hallucinations</strong></h3>
<pre><code class="language-text">Problem: LLM confidently generates false information
<p>Example:
You: &quot;What&#039;s the fix for K8s error XYZ123?&quot;
LLM: &quot;Run command: kubectl fix --error XYZ123&quot;
Reality: That command doesn&#039;t exist!</p>
<p>Why: LLM learned patterns, not truth
Solution: Always validate LLM outputs</code></pre></p>
<h3><strong>2. Consistency</strong></h3>
<pre><code class="language-text">Problem: Same input ‚Üí Different outputs
<p>Ask twice: &quot;Is the pod healthy?&quot;
Response 1: &quot;Yes, pod is running&quot;
Response 2: &quot;No, pod is in CrashLoopBackOff&quot;</p>
<p>Why: Probabilistic sampling
Solution: Use low temperature (0.0-0.3) for factual tasks</code></pre></p>
<h3><strong>3. Token Limits</strong></h3>
<pre><code class="language-text">Problem: Cannot process infinite context
<p>Your K8s cluster has 1000 pods
Logs are 100MB
LLM can only see 8K tokens ‚âà 6K words</p>
<p>Solution: 
<li>Summarize logs before sending to LLM</li>
<li>Use RAG to retrieve only relevant parts</li>
<li>Process in chunks</code></pre></li></p>
<p>---</p>
<h2>üèóÔ∏è <strong>LLM Architectures You'll Hear About</strong></h2>
<h3><strong>Transformer Architecture</strong></h3>
<pre><code class="language-text">What: Neural network design that powers all modern LLMs
Invented: 2017 (Paper: &quot;Attention Is All You Need&quot;)
Key innovation: Attention mechanism
<p>Before Transformers:
&quot;The cat sat on the mat&quot;
Model processed word-by-word sequentially ‚Üí Slow</p>
<p>With Transformers:
Model processes all words simultaneously
Attention mechanism figures out relationships
&quot;cat&quot; and &quot;mat&quot; are related ‚Üí Much faster and better</code></pre></p>
<h3><strong>Pre-training + Fine-tuning</strong></h3>
<pre><code class="language-text">Step 1: Pre-training (Expensive)
<li>Train on all of internet</li>
<li>Learn general language patterns</li>
<li>Cost: $10M - $100M for GPT-4 scale</li>
<p>Step 2: Fine-tuning (Cheaper)
<li>Train on specific domain data</li>
<li>Example: Medical diagnosis, legal documents</li>
<li>Cost: $1K - $100K</li></p>
<p>Your project could use:
<li>Pre-trained Llama 3 (free, open source)</li>
<li>Fine-tune on your K8s issues (if you had 1000+ examples)</code></pre></li></p>
<p>---</p>
<h2>üìù <strong>Self-Check Questions</strong></h2>
<p>Test yourself before moving to Module 2:</p>
<p>1. <strong>What is an LLM in one sentence?</strong>
   <details>
   <summary>Answer</summary>
   A neural network trained on massive text data that predicts the next word in a sequence.
   </details></p>
<p>2. <strong>Why do LLMs have token limits?</strong>
   <details>
   <summary>Answer</summary>
   The attention mechanism has quadratic complexity - doubling tokens = 4x memory/compute. Physical limits force a maximum context window.
   </details></p>
<p>3. <strong>When should you use temperature = 0.0 vs 1.0?</strong>
   <details>
   <summary>Answer</summary>
   Temp 0.0 for factual/consistent outputs (diagnostics, data extraction). Temp 1.0 for creative tasks (brainstorming, story writing).
   </details></p>
<p>4. <strong>What's the difference between 7B and 70B parameter models?</strong>
   <details>
   <summary>Answer</summary>
   70B has 10x more parameters = better reasoning and knowledge, but slower and more expensive to run. 7B is faster, cheaper, good for simpler tasks.
   </details></p>
<p>5. <strong>Why can't LLMs run kubectl directly?</strong>
   <details>
   <summary>Answer</summary>
   LLMs only generate text. They can't execute code or interact with systems. Need to give them tools/functions to call external systems.
   </details></p>
<p>6. <strong>What is a hallucination?</strong>
   <details>
   <summary>Answer</summary>
   When an LLM confidently generates false information that sounds plausible. Happens because it's trained to predict text patterns, not verify truth.
   </details></p>
<p>---</p>
<h2>üéì <strong>Key Takeaways</strong></h2>
<p>‚úÖ LLMs predict next tokens based on patterns learned from training data  
‚úÖ They have token limits (context windows) you must manage  
‚úÖ Temperature controls randomness/creativity of outputs  
‚úÖ Larger models (more parameters) = better but slower  
‚úÖ LLMs hallucinate and need validation  
‚úÖ LLMs can't directly interact with systems - need tools</p>
<p>---</p>
<h2>üöÄ <strong>Ready for Module 2?</strong></h2>
<p>If you can explain:
<li>What an LLM is to someone non-technical</li>
<li>Why token limits matter</li>
<li>When to use different temperatures</li>
<li>Why LLMs need tools to be useful</li></ul></p>
<strong>‚Üí Move to `02_Tokens_Context_Embeddings.md`</strong>
<p>Otherwise, re-read the sections you're unclear on and try explaining them out loud!</p>
    </div>
    

    <div class="module-content" id="module-2">
        <h1>Module 2: Tokens, Context Windows, and Embeddings</h1>
<strong>Time:</strong> 45 minutes  
<strong>Goal:</strong> Deep dive into tokens, understand context management, and learn about embeddings
<p>---</p>
<h2>üé´ <strong>Tokens: The Currency of LLMs</strong></h2>
<h3><strong>What Exactly is a Token?</strong></h3>
<pre><code class="language-text">Token = Smallest unit an LLM processes
<p>NOT always a word!</p>
<p>Examples:
&quot;Hello&quot; ‚Üí 1 token
&quot;Hello world&quot; ‚Üí 2 tokens  [&quot;Hello&quot;, &quot; world&quot;]
&quot;don&#039;t&quot; ‚Üí 2 tokens  [&quot;don&quot;, &quot;&#039;t&quot;]
&quot;Kubernetes&quot; ‚Üí 1 token  [&quot;Kubernetes&quot;]
&quot;kubectl&quot; ‚Üí 2 tokens  [&quot;k&quot;, &quot;ubectl&quot;]
&quot;GPT-4&quot; ‚Üí 2 tokens  [&quot;G&quot;, &quot;PT-4&quot;]
&quot;&lt;|endoftext|&gt;&quot; ‚Üí 1 special token</p>
<p>Why weird splits?
<ul><li>Tokenizer trained on common patterns</li>
<li>Common words = 1 token</li>
<li>Rare words = multiple tokens</li>
<li>Keeps vocabulary manageable (~50K tokens)</code></pre></li></p>
<h3><strong>Counting Tokens</strong></h3>
<pre><code class="language-python"># Using tiktoken (OpenAI&#039;s tokenizer)
import tiktoken
<p>encoder = tiktoken.encoding_for_model(&quot;gpt-4&quot;)</p>
<p>text = &quot;Why is my Kubernetes pod failing?&quot;
tokens = encoder.encode(text)
print(f&quot;Tokens: {tokens}&quot;)
print(f&quot;Count: {len(tokens)}&quot;)</p>
<h1>Output:</h1>
<h1>Tokens: [10445, 374, 856, 39195, 7661, 22109, 30]</h1>
<h1>Count: 7 tokens</h1>
<h1>Rule of thumb:</h1>
<h1>1 token ‚âà 0.75 words (English)</h1>
<h1>100 tokens ‚âà 75 words</h1>
<h1>1000 tokens ‚âà 750 words or 3-4 paragraphs</code></pre></h1>
<p>---</p>
<h2>ü™ü <strong>Context Windows: The Memory Limit</strong></h2>
<h3><strong>What is a Context Window?</strong></h3>
<pre><code class="language-text">Context Window = Maximum number of tokens LLM can process at once
<p>Includes:
1. System prompt
2. Conversation history
3. Your current question
4. LLM&#039;s response (being generated)</p>
<p>Example with 8K token limit:</p>
<p>System prompt: 200 tokens
User message 1: 50 tokens
Assistant reply 1: 100 tokens
User message 2: 50 tokens
Assistant reply 2: 100 tokens
... (conversation continues)
Current question: 100 tokens
---
Total so far: 600 tokens
Remaining: 7,400 tokens for response + future messages</code></pre></p>
<h3><strong>Model Context Limits</strong></h3>
<p>| Model | Context Window | Typical Use |
|-------|----------------|-------------|
| <strong>GPT-4</strong> | 8K tokens | Standard API calls |
| <strong>GPT-4-32K</strong> | 32K tokens | Long documents |
| <strong>GPT-4-Turbo</strong> | 128K tokens | Very long context |
| <strong>Claude 3</strong> | 200K tokens | Entire codebases |
| <strong>Llama 3 7B</strong> | 8K tokens | Your project (free) |
| <strong>Llama 3 70B</strong> | 8K tokens | Better quality, same limit |</p>
<strong>8K tokens example:</strong>
<li>~6,000 words</li>
<li>~25 pages of text</li>
<li>~300 lines of code</li>
<li>20-30 back-and-forth messages</li>
<p>---</p>
<h2>‚ö†Ô∏è <strong>What Happens When You Hit the Limit?</strong></h2>
<h3><strong>Problem: Context Overflow</strong></h3>
<pre><code class="language-text">Scenario: Debugging session with your K8s agent
<p>Message 1: &quot;Check nginx pod&quot; (50 tokens)
Reply 1: &quot;Pod is CrashLoopBackOff...&quot; (100 tokens)
Message 2: &quot;Get logs&quot; (10 tokens)
Reply 2: &quot;Error: port 80 in use...&quot; (500 tokens - long logs)
Message 3: &quot;Check other pods&quot; (15 tokens)
Reply 3: &quot;Found 5 pods...&quot; (200 tokens)
... 15 more exchanges ...
Message 20: &quot;What was the original issue?&quot;</p>
<p>Problem: Message 1-5 exceeded 8K limit!
Result: LLM forgot the original nginx problem!</code></pre></p>
<h3><strong>Solutions:</strong></h3>
<p>#### <strong>Solution 1: Sliding Window Memory</strong>
<pre><code class="language-python">from langchain.memory import ConversationBufferWindowMemory</p>
<p>memory = ConversationBufferWindowMemory(
    k=10  # Keep only last 10 messages
)</p>
<h1>Automatically drops old messages</h1>
<h1>Pros: Simple, predictable</h1>
<h1>Cons: Loses important context</code></pre></h1>
<p>#### <strong>Solution 2: Summarization</strong>
<pre><code class="language-python">from langchain.memory import ConversationSummaryMemory</p>
<p>memory = ConversationSummaryMemory(
    llm=llm  # Uses LLM to summarize!
)</p>
<h1>Old messages get condensed:</h1>
<h1>Before: &quot;Check nginx ‚Üí CrashLoopBackOff ‚Üí Port 80 in use&quot;</h1>
<h1>After (summary): &quot;Diagnosed nginx port conflict&quot;</h1>
<h1>Pros: Preserves key info</h1>
<h1>Cons: Extra LLM calls (cost)</code></pre></h1>
<p>#### <strong>Solution 3: Chunk and Process</strong>
<pre><code class="language-python"># For large inputs (logs, code files)</p>
<p>def process_large_logs(logs):
    # Split into chunks
    chunks = split_into_chunks(logs, chunk_size=2000)
    
    summaries = []
    for chunk in chunks:
        summary = llm.summarize(chunk)
        summaries.append(summary)
    
    # Final analysis on summaries only
    return llm.analyze(summaries)</p>
<h1>100K token logs ‚Üí 10 x 2K chunks ‚Üí 10 x 100 token summaries</h1>
<h1>= 1000 tokens to analyze (fits in context!)</code></pre></h1>
<p>---</p>
<h2>üî¢ <strong>Embeddings: Numeric Representations of Text</strong></h2>
<h3><strong>What are Embeddings?</strong></h3>
<pre><code class="language-text">Embedding = A vector (list of numbers) that represents text
<p>Example:
&quot;Kubernetes pod&quot; ‚Üí [0.2, -0.5, 0.8, ..., 0.1]  (1536 numbers)
&quot;K8s container&quot; ‚Üí [0.3, -0.4, 0.7, ..., 0.2]  (similar!)
&quot;Pizza recipe&quot; ‚Üí [-0.9, 0.1, -0.2, ..., 0.5]  (very different!)</p>
<p>Why useful:
<li>Similar meanings = similar vectors</li>
<li>Can compute similarity mathematically</li>
<li>Enables semantic search</code></pre></li></p>
<h3><strong>How Embeddings Work</strong></h3>
<pre><code class="language-text">Text ‚Üí Embedding Model ‚Üí Vector
<p>Models:
<li>OpenAI: text-embedding-ada-002 (1536 dimensions)</li>
<li>Sentence Transformers: all-MiniLM-L6-v2 (384 dimensions)</li>
<li>Cohere: embed-english-v3.0 (1024 dimensions)</li></p>
<p>Process:
1. Text: &quot;nginx pod failing&quot;
2. Model converts to: [0.5, -0.2, ...]  # 1536 numbers
3. Store in vector database
4. Later search: &quot;container crash&quot; ‚Üí find similar vectors</code></pre></p>
<h3><strong>Practical Example</strong></h3>
<pre><code class="language-python">from openai import OpenAI
<p>client = OpenAI()</p>
<h1>Create embeddings</h1>
text1 = &quot;Kubernetes pod is crashing&quot;
text2 = &quot;Container keeps failing&quot;
text3 = &quot;How to bake cookies&quot;
<p>emb1 = client.embeddings.create(input=text1, model=&quot;text-embedding-ada-002&quot;)
emb2 = client.embeddings.create(input=text2, model=&quot;text-embedding-ada-002&quot;)
emb3 = client.embeddings.create(input=text3, model=&quot;text-embedding-ada-002&quot;)</p>
<h1>Compute similarity (cosine similarity)</h1>
similarity_1_2 = cosine_similarity(emb1, emb2)  # ~0.85 (very similar!)
similarity_1_3 = cosine_similarity(emb1, emb3)  # ~0.12 (not similar)
<h1>Use case: Find similar past incidents</h1>
<h1>User: &quot;My pod won&#039;t start&quot;</h1>
<h1>Search embeddings ‚Üí Find &quot;pod startup failure&quot; from last week</h1>
<h1>Retrieve that solution</code></pre></h1>
<p>---</p>
<h2>üéØ <strong>When to Use Embeddings in Your Project</strong></h2>
<h3><strong>Use Case 1: Semantic Search in Documentation</strong></h3>
<pre><code class="language-text">Problem: You have 1000 pages of K8s docs
User asks: &quot;How do I fix ImagePullBackOff?&quot;
<p>Without embeddings:
<li>Keyword search for &quot;ImagePullBackOff&quot;</li>
<li>Might miss docs that say &quot;image pull failure&quot; or &quot;cannot pull container image&quot;</li></p>
<p>With embeddings:
1. Convert all docs to embeddings (one-time setup)
2. Store in vector database (Chroma, Pinecone)
3. User query ‚Üí embedding
4. Find most similar doc embeddings
5. Return relevant docs even with different wording!</code></pre></p>
<h3><strong>Use Case 2: Finding Similar Past Incidents</strong></h3>
<pre><code class="language-text">Problem: Same issues happen repeatedly
<p>Solution:
1. Store all resolved incidents with embeddings
   - Incident 1: &quot;nginx CrashLoopBackOff due to port conflict&quot;
   - Incident 2: &quot;API service OOMKilled, increased memory&quot;
   
2. New incident: &quot;nginx keeps restarting&quot;
   ‚Üí Find similar embedding ‚Üí Suggest &quot;Check port conflicts&quot;</p>
<p>This is how chatbots &quot;remember&quot; past conversations semantically!</code></pre></p>
<h3><strong>Use Case 3: Code Search</strong></h3>
<pre><code class="language-text">User: &quot;Find code that handles pod failures&quot;
<p>Traditional search: grep for &quot;pod&quot; and &quot;failure&quot;
‚Üí Misses code with different variable names</p>
<p>Embedding search:
<li>Finds semantically similar code</li>
<li>&quot;container crash handler&quot;</li>
<li>&quot;k8s restart logic&quot;</li>
<li>&quot;pod error management&quot;</li></p>
<p>All related code, even with different keywords!</code></pre></p>
<p>---</p>
<h2>üèóÔ∏è <strong>Vector Databases</strong></h2>
<h3><strong>What are They?</strong></h3>
<pre><code class="language-text">Vector Database = Database optimized for storing and searching embeddings
<p>Popular options:
<li>Chroma (free, local, great for learning)</li>
<li>Pinecone (managed, scalable)</li>
<li>Weaviate (open-source, production-ready)</li>
<li>Milvus (high performance)</li></p>
<p>Why special database?
<li>Regular DB: Search exact matches (SQL WHERE clause)</li>
<li>Vector DB: Search similar vectors (nearest neighbor)</code></pre></li></p>
<h3><strong>Example with Chroma</strong></h3>
<pre><code class="language-python">from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
<h1>1. Create vector store</h1>
vectorstore = Chroma(
    collection_name=&quot;k8s_incidents&quot;,
    embedding_function=OpenAIEmbeddings()
)
<h1>2. Add documents</h1>
incidents = [
    &quot;nginx pod failing due to port 80 conflict&quot;,
    &quot;postgres OOMKilled, need more memory&quot;,
    &quot;api service ImagePullBackOff, wrong image tag&quot;
]
<p>vectorstore.add_texts(incidents)</p>
<h1>3. Search semantically</h1>
query = &quot;container memory issue&quot;
results = vectorstore.similarity_search(query, k=2)
<h1>Returns:</h1>
<h1>1. &quot;postgres OOMKilled...&quot; (most similar!)</h1>
<h1>2. &quot;api service ImagePullBackOff...&quot; (less similar)</code></pre></h1>
<p>---</p>
<h2>üìä <strong>Token & Context Management Strategies</strong></h2>
<h3><strong>Strategy 1: Prompt Compression</strong></h3>
<pre><code class="language-text">Bad prompt (verbose):
&quot;You are a helpful assistant that helps with Kubernetes. You should 
be polite and professional. You have access to kubectl commands. You 
should analyze pod status carefully. If you&#039;re not sure, you should 
ask for more information...&quot; (200 tokens!)
<p>Good prompt (concise):
&quot;K8s troubleshooting expert. Use kubectl tools. Be precise.&quot; (15 tokens!)</p>
<p>Saved: 185 tokens for actual content!</code></pre></p>
<h3><strong>Strategy 2: Smart Truncation</strong></h3>
<pre><code class="language-python">def truncate_logs(logs, max_tokens=1000):
    &quot;&quot;&quot;Keep most relevant parts of logs&quot;&quot;&quot;
    
    # Get last N lines (most recent)
    recent_logs = logs[-50:]
    
    # Extract error lines
    error_logs = [line for line in logs if &quot;error&quot; in line.lower()]
    
    # Combine
    relevant_logs = error_logs + recent_logs
    
    # Truncate to token limit
    return truncate_to_tokens(relevant_logs, max_tokens)
<h1>100K token logs ‚Üí 1K token summary</h1>
<h1>Fits in context, keeps important info!</code></pre></h1>
<h3><strong>Strategy 3: Hierarchical Processing</strong></h3>
<pre><code class="language-text">For large codebase analysis:
<p>Level 1: Summarize each file (parallel)
  file1.py ‚Üí &quot;Authentication logic&quot; (50 tokens)
  file2.py ‚Üí &quot;Database models&quot; (50 tokens)
  file3.py ‚Üí &quot;API routes&quot; (50 tokens)</p>
<p>Level 2: Combine summaries (150 tokens total)
  ‚Üí Ask LLM: &quot;Which file likely has the bug?&quot;
  ‚Üí &quot;file3.py (API routes)&quot;</p>
<p>Level 3: Deep dive only that file
  ‚Üí Read full file, analyze in detail</p>
<p>Instead of: All files (10K tokens) ‚Üí Overflow!
We do: Summaries (150 tokens) ‚Üí Focus (500 tokens) = Fits!</code></pre></p>
<p>---</p>
<h2>üìù <strong>Self-Check Questions</strong></h2>
<p>1. <strong>How many tokens is "Kubernetes pod is failing"?</strong>
   <details>
   <summary>Answer</summary>
   Approximately 6-7 tokens. "Kubernetes"=1, "pod"=1, "is"=1, "failing"=1, plus spaces. Use tiktoken to count exactly.
   </details></p>
<p>2. <strong>What happens when you exceed context window?</strong>
   <details>
   <summary>Answer</summary>
   Older messages get truncated/forgotten. LLM cannot see the full conversation history. Need memory management strategies.
   </details></p>
<p>3. <strong>When would you use embeddings vs direct LLM calls?</strong>
   <details>
   <summary>Answer</summary>
   Embeddings: Semantic search in large document sets, finding similar items. LLM: Generation, reasoning, specific questions.
   </details></p>
<p>4. <strong>Why use a vector database instead of regular database?</strong>
   <details>
   <summary>Answer</summary>
   Vector DBs are optimized for similarity search (nearest neighbors). Regular DBs only do exact matches. Need special algorithms (HNSW, IVF) for fast vector search.
   </details></p>
<p>5. <strong>How would you handle 100K token logs in 8K context window?</strong>
   <details>
   <summary>Answer</summary>
   Chunk into smaller pieces, summarize each chunk, analyze summaries. Or extract only error lines and recent logs. Or use embeddings to find relevant sections.
   </details></p>
<p>---</p>
<h2>üéì <strong>Key Takeaways</strong></h2>
<p>‚úÖ Tokens are the atomic unit - 1 token ‚âà 0.75 words  
‚úÖ Context windows are hard limits - plan for overflow  
‚úÖ Embeddings convert text to vectors for similarity search  
‚úÖ Vector databases enable semantic search at scale  
‚úÖ Always count tokens before sending to LLM  
‚úÖ Use compression, truncation, chunking for large inputs</p>
<p>---</p>
<h2>üöÄ <strong>Next Steps</strong></h2>
<p>Can you explain:
<li>Why token limits matter for conversation agents?</li>
<li>How embeddings enable semantic search?</li>
<li>When to use vector databases?</li></ul></p>
<strong>‚Üí Ready for `03_Prompting_Temperature.md`</strong>

    </div>
    

    <div class="module-content" id="module-3">
        <h1>Module 3: Prompting & Temperature</h1>
<strong>Study Time</strong>: ~45 minutes  
<strong>Prerequisites</strong>: Module 1 (LLM Fundamentals)
<p>---</p>
<h2>üéØ <strong>Learning Objectives</strong></h2>
<p>By the end of this module, you'll understand:
1. How to write effective prompts that get better results
2. What temperature controls and when to adjust it
3. Other sampling parameters (top_p, top_k, max_tokens)
4. Prompt engineering patterns for different use cases
5. Why prompting matters for your K8s agent</p>
<p>---</p>
<h2>üìù <strong>What is Prompting?</strong></h2>
<strong>Prompting</strong> is how you communicate with an LLM. It's the art and science of crafting inputs that get the outputs you want.
<h3><strong>Why It Matters</strong></h3>
<p>The same LLM can give vastly different results based on how you prompt it:</p>
<p>#### <strong>Bad Prompt</strong>
<pre><code class="language-text">User: &quot;Fix my Kubernetes.&quot;
LLM: &quot;I need more information. What&#039;s wrong with your Kubernetes cluster?&quot;</code></pre></p>
<p>#### <strong>Good Prompt</strong>
<pre><code class="language-text">User: &quot;My pod nginx-deployment-abc123 in namespace production is stuck in 
CrashLoopBackOff state. The logs show &#039;Error: ECONNREFUSED connecting to 
database at db-service:5432&#039;. How can I troubleshoot this?&quot;</p>
<p>LLM: &quot;The pod can&#039;t connect to the database. Let&#039;s check:
1. Is the database pod running? Check with: kubectl get pods -l app=database
2. Is the service correctly configured? Check with: kubectl get svc db-service
3. Are the network policies allowing traffic?
...&quot;</code></pre></p>
<strong>Key Difference</strong>: Specificity. Good prompts provide context, constraints, and clear expectations.
<p>---</p>
<h2>üî• <strong>Temperature: The Creativity Dial</strong></h2>
<h3><strong>What is Temperature?</strong></h3>
<p>Temperature controls <strong>randomness</strong> in the LLM's outputs.</p>
<ul><li><strong>Range</strong>: 0.0 to 2.0 (typically use 0.0 to 1.0)</li>
<li><strong>Low (0.0)</strong>: Deterministic, consistent, factual</li>
<li><strong>High (1.0+)</strong>: Creative, varied, unpredictable</li>
<h3><strong>How It Works (Simplified)</strong></h3>
<p>When predicting the next token, the LLM assigns probabilities:</p>
<pre><code class="language-text">Input: &quot;The capital of France is&quot;
<p>Predictions:
<li>&quot;Paris&quot; ‚Üí 95% probability</li>
<li>&quot;paris&quot; ‚Üí 3% probability  </li>
<li>&quot;Lyon&quot; ‚Üí 1% probability</li>
<li>&quot;London&quot; ‚Üí 0.5% probability</li>
<li>&quot;banana&quot; ‚Üí 0.01% probability</code></pre></li></p>
<p>#### <strong>Temperature = 0.0</strong>
Always picks the highest probability token ‚Üí "Paris"</p>
<p>#### <strong>Temperature = 0.7</strong>
Samples from the distribution, weighted by probabilities ‚Üí Usually "Paris", sometimes "paris"</p>
<p>#### <strong>Temperature = 1.5</strong>
Flattens probabilities more ‚Üí Could pick "Lyon" or even "London" sometimes</p>
<h3><strong>Visual Analogy</strong></h3>
<p>Think of temperature as a slider:</p>
<pre><code class="language-text">0.0 ========|                          1.0
    Robot                          Creative Human
    
<li>Repeatable                      - Varied</li>
<li>Factual                         - Imaginative</li>
<li>Safe                            - Risky</li>
<li>Boring                          - Interesting</code></pre></li>
<p>---</p>
<h2>üéöÔ∏è <strong>When to Use Different Temperatures</strong></h2>
<h3><strong>Temperature 0.0 - 0.3: Deterministic</strong></h3>
<strong>Use for</strong>:
<li>‚úÖ Code generation (want correct syntax)</li>
<li>‚úÖ Data extraction (want accurate parsing)</li>
<li>‚úÖ Math/logic problems</li>
<li>‚úÖ Tool calling in agents (want consistent tool selection)</li>
<li>‚úÖ <strong>Your K8s agent</strong> (want reliable diagnostics)</li>
<strong>Example</strong>:
<pre><code class="language-python">from openai import OpenAI
client = OpenAI()
<p>response = client.chat.completions.create(
    model=&quot;gpt-4&quot;,
    messages=[
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Extract the pod name from: &#039;Pod nginx-abc123 is failing&#039;&quot;}
    ],
    temperature=0.0  # Want exact extraction
)
<h1>Output: &quot;nginx-abc123&quot; (same every time)</code></pre></h1></p>
<h3><strong>Temperature 0.4 - 0.7: Balanced</strong></h3>
<strong>Use for</strong>:
<li>‚úÖ Chatbots (want varied but reasonable responses)</li>
<li>‚úÖ Summaries (want natural language)</li>
<li>‚úÖ Explanations (want readability)</li>
<li>‚úÖ General Q&A</li>
<strong>Example</strong>:
<pre><code class="language-python">response = client.chat.completions.create(
    model=&quot;gpt-4&quot;,
    messages=[
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Explain Kubernetes pods to a beginner&quot;}
    ],
    temperature=0.7  # Want natural, engaging explanation
)</code></pre>
<h3><strong>Temperature 0.8 - 1.0: Creative</strong></h3>
<strong>Use for</strong>:
<li>‚úÖ Creative writing (stories, marketing copy)</li>
<li>‚úÖ Brainstorming (want diverse ideas)</li>
<li>‚úÖ Humor generation</li>
<li>‚ùå <strong>NOT for your K8s agent</strong> (too unpredictable)</li>
<strong>Example</strong>:
<pre><code class="language-python">response = client.chat.completions.create(
    model=&quot;gpt-4&quot;,
    messages=[
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Write a haiku about Kubernetes&quot;}
    ],
    temperature=1.0  # Want creative poetry
)</code></pre>
<h3><strong>Temperature 1.0+: Experimental</strong></h3>
<strong>Use for</strong>:
<li>‚úÖ Art projects</li>
<li>‚úÖ Exploring unexpected outputs</li>
<li>‚ùå Production systems (too risky)</li>
<p>---</p>
<h2>üõ†Ô∏è <strong>Other Sampling Parameters</strong></h2>
<h3><strong>max_tokens</strong></h3>
<p>Controls the <strong>maximum length</strong> of the response.</p>
<pre><code class="language-python">response = client.chat.completions.create(
    model=&quot;gpt-4&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Explain Kubernetes&quot;}],
    max_tokens=50  # Stop after 50 tokens (~37 words)
)</code></pre>
<strong>Use cases</strong>:
<li>Prevent overly long responses</li>
<li>Control costs (you pay per token)</li>
<li>Force concise answers</li>
<h3><strong>top_p (Nucleus Sampling)</strong></h3>
<p>Alternative to temperature. Samples from the smallest set of tokens whose cumulative probability exceeds `p`.</p>
<pre><code class="language-python">response = client.chat.completions.create(
    model=&quot;gpt-4&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is a pod?&quot;}],
    top_p=0.9  # Consider tokens that make up top 90% of probability
)</code></pre>
<strong>Comparison</strong>:
<li>`temperature=0.7` ‚Üí Adjusts all probabilities</li>
<li>`top_p=0.9` ‚Üí Only considers top 90% of tokens, ignores rare ones</li>
<strong>Rule of Thumb</strong>: Use temperature OR top_p, not both.
<h3><strong>top_k</strong></h3>
<p>Limits sampling to the top `k` most probable tokens.</p>
<pre><code class="language-python"># Only consider the top 50 most likely tokens
top_k=50</code></pre>
<strong>Less common</strong> in modern APIs (OpenAI doesn't expose it). More common with Hugging Face models.
<p>---</p>
<h2>üìö <strong>Prompt Engineering Patterns</strong></h2>
<h3><strong>Pattern 1: Zero-Shot Prompting</strong></h3>
<p>Ask the model to do something without examples.</p>
<pre><code class="language-python">prompt = &quot;Classify this as positive or negative sentiment: &#039;I love this product!&#039;&quot;
<h1>Output: &quot;Positive&quot;</code></pre></h1>
<strong>When to use</strong>: Simple tasks, well-known domains
<h3><strong>Pattern 2: Few-Shot Prompting</strong></h3>
<p>Provide examples before asking.</p>
<pre><code class="language-python">prompt = &quot;&quot;&quot;
Classify sentiment as positive or negative:
<p>Example 1: &quot;Great service!&quot; ‚Üí Positive
Example 2: &quot;Terrible experience.&quot; ‚Üí Negative
Example 3: &quot;Not what I expected.&quot; ‚Üí Negative</p>
<p>Now classify: &quot;Amazing quality!&quot;
&quot;&quot;&quot;
<h1>Output: &quot;Positive&quot;</code></pre></h1></p>
<strong>When to use</strong>: Complex tasks, need specific format, improve accuracy
<h3><strong>Pattern 3: Chain-of-Thought (CoT)</strong></h3>
<p>Ask the model to think step-by-step.</p>
<pre><code class="language-python">prompt = &quot;&quot;&quot;
A pod is crashing. The logs show &quot;OOMKilled&quot;. 
Let&#039;s diagnose step by step:
1. What does OOMKilled mean?
2. What could cause this?
3. How do we fix it?
&quot;&quot;&quot;</code></pre>
<strong>Output</strong>:
<pre><code class="language-text">1. OOMKilled means the pod exceeded its memory limit and was killed by the 
   kernel&#039;s Out-Of-Memory killer.
2. Causes: Memory leak, underestimated resource limits, traffic spike
3. Fixes: Increase memory limit, find and fix memory leak, add horizontal 
   pod autoscaler</code></pre>
<strong>When to use</strong>: Complex reasoning, debugging, multi-step problems
<h3><strong>Pattern 4: Role Prompting</strong></h3>
<p>Tell the model what role to play.</p>
<pre><code class="language-python">prompt = &quot;&quot;&quot;
You are an expert Kubernetes administrator with 10 years of experience.
A junior developer asks: &quot;Why is my pod pending?&quot;
<p>Respond as the expert:
&quot;&quot;&quot;</code></pre></p>
<strong>When to use</strong>: Need specific expertise, tone, or perspective
<h3><strong>Pattern 5: Constrained Output</strong></h3>
<p>Specify the exact format you want.</p>
<pre><code class="language-python">prompt = &quot;&quot;&quot;
Extract pod information from this text:
&quot;Pod nginx-abc123 in namespace production is CrashLoopBackOff&quot;
<p>Respond in JSON format:
{
  &quot;pod_name&quot;: &quot;...&quot;,
  &quot;namespace&quot;: &quot;...&quot;,
  &quot;status&quot;: &quot;...&quot;
}
&quot;&quot;&quot;</code></pre></p>
<strong>When to use</strong>: Need structured output for parsing, APIs, downstream systems
<p>---</p>
<h2>ü§ñ <strong>Prompting for Your K8s Agent</strong></h2>
<h3><strong>System Prompt (Define Agent Behavior)</strong></h3>
<pre><code class="language-python">system_prompt = &quot;&quot;&quot;
You are a Kubernetes troubleshooting assistant. Your role is to help users 
diagnose and resolve cluster issues.
<p>Guidelines:
<li>Use the provided tools to gather real cluster data</li>
<li>Think step-by-step before taking actions</li>
<li>Always explain your reasoning</li>
<li>If you don&#039;t have enough information, use tools to gather it</li>
<li>Provide actionable recommendations</li>
<li>Be concise but thorough</li></p>
<p>Available tools:
<li>GetPodStatus: Check if a pod is Running, Pending, CrashLoopBackOff, etc.</li>
<li>GetPodLogs: Retrieve pod logs for error analysis</li>
<li>DescribePod: Get detailed pod configuration and events</li>
<li>AnalyzeErrors: Extract and explain error messages</li>
<li>CheckResources: Verify CPU/memory limits and usage</li></p>
<p>Always use tools rather than guessing. If unsure, ask for clarification.
&quot;&quot;&quot;</code></pre></p>
<strong>Why this works</strong>:
<li>‚úÖ Clear role definition</li>
<li>‚úÖ Explicit guidelines</li>
<li>‚úÖ Lists available tools</li>
<li>‚úÖ Sets expectations (use tools, don't guess)</li>
<h3><strong>User Query Optimization</strong></h3>
<p>Help users write better queries:</p>
<p>#### <strong>Bad User Query</strong>
<pre><code class="language-text">&quot;Help me with Kubernetes&quot;</code></pre></p>
<p>#### <strong>Good User Query</strong>
<pre><code class="language-text">&quot;Pod nginx-deployment-5678 in namespace production is stuck in CrashLoopBackOff. 
Can you help diagnose why?&quot;</code></pre></p>
<strong>Your agent should handle both</strong>, but better input = better output.
<h3><strong>Temperature for Agents</strong></h3>
<pre><code class="language-python">agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    temperature=0.0,  # ‚≠ê Critical for consistent tool calling
    max_iterations=5
)</code></pre>
<strong>Why 0.0?</strong>
<li>‚úÖ Consistent tool selection (won't randomly pick wrong tool)</li>
<li>‚úÖ Reliable reasoning (same input = same output)</li>
<li>‚úÖ Easier to debug (reproducible behavior)</li>
<li>‚úÖ Safer in production (no surprises)</li>
<p>---</p>
<h2>üéØ <strong>Practical Examples</strong></h2>
<h3><strong>Example 1: Bad vs Good Prompting</strong></h3>
<p>#### <strong>Bad</strong>
<pre><code class="language-python">prompt = &quot;Kubernetes issue&quot;
temperature = 1.5  # Way too high
max_tokens = 10   # Too short</code></pre></p>
<strong>Problems</strong>:
<li>Too vague (what issue?)</li>
<li>High temperature makes it unpredictable</li>
<li>10 tokens can't give meaningful answer</li>
<p>#### <strong>Good</strong>
<pre><code class="language-python">prompt = &quot;&quot;&quot;
You are a K8s expert. Analyze this scenario:</p>
<p>Pod: payment-service-abc123
Namespace: production
Status: CrashLoopBackOff
Recent logs: &quot;Error: Connection timeout to redis-cache:6379&quot;</p>
<p>Step-by-step diagnosis:
&quot;&quot;&quot;</p>
<p>temperature = 0.0  # Deterministic
max_tokens = 500   # Enough for thorough answer</code></pre></p>
<h3><strong>Example 2: Tool Calling with Low Temperature</strong></h3>
<pre><code class="language-python">from langchain.chat_models import ChatOpenAI
from langchain.agents import create_react_agent, AgentExecutor
<p>llm = ChatOpenAI(
    model=&quot;gpt-4&quot;,
    temperature=0.0  # ‚≠ê Critical for agents
)</p>
<p>agent = create_react_agent(llm, tools, prompt_template)
agent_executor = AgentExecutor(agent=agent, tools=tools)</p>
<h1>This will consistently call the right tools</h1>
result = agent_executor.invoke({
    &quot;input&quot;: &quot;Why is pod nginx-abc in CrashLoopBackOff?&quot;
})</code></pre>
<strong>With temperature=0.0</strong>:
<li>Run 1: GetPodStatus ‚Üí GetPodLogs ‚Üí DescribePod</li>
<li>Run 2: GetPodStatus ‚Üí GetPodLogs ‚Üí DescribePod (same!)</li>
<strong>With temperature=1.0</strong>:
<li>Run 1: GetPodStatus ‚Üí GetPodLogs ‚Üí DescribePod</li>
<li>Run 2: DescribePod ‚Üí GetPodStatus ‚Üí CheckResources (different!)</li>
<h3><strong>Example 3: Prompt Engineering for Error Analysis</strong></h3>
<pre><code class="language-python">error_analysis_prompt = &quot;&quot;&quot;
You are analyzing Kubernetes errors. For each error:
1. Identify the error type
2. Explain the root cause
3. Suggest 2-3 fixes
4. Rank fixes by likelihood of success
<p>Error log:
{log_content}</p>
<p>Analysis:
&quot;&quot;&quot;</p>
<p>response = llm.invoke(
    error_analysis_prompt.format(log_content=pod_logs),
    temperature=0.2  # Low but not 0 for slight variation in explanations
)</code></pre></p>
<p>---</p>
<h2>‚ö†Ô∏è <strong>Common Prompting Mistakes</strong></h2>
<h3><strong>Mistake 1: Too Vague</strong></h3>
<p>‚ùå <strong>Bad</strong>: "Fix my app"  
‚úÖ <strong>Good</strong>: "Pod cart-service-xyz is failing with exit code 137 (OOMKilled). Memory limit is 128Mi. How should I fix this?"</p>
<h3><strong>Mistake 2: Wrong Temperature</strong></h3>
<p>‚ùå <strong>Bad</strong>: Using temperature=1.0 for tool calling (inconsistent tool selection)  
‚úÖ <strong>Good</strong>: Using temperature=0.0 for agents, 0.7 for explanations</p>
<h3><strong>Mistake 3: No Context</strong></h3>
<p>‚ùå <strong>Bad</strong>: "What's wrong?"  
‚úÖ <strong>Good</strong>: "Previous message said pod is Pending. Now checking: kubectl describe pod shows 'Insufficient CPU'. What should I do?"</p>
<h3><strong>Mistake 4: Ignoring Format</strong></h3>
<p>‚ùå <strong>Bad</strong>: Expecting JSON but not asking for it  
‚úÖ <strong>Good</strong>: Explicitly stating "Respond in JSON format: {field1: ..., field2: ...}"</p>
<h3><strong>Mistake 5: Not Testing</strong></h3>
<p>‚ùå <strong>Bad</strong>: Using same prompt/temperature in production without testing  
‚úÖ <strong>Good</strong>: Testing with multiple examples, adjusting temperature based on results</p>
<p>---</p>
<h2>üß™ <strong>Hands-On Practice</strong></h2>
<h3><strong>Exercise 1: Temperature Experiment</strong></h3>
<p>Try this with different temperatures:</p>
<pre><code class="language-python">from openai import OpenAI
client = OpenAI()
<p>prompt = &quot;List 5 common Kubernetes issues&quot;</p>
<p>for temp in [0.0, 0.5, 1.0]:
    print(f&quot;\n--- Temperature: {temp} ---&quot;)
    response = client.chat.completions.create(
        model=&quot;gpt-4&quot;,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
        temperature=temp
    )
    print(response.choices[0].message.content)</code></pre></p>
<strong>Observe</strong>: At 0.0, same output every time. At 1.0, varied outputs.
<h3><strong>Exercise 2: Prompt Improvement</strong></h3>
<p>Improve this prompt:</p>
<p>‚ùå <strong>Bad</strong>:
<pre><code class="language-python">&quot;Help with Kubernetes&quot;</code></pre></p>
<p>‚úÖ <strong>Better</strong>: (You write this!)</p>
<details>
<summary>Show Solution</summary>
<pre><code class="language-python">&quot;&quot;&quot;
I&#039;m troubleshooting a Kubernetes issue:
<li>Pod name: api-gateway-def456</li>
<li>Namespace: staging</li>
<li>Status: ImagePullBackOff</li>
<li>Error message: &quot;Failed to pull image &#039;myregistry.io/api:v2.0.1&#039;: </li>
  authentication required&quot;
<p>Please help diagnose:
1. What&#039;s causing this?
2. How do I verify the registry credentials?
3. What&#039;s the fix?
&quot;&quot;&quot;</code></pre></p>
</details>
<h3><strong>Exercise 3: System Prompt for Agent</strong></h3>
<p>Write a system prompt for your K8s agent. Include:
<li>Role definition</li>
<li>Guidelines</li>
<li>Tool descriptions</li>
<li>Expected behavior</li></p>
<details>
<summary>Show Solution</summary>
<pre><code class="language-python">system_prompt = &quot;&quot;&quot;
You are an AI-powered Kubernetes troubleshooting assistant with expertise in 
cluster diagnostics and issue resolution.
<p>Your capabilities:
<li>Analyze pod failures, resource issues, networking problems</li>
<li>Use kubectl-based tools to gather real cluster data</li>
<li>Provide step-by-step reasoning for all diagnoses</li>
<li>Suggest actionable fixes ranked by likelihood of success</li></p>
<p>Available tools:
1. GetPodStatus(pod_name, namespace): Check pod state (Running, Pending, etc.)
2. GetPodLogs(pod_name, namespace, tail): Retrieve recent logs
3. DescribePod(pod_name, namespace): Get detailed config and events
4. AnalyzeErrors(log_text): Extract and explain errors
5. CheckResources(pod_name, namespace): Verify CPU/memory limits/usage</p>
<p>Guidelines:
<li>Always use tools to gather data before concluding</li>
<li>Think step-by-step: observe ‚Üí hypothesize ‚Üí verify ‚Üí conclude</li>
<li>If stuck, use DescribePod to see events</li>
<li>Explain your reasoning at each step</li>
<li>Provide 2-3 potential fixes, ranked by likelihood</li>
<li>If you need more info, ask specific follow-up questions</li>
<li>Never guess - if unsure, gather more data with tools</li></p>
<p>Response format:
1. Current Status: [What you observed]
2. Analysis: [Step-by-step reasoning]
3. Root Cause: [Conclusion]
4. Recommended Fixes: [Actionable steps]
&quot;&quot;&quot;</code></pre></p>
</details>
<p>---</p>
<h2>üìä <strong>Cheat Sheet: Temperature Guide</strong></h2>
<p>| Use Case | Temperature | Why |
|----------|-------------|-----|
| Code generation | 0.0 - 0.2 | Need correct syntax |
| Data extraction | 0.0 - 0.1 | Need accuracy |
| Tool calling (agents) | 0.0 - 0.3 | Need consistency |
| <strong>K8s agent</strong> | <strong>0.0</strong> | <strong>Reliable diagnostics</strong> |
| Explanations | 0.5 - 0.7 | Natural language |
| Chatbots | 0.6 - 0.8 | Varied responses |
| Creative writing | 0.8 - 1.0 | Imagination |
| Brainstorming | 0.7 - 1.0 | Diverse ideas |</p>
<p>---</p>
<h2>üéì <strong>Self-Check Questions</strong></h2>
<h3><strong>Question 1</strong>: What temperature should you use for your K8s troubleshooting agent?</h3>
<details>
<summary>Show Answer</summary>
<strong>0.0 or very close to it (0.0-0.2)</strong>
<strong>Why</strong>: You want deterministic, consistent tool calling. If the agent sees the same error twice, it should take the same diagnostic steps. Higher temperature would cause unpredictable behavior (calling different tools, inconsistent reasoning).
</details>
<h3><strong>Question 2</strong>: When would you use temperature 0.8-1.0?</h3>
<details>
<summary>Show Answer</summary>
<strong>Creative tasks</strong>: Writing stories, generating marketing copy, brainstorming ideas, humor generation.
<strong>NOT for</strong>: Code generation, data extraction, troubleshooting, tool calling, production systems.
</details>
<h3><strong>Question 3</strong>: What's the difference between zero-shot and few-shot prompting?</h3>
<details>
<summary>Show Answer</summary>
<li><strong>Zero-shot</strong>: Ask the model without providing examples</li>
  - Example: "Classify this sentiment: 'I love it!'"
  
<li><strong>Few-shot</strong>: Provide 2-3 examples before asking</li>
  - Example: "Positive: 'Great!' | Negative: 'Terrible.' | Now classify: 'Love it!'"
<strong>When to use few-shot</strong>: Complex tasks, specific formats, need higher accuracy, model struggles with zero-shot.
</details>
<h3><strong>Question 4</strong>: Write a prompt that would work well with temperature=0.0</h3>
<details>
<summary>Show Answer</summary>
<pre><code class="language-python">&quot;&quot;&quot;
Extract the following information from the log entry:
<li>Timestamp</li>
<li>Pod name</li>
<li>Error message</li>
<p>Log entry:
&quot;2025-12-09 14:32:15 [ERROR] Pod nginx-abc123: Connection refused to database&quot;</p>
<p>Respond in JSON format:
{
  &quot;timestamp&quot;: &quot;...&quot;,
  &quot;pod_name&quot;: &quot;...&quot;,
  &quot;error_message&quot;: &quot;...&quot;
}
&quot;&quot;&quot;</code></pre></p>
<strong>Why this works at temp=0.0</strong>: Precise extraction task, structured output, no creativity needed.
</details>
<h3><strong>Question 5</strong>: Why is it bad to use temperature=1.0 for an agent that calls tools?</h3>
<details>
<summary>Show Answer</summary>
<strong>Problems</strong>:
1. <strong>Inconsistent tool selection</strong>: Same error might trigger different tools each time
2. <strong>Unreliable reasoning</strong>: Agent's thought process varies unpredictably
3. <strong>Harder to debug</strong>: Can't reproduce issues
4. <strong>Production risk</strong>: Might call wrong tool, give bad advice
5. <strong>User confusion</strong>: Same question gets different diagnostic approaches
<strong>Example</strong>:
<li>Run 1 (temp=1.0): GetPodStatus ‚Üí GetPodLogs ‚Üí success</li>
<li>Run 2 (temp=1.0): DescribePod ‚Üí CheckResources ‚Üí missed the actual error</li></ul>
<p>With temp=0.0, both runs would take the same path.</p>
</details>
<p>---</p>
<h2>üöÄ <strong>Key Takeaways</strong></h2>
<p>1. <strong>Prompting is critical</strong>: Same model, different prompts = vastly different results
2. <strong>Temperature controls randomness</strong>: 0.0 = deterministic, 1.0 = creative
3. <strong>For agents, use temp=0.0</strong>: Ensures consistent tool calling and reasoning
4. <strong>Provide context</strong>: Better prompts = better outputs
5. <strong>System prompts set behavior</strong>: Define role, guidelines, and expectations
6. <strong>Test different temperatures</strong>: Find what works best for your use case
7. <strong>Use patterns</strong>: Zero-shot, few-shot, CoT, role prompting, constrained output</p>
<p>---</p>
<h2>üîó <strong>Next Module</strong></h2>
<p>Move on to <strong>Module 4: Chain-of-Thought vs ReAct</strong> to understand advanced reasoning patterns!</p>
<p>---</p>
<strong>Time to complete this module</strong>: 45 minutes  
<strong>Hands-on practice</strong>: 15-20 minutes  
<strong>Total</strong>: ~1 hour

    </div>
    

    <div class="module-content" id="module-4">
        <h1>Module 4: Chain-of-Thought vs ReAct Pattern</h1>
<strong>Time:</strong> 45 minutes  
<strong>Goal:</strong> Understand different reasoning patterns and why ReAct is perfect for your K8s agent
<p>---</p>
<h2>üß† <strong>Reasoning Patterns in AI</strong></h2>
<h3><strong>The Evolution</strong></h3>
<pre><code class="language-text">2018: Direct Prompting
&quot;What&#039;s 2+2?&quot; ‚Üí &quot;4&quot;
Problem: No reasoning, just pattern matching
<p>2022: Chain-of-Thought (CoT)
&quot;What&#039;s 2+2? Let&#039;s think step by step&quot;
‚Üí &quot;2+2 means adding 2 and 2. That equals 4.&quot;
Better: Shows reasoning!</p>
<p>2023: ReAct (Reasoning + Acting)
&quot;What&#039;s 2+2? You have a calculator tool.&quot;
‚Üí &quot;I should use the calculator. Action: calculate(2+2). Result: 4.&quot;
Best: Can use tools to verify!</code></pre></p>
<p>---</p>
<h2>üîó <strong>Chain-of-Thought (CoT)</strong></h2>
<h3><strong>What is CoT?</strong></h3>
<pre><code class="language-text">Chain-of-Thought = Making LLM show its reasoning steps
<p>Without CoT:
User: &quot;Is the pod healthy?&quot;
LLM: &quot;No&quot;</p>
<p>With CoT:
User: &quot;Is the pod healthy? Think step by step.&quot;
LLM: &quot;Let me analyze:
1. Check pod status ‚Üí CrashLoopBackOff
2. This means container is crashing
3. Crashing = not healthy
Therefore: No, the pod is not healthy.&quot;</p>
<p>Better because: You can see the reasoning, catch errors</code></pre></p>
<h3><strong>CoT Patterns</strong></h3>
<p>#### <strong>Pattern 1: Zero-Shot CoT</strong>
<pre><code class="language-text">User: &quot;Why is nginx failing? Let&#039;s think step by step.&quot;</p>
<p>LLM:
&quot;Step 1: Check if pod is running
Step 2: Look at pod status
Step 3: Check for errors in logs
Step 4: Identify root cause</p>
<p>Without looking at actual data, common causes could be:
<ul><li>Port conflicts</li>
<li>Image issues</li>
<li>Resource limits&quot;</li></p>
<p>Problem: LLM is GUESSING! No real data!</code></pre></p>
<p>#### <strong>Pattern 2: Few-Shot CoT</strong>
<pre><code class="language-text">User: &quot;Here are examples of good reasoning:</p>
<p>Example 1:
Problem: API pod failing
Reasoning: Check status ‚Üí CrashLoopBackOff ‚Üí Check logs ‚Üí Port 3000 in use ‚Üí Conclusion: Port conflict</p>
<p>Example 2:
Problem: DB pod failing  
Reasoning: Check status ‚Üí OOMKilled ‚Üí Check memory limit ‚Üí 512Mi ‚Üí Conclusion: Need more memory</p>
<p>Now analyze: nginx pod failing&quot;</p>
<p>LLM: &quot;Let me follow the same pattern:
Check status ‚Üí CrashLoopBackOff
Check logs ‚Üí Error: listen tcp :80: bind: address already in use
Conclusion: Port 80 conflict&quot;</p>
<p>Better! LLM learned the reasoning pattern!</code></pre></p>
<p>---</p>
<h2>‚öõÔ∏è <strong>ReAct Pattern (What You're Using!)</strong></h2>
<h3><strong>What is ReAct?</strong></h3>
<pre><code class="language-text">ReAct = Reasoning + Acting
Combines thinking with actions (tool calling)
<p>Pattern:
Thought ‚Üí Action ‚Üí Observation ‚Üí Thought ‚Üí Action ‚Üí ...</p>
<p>Real example:
Thought: &quot;I need to check pod status&quot;
Action: GetPodStatus(namespace=&quot;default&quot;, pod_name=&quot;nginx&quot;)
Observation: {&quot;status&quot;: &quot;CrashLoopBackOff&quot;, &quot;restarts&quot;: 15}
Thought: &quot;It&#039;s crashing, I should check logs&quot;
Action: GetPodLogs(namespace=&quot;default&quot;, pod_name=&quot;nginx&quot;)
Observation: &quot;Error: listen tcp :80: bind: address already in use&quot;
Thought: &quot;Port conflict! I now know the issue&quot;
Final Answer: &quot;nginx is crashing due to port 80 conflict&quot;</code></pre></p>
<h3><strong>Why ReAct is Better than CoT</strong></h3>
<p>| Aspect | Chain-of-Thought | ReAct |
|--------|------------------|-------|
| <strong>Can it verify facts?</strong> | ‚ùå No, just reasoning | ‚úÖ Yes, calls tools! |
| <strong>Can it interact with systems?</strong> | ‚ùå No | ‚úÖ Yes (kubectl, APIs) |
| <strong>Can it get real data?</strong> | ‚ùå No, uses training data | ‚úÖ Yes, live data |
| <strong>Accuracy</strong> | Medium (might hallucinate) | High (facts from tools) |
| <strong>Use case</strong> | Explaining concepts | Diagnosing real issues |</p>
<strong>For your K8s agent:</strong> ReAct is the ONLY choice! You need real cluster data.
<p>---</p>
<h2>üîÑ <strong>ReAct Loop in Detail</strong></h2>
<h3><strong>The Full Cycle</strong></h3>
<pre><code class="language-python"># This is what happens inside the agent
<p>while not finished and iterations &lt; max_iterations:
    # 1. REASONING PHASE
    prompt = f&quot;&quot;&quot;
    Question: {user_question}
    
    Tools available:
    - GetPodStatus: Check if pod is running
    - GetPodLogs: Read container logs
    - DescribePod: Get detailed pod info
    
    Previous steps:
    {scratchpad}
    
    What should you do next?
    Format: Thought: ... Action: ... Action Input: ...
    &quot;&quot;&quot;
    
    response = llm.generate(prompt)
    # &quot;Thought: I need pod status. Action: GetPodStatus. Action Input: nginx&quot;
    
    # 2. PARSE RESPONSE
    thought = extract_thought(response)
    action = extract_action(response)  # &quot;GetPodStatus&quot;
    action_input = extract_input(response)  # &quot;nginx&quot;
    
    # 3. EXECUTION PHASE
    if action == &quot;GetPodStatus&quot;:
        observation = get_pod_status_tool(action_input)
    elif action == &quot;GetPodLogs&quot;:
        observation = get_pod_logs_tool(action_input)
    # ... other tools
    
    # 4. UPDATE SCRATCHPAD (memory)
    scratchpad += f&quot;\nThought: {thought}&quot;
    scratchpad += f&quot;\nAction: {action}&quot;
    scratchpad += f&quot;\nAction Input: {action_input}&quot;
    scratchpad += f&quot;\nObservation: {observation}&quot;
    
    # 5. CHECK IF DONE
    if &quot;Final Answer:&quot; in response:
        finished = True
        return extract_final_answer(response)
    
    iterations += 1</p>
<h1>Loop continues until agent says &quot;Final Answer&quot; or max iterations</code></pre></h1>
<p>---</p>
<h2>üéØ <strong>ReAct Prompt Template</strong></h2>
<h3><strong>The System Prompt</strong></h3>
<pre><code class="language-text">You are a Kubernetes troubleshooting expert.
<p>Answer the user&#039;s question as best you can. You have access to the following tools:</p>
<p>GetPodStatus: Check if a pod is running, pending, or failed. Input: pod_name
GetPodLogs: Get logs from a pod&#039;s container. Input: pod_name
DescribePod: Get detailed information about a pod including events. Input: pod_name
SearchErrors: Search for common error patterns. Input: error_text</p>
<p>Use the following format:</p>
<p>Thought: What do I need to find out?
Action: tool_name
Action Input: input_to_tool
Observation: result from tool
... (repeat Thought/Action/Observation as needed)
Thought: I now know the final answer
Final Answer: comprehensive answer to the original question</p>
<p>Begin! Remember to always think before acting.</p>
<p>Question: {user_question}
{agent_scratchpad}</code></pre></p>
<h3><strong>Example Execution</strong></h3>
<pre><code class="language-text">User: &quot;Why is my nginx pod in production namespace failing?&quot;
<p>Agent:
Thought: I should check the pod status first to see what state it&#039;s in
Action: GetPodStatus
Action Input: nginx, production
Observation: Pod nginx in namespace production is in CrashLoopBackOff with 15 restarts</p>
<p>Thought: The pod is crashing repeatedly. I need to check the logs to see why
Action: GetPodLogs
Action Input: nginx, production
Observation: 
2024-12-08 10:30:15 nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)
2024-12-08 10:30:15 nginx: [emerg] still could not bind()</p>
<p>Thought: Port 80 is already in use. Let me check what else is using that port
Action: DescribePod
Action Input: nginx, production
Observation:
Events:
<li>Warning BackOff: Back-off restarting failed container</li>
<li>Normal Started: Started container nginx</li>
<li>Warning Failed: Error: failed to create containerd task</li></ul></p>
<p>Thought: I now have enough information. The nginx pod is crashing because port 80 is already bound by another process
Final Answer: Your nginx pod is failing because it cannot bind to port 80 - another process is already using that port. This is why you see &quot;Address already in use&quot; errors and the pod is in CrashLoopBackOff. Solutions: 1) Stop the conflicting service using port 80, 2) Configure nginx to use a different port, or 3) Use a service/ingress instead of hostPort.</code></pre></p>
<p>---</p>
<h2>üîß <strong>Building a ReAct Agent (LangChain)</strong></h2>
<h3><strong>Step 1: Define Tools</strong></h3>
<pre><code class="language-python">from langchain.agents import Tool
<p>def get_pod_status(pod_name: str) -&gt; str:
    &quot;&quot;&quot;Get status of a Kubernetes pod&quot;&quot;&quot;
    result = subprocess.run(
        [&quot;kubectl&quot;, &quot;get&quot;, &quot;pod&quot;, pod_name, &quot;-o&quot;, &quot;json&quot;],
        capture_output=True, text=True
    )
    if result.returncode != 0:
        return f&quot;Error: {result.stderr}&quot;
    
    pod_data = json.loads(result.stdout)
    status = pod_data[&quot;status&quot;][&quot;phase&quot;]
    return f&quot;Pod {pod_name} status: {status}&quot;</p>
<p>def get_pod_logs(pod_name: str) -&gt; str:
    &quot;&quot;&quot;Get logs from a pod&quot;&quot;&quot;
    result = subprocess.run(
        [&quot;kubectl&quot;, &quot;logs&quot;, pod_name, &quot;--tail=50&quot;],
        capture_output=True, text=True
    )
    return result.stdout or result.stderr</p>
<h1>Create tools</h1>
tools = [
    Tool(
        name=&quot;GetPodStatus&quot;,
        func=get_pod_status,
        description=&quot;Get the current status of a Kubernetes pod. Input should be the pod name.&quot;
    ),
    Tool(
        name=&quot;GetPodLogs&quot;,
        func=get_pod_logs,
        description=&quot;Get logs from a pod to see errors. Input should be the pod name.&quot;
    )
]</code></pre>
<h3><strong>Step 2: Create Agent</strong></h3>
<pre><code class="language-python">from langchain.agents import create_react_agent, AgentExecutor
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
<h1>Initialize LLM</h1>
llm = Ollama(model=&quot;llama3&quot;, temperature=0.0)
<h1>ReAct prompt template</h1>
react_prompt = PromptTemplate.from_template(&quot;&quot;&quot;
Answer the following question. You have access to these tools:
<p>{tools}</p>
<p>Use this format:
Thought: reasoning about what to do
Action: tool name
Action Input: input to the tool
Observation: result from tool
... (repeat as needed)
Thought: I now know the answer
Final Answer: the final answer</p>
<p>Question: {input}
{agent_scratchpad}
&quot;&quot;&quot;)</p>
<h1>Create agent</h1>
agent = create_react_agent(
    llm=llm,
    tools=tools,
    prompt=react_prompt
)
<h1>Wrap in executor</h1>
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,  # Shows reasoning steps!
    max_iterations=5,  # Prevent infinite loops
    handle_parsing_errors=True  # Graceful error handling
)
<h1>Use it!</h1>
result = agent_executor.invoke({
    &quot;input&quot;: &quot;Why is nginx pod failing?&quot;
})
print(result[&quot;output&quot;])</code></pre>
<p>---</p>
<h2>‚öôÔ∏è <strong>Important ReAct Configurations</strong></h2>
<h3><strong>1. Max Iterations</strong></h3>
<pre><code class="language-python">agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    max_iterations=5  # Stop after 5 tool calls
)
<h1>Why needed:</h1>
<h1>Prevent infinite loops if agent gets stuck</h1>
<h1>&quot;Check pod ‚Üí Check logs ‚Üí Check pod ‚Üí Check logs ‚Üí ...&quot;</h1>
<h1>Good value: 5-10 for most tasks</h1>
<h1>Too low (1-2): Agent can&#039;t gather enough info</h1>
<h1>Too high (20+): Expensive, might loop</code></pre></h1>
<h3><strong>2. Temperature</strong></h3>
<pre><code class="language-python">llm = Ollama(model=&quot;llama3&quot;, temperature=0.0)
<h1>For ReAct agents: Use LOW temperature (0.0 - 0.3)</h1>
<h1>Why: Need consistent, deterministic tool selection</h1>
<h1>High temperature ‚Üí Agent might randomly pick wrong tools</code></pre></h1>
<h3><strong>3. Error Handling</strong></h3>
<pre><code class="language-python">agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    handle_parsing_errors=True,  # If agent outputs wrong format
    return_intermediate_steps=True  # Get reasoning steps
)
<h1>handle_parsing_errors: If agent&#039;s output doesn&#039;t match format</h1>
<h1>return_intermediate_steps: Access full thought process</code></pre></h1>
<p>---</p>
<h2>üìä <strong>CoT vs ReAct: When to Use What</strong></h2>
<p>| Use Case | Best Pattern | Why |
|----------|--------------|-----|
| <strong>K8s Troubleshooting</strong> | ‚úÖ ReAct | Need real cluster data |
| <strong>Code Debugging</strong> | ‚úÖ ReAct | Need to run code, check outputs |
| <strong>Math Problems</strong> | ‚úÖ ReAct | Give calculator tool |
| <strong>Explaining Concepts</strong> | CoT | Just needs reasoning, no tools |
| <strong>Writing Documentation</strong> | CoT | Pure text generation |
| <strong>Answering Trivia</strong> | Neither | Direct prompting is fine |</p>
<p>---</p>
<h2>üìù <strong>Self-Check Questions</strong></h2>
<p>1. <strong>What's the key difference between CoT and ReAct?</strong>
   <details>
   <summary>Answer</summary>
   CoT only reasons with training data. ReAct can call tools to get real data and interact with systems.
   </details></p>
<p>2. <strong>Draw the ReAct loop from memory</strong>
   <details>
   <summary>Answer</summary>
   Thought ‚Üí Action ‚Üí Observation ‚Üí Thought ‚Üí Action ‚Üí Observation ‚Üí ... ‚Üí Final Answer
   </details></p>
<p>3. <strong>Why use temperature=0.0 for ReAct agents?</strong>
   <details>
   <summary>Answer</summary>
   Need deterministic, consistent tool selection. High temperature causes random tool choices.
   </details></p>
<p>4. <strong>What happens if you don't set max_iterations?</strong>
   <details>
   <summary>Answer</summary>
   Agent might loop forever: "Check pod ‚Üí Check logs ‚Üí Check pod ‚Üí ...". Wastes tokens and money.
   </details></p>
<p>5. <strong>When would you use CoT instead of ReAct?</strong>
   <details>
   <summary>Answer</summary>
   When no external tools needed - just explaining, reasoning, or generating text.
   </details></p>
<p>---</p>
<h2>üéì <strong>Key Takeaways</strong></h2>
<p>‚úÖ ReAct = Reasoning + Acting (thinking + tool use)  
‚úÖ ReAct is essential for interacting with real systems  
‚úÖ Loop: Thought ‚Üí Action ‚Üí Observation (repeat)  
‚úÖ Use max_iterations to prevent infinite loops  
‚úÖ Use low temperature for consistent tool selection  
‚úÖ CoT for reasoning only, ReAct when tools needed</p>
<p>---</p>
<h2>üöÄ <strong>Next Module</strong></h2>
<p>Ready to understand function calling in depth?</p>
<strong>‚Üí Continue to `05_Tool_Calling_Function_Calling.md`</strong>

    </div>
    

    <div class="module-content" id="module-5">
        <h1>Module 5: Tool Calling & Function Calling</h1>
<strong>Time:</strong> 1 hour  
<strong>Goal:</strong> Master how LLMs interact with external systems through tools
<p>---</p>
<h2>üîß <strong>What is Tool Calling?</strong></h2>
<h3><strong>The Problem</strong></h3>
<pre><code class="language-text">LLMs can only generate text. They CANNOT:
‚ùå Run kubectl commands
‚ùå Query databases
‚ùå Call APIs
‚ùå Read files
‚ùå Do math accurately
<p>But we NEED them to do these things!</code></pre></p>
<h3><strong>The Solution: Tool Calling</strong></h3>
<pre><code class="language-text">Tool Calling = Giving LLM the ability to request function executions
<p>Process:
1. LLM decides: &quot;I need to run kubectl get pods&quot;
2. LLM outputs: {&quot;tool&quot;: &quot;GetPods&quot;, &quot;input&quot;: {&quot;namespace&quot;: &quot;default&quot;}}
3. YOUR CODE executes the actual kubectl command
4. YOUR CODE returns result to LLM
5. LLM sees result, decides next step</p>
<p>LLM doesn&#039;t execute - it just REQUESTS execution!</code></pre></p>
<p>---</p>
<h2>üèóÔ∏è <strong>Anatomy of a Tool</strong></h2>
<h3><strong>Every Tool Has 3 Parts</strong></h3>
<pre><code class="language-python">from langchain.tools import Tool
<p>def my_tool_function(input_param: str) -&gt; str:
    &quot;&quot;&quot;The actual Python function that does the work&quot;&quot;&quot;
    result = do_something(input_param)
    return result</p>
<p>tool = Tool(
    name=&quot;ToolName&quot;,  # 1Ô∏è‚É£ NAME (what LLM calls it)
    
    func=my_tool_function,  # 2Ô∏è‚É£ FUNCTION (what gets executed)
    
    description=&quot;&quot;&quot;  # 3Ô∏è‚É£ DESCRIPTION (how LLM knows when to use it)
    This tool does X. Use it when you need to Y.
    Input should be Z format.
    &quot;&quot;&quot;
)</code></pre></p>
<h3><strong>Example: GetPodStatus Tool</strong></h3>
<pre><code class="language-python">import subprocess
import json
from langchain.tools import Tool
<h1>The actual function</h1>
def get_pod_status(pod_name: str) -&gt; str:
    &quot;&quot;&quot;Get status of a Kubernetes pod&quot;&quot;&quot;
    try:
        result = subprocess.run(
            [&quot;kubectl&quot;, &quot;get&quot;, &quot;pod&quot;, pod_name, &quot;-o&quot;, &quot;json&quot;],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if result.returncode != 0:
            return f&quot;Error: Pod not found or kubectl error: {result.stderr}&quot;
        
        pod_data = json.loads(result.stdout)
        status = pod_data[&quot;status&quot;][&quot;phase&quot;]
        restarts = pod_data[&quot;status&quot;][&quot;containerStatuses&quot;][0][&quot;restartCount&quot;]
        
        return f&quot;Pod &#039;{pod_name}&#039; is {status} with {restarts} restarts&quot;
        
    except Exception as e:
        return f&quot;Error checking pod status: {str(e)}&quot;
<h1>Wrap as Tool</h1>
get_pod_status_tool = Tool(
    name=&quot;GetPodStatus&quot;,
    
    func=get_pod_status,
    
    description=&quot;&quot;&quot;
    Get the current status of a Kubernetes pod.
    Use this when you need to check if a pod is Running, Pending, Failed, or CrashLoopBackOff.
    Input: Just the pod name as a string (e.g., &#039;nginx&#039; or &#039;api-service&#039;)
    Output: Status and restart count
    &quot;&quot;&quot;
)</code></pre>
<p>---</p>
<h2>üéØ <strong>Writing GOOD Tool Descriptions</strong></h2>
<h3><strong>Bad vs Good Descriptions</strong></h3>
<p>#### <strong>‚ùå Bad Description</strong>
<pre><code class="language-python">Tool(
    name=&quot;GetPods&quot;,
    func=get_pods,
    description=&quot;Gets pods&quot;
)</p>
<h1>Problem: LLM doesn&#039;t know:</h1>
<h1>- WHEN to use it</h1>
<h1>- WHAT input format</h1>
<h1>- WHAT it returns</code></pre></h1>
<p>#### <strong>‚úÖ Good Description</strong>
<pre><code class="language-python">Tool(
    name=&quot;GetPods&quot;,
    func=get_pods,
    description=&quot;&quot;&quot;
    Get list of all pods in a Kubernetes namespace.
    
    Use this when:
    - User asks to &quot;list pods&quot; or &quot;show all pods&quot;
    - Need to find a pod by name
    - Want overview of cluster state
    
    Input format: namespace name as string (e.g., &quot;default&quot;, &quot;production&quot;)
    
    Output: JSON list of pods with names, status, and age
    
    Example:
    Input: &quot;default&quot;
    Output: [{&quot;name&quot;: &quot;nginx&quot;, &quot;status&quot;: &quot;Running&quot;, &quot;age&quot;: &quot;2d&quot;}]
    &quot;&quot;&quot;
)</p>
<h1>LLM now knows exactly when and how to use this tool!</code></pre></h1>
<h3><strong>Description Template</strong></h3>
<pre><code class="language-text">Use this template for ALL your tools:
<p>&quot;&quot;&quot;
[ONE SENTENCE: What this tool does]</p>
<p>Use this when:
<ul><li>[Scenario 1]</li>
<li>[Scenario 2]</li>
<li>[Scenario 3]</li></p>
<p>Input format: [Exact format expected]</p>
<p>Output: [What it returns]</p>
<p>Example:
Input: [Example input]
Output: [Example output]</p>
<p>Special notes: [Any warnings or gotchas]
&quot;&quot;&quot;</code></pre></p>
<p>---</p>
<h2>üîÑ <strong>How Tool Calling Works (Under the Hood)</strong></h2>
<h3><strong>Step-by-Step Flow</strong></h3>
<pre><code class="language-text">1. User asks: &quot;Is nginx pod running?&quot;
<p>2. Agent receives tools:
   - GetPodStatus(pod_name) ‚Üí Check pod status
   - GetPodLogs(pod_name) ‚Üí Get logs
   - DescribePod(pod_name) ‚Üí Get details</p>
<p>3. Agent generates:
   {
     &quot;thought&quot;: &quot;I need to check pod status&quot;,
     &quot;action&quot;: &quot;GetPodStatus&quot;,
     &quot;action_input&quot;: &quot;nginx&quot;
   }</p>
<p>4. LangChain parser extracts:
   - Tool name: &quot;GetPodStatus&quot;
   - Input: &quot;nginx&quot;</p>
<p>5. LangChain finds matching tool and executes:
   result = get_pod_status(&quot;nginx&quot;)</p>
<p>6. Tool returns:
   &quot;Pod &#039;nginx&#039; is Running with 0 restarts&quot;</p>
<p>7. This is fed back to agent as &quot;Observation&quot;</p>
<p>8. Agent decides next step:
   - Need more info? Call another tool
   - Have enough info? Generate Final Answer</code></pre></p>
<h3><strong>The Prompt the LLM Sees</strong></h3>
<pre><code class="language-text">You have access to these tools:
<p>GetPodStatus: Get the current status of a Kubernetes pod...
GetPodLogs: Get logs from a pod...</p>
<p>Question: Is nginx pod running?</p>
<p>Thought: I need to check the pod status
Action: GetPodStatus
Action Input: nginx
Observation: Pod &#039;nginx&#039; is Running with 0 restarts</p>
<p>Thought: I now know the answer
Final Answer: Yes, the nginx pod is running successfully with no restarts.</code></pre></p>
<p>---</p>
<h2>üèÖ <strong>Advanced Tool Patterns</strong></h2>
<h3><strong>Pattern 1: Structured Tools (Type Safety)</strong></h3>
<pre><code class="language-python">from langchain.tools import StructuredTool
from pydantic import BaseModel, Field
<h1>Define input schema</h1>
class GetPodInput(BaseModel):
    pod_name: str = Field(description=&quot;Name of the pod to check&quot;)
    namespace: str = Field(
        default=&quot;default&quot;,
        description=&quot;Kubernetes namespace&quot;
    )
<p>def get_pod_status_typed(pod_name: str, namespace: str = &quot;default&quot;) -&gt; str:
    &quot;&quot;&quot;Get pod status with namespace support&quot;&quot;&quot;
    result = subprocess.run(
        [&quot;kubectl&quot;, &quot;get&quot;, &quot;pod&quot;, pod_name, &quot;-n&quot;, namespace, &quot;-o&quot;, &quot;json&quot;],
        capture_output=True, text=True
    )
    # ... processing ...
    return status</p>
<h1>Create structured tool</h1>
get_pod_tool = StructuredTool.from_function(
    func=get_pod_status_typed,
    name=&quot;GetPodStatus&quot;,
    description=&quot;Check pod status in a specific namespace&quot;,
    args_schema=GetPodInput  # ‚úÖ Type-safe inputs!
)
<h1>Now LLM must provide correct types:</h1>
<h1>‚úÖ {&quot;pod_name&quot;: &quot;nginx&quot;, &quot;namespace&quot;: &quot;prod&quot;}</h1>
<h1>‚ùå {&quot;pod&quot;: &quot;nginx&quot;}  ‚Üê Will error, missing required field</code></pre></h1>
<h3><strong>Pattern 2: Tool with Error Handling</strong></h3>
<pre><code class="language-python">def safe_get_pod_logs(pod_name: str, lines: int = 50) -&gt; str:
    &quot;&quot;&quot;Get pod logs with comprehensive error handling&quot;&quot;&quot;
    
    # Validation
    if not pod_name:
        return &quot;Error: pod_name is required&quot;
    
    if lines &lt; 1 or lines &gt; 1000:
        return &quot;Error: lines must be between 1 and 1000&quot;
    
    try:
        # Execute command with timeout
        result = subprocess.run(
            [&quot;kubectl&quot;, &quot;logs&quot;, pod_name, f&quot;--tail={lines}&quot;],
            capture_output=True,
            text=True,
            timeout=30  # Don&#039;t hang forever
        )
        
        if result.returncode != 0:
            # Parse kubectl errors
            if &quot;NotFound&quot; in result.stderr:
                return f&quot;Error: Pod &#039;{pod_name}&#039; not found&quot;
            elif &quot;Forbidden&quot; in result.stderr:
                return &quot;Error: Permission denied. Check RBAC settings&quot;
            else:
                return f&quot;Error: {result.stderr}&quot;
        
        if not result.stdout:
            return f&quot;Pod &#039;{pod_name}&#039; has no logs (may be starting)&quot;
        
        return result.stdout
        
    except subprocess.TimeoutExpired:
        return f&quot;Error: Timeout getting logs from pod &#039;{pod_name}&#039;&quot;
    except Exception as e:
        return f&quot;Unexpected error: {str(e)}&quot;
<h1>‚úÖ Robust tool that handles all edge cases!</code></pre></h1>
<h3><strong>Pattern 3: Tool with Caching</strong></h3>
<pre><code class="language-python">from functools import lru_cache
import time
<p>@lru_cache(maxsize=100)
def cached_get_pods(namespace: str, _cache_key: float) -&gt; str:
    &quot;&quot;&quot;Get pods with 30-second cache&quot;&quot;&quot;
    result = subprocess.run(
        [&quot;kubectl&quot;, &quot;get&quot;, &quot;pods&quot;, &quot;-n&quot;, namespace, &quot;-o&quot;, &quot;json&quot;],
        capture_output=True, text=True
    )
    return result.stdout</p>
<p>def get_pods_with_cache(namespace: str = &quot;default&quot;) -&gt; str:
    &quot;&quot;&quot;Public function that adds cache key&quot;&quot;&quot;
    # Cache key changes every 30 seconds
    cache_key = int(time.time() / 30)
    return cached_get_pods(namespace, cache_key)</p>
<h1>Why cache?</h1>
<h1>If agent calls GetPods multiple times in one session,</h1>
<h1>don&#039;t hammer kubectl - use cached result!</code></pre></h1>
<p>---</p>
<h2>üé® <strong>Designing Your Tool Set</strong></h2>
<h3><strong>Your 5 Core Tools</strong></h3>
<pre><code class="language-python">tools = [
    # 1. Status Check (always needed first)
    Tool(
        name=&quot;GetPodStatus&quot;,
        func=get_pod_status,
        description=&quot;Check if pod is Running/Pending/Failed...&quot;
    ),
    
    # 2. Logs (for error messages)
    Tool(
        name=&quot;GetPodLogs&quot;,
        func=get_pod_logs,
        description=&quot;Get container logs to see errors...&quot;
    ),
    
    # 3. Detailed Info (when status isn&#039;t enough)
    Tool(
        name=&quot;DescribePod&quot;,
        func=describe_pod,
        description=&quot;Get full pod details including events...&quot;
    ),
    
    # 4. Pattern Matching (identify known errors)
    Tool(
        name=&quot;AnalyzeErrors&quot;,
        func=analyze_errors,
        description=&quot;Check logs for common error patterns...&quot;
    ),
    
    # 5. Resource Check (for OOMKilled, etc.)
    Tool(
        name=&quot;CheckResources&quot;,
        func=check_resources,
        description=&quot;Check CPU and memory usage...&quot;
    )
]
<h1>Agent uses them in order:</h1>
<h1>1. GetPodStatus ‚Üí See if failing</h1>
<h1>2. GetPodLogs ‚Üí Find error message</h1>
<h1>3. AnalyzeErrors ‚Üí Match to known patterns</h1>
<h1>4. DescribePod ‚Üí If still unclear, get full details</h1>
<h1>5. CheckResources ‚Üí If resource-related issue</code></pre></h1>
<p>---</p>
<h2>üö´ <strong>Common Tool Mistakes</strong></h2>
<h3><strong>Mistake 1: Too Many Tools</strong></h3>
<pre><code class="language-python"># ‚ùå Bad: 20 tools for every kubectl command
tools = [
    &quot;GetPods&quot;, &quot;GetDeployments&quot;, &quot;GetServices&quot;, &quot;GetIngress&quot;,
    &quot;GetConfigMaps&quot;, &quot;GetSecrets&quot;, &quot;GetNodes&quot;, &quot;GetNamespaces&quot;,
    &quot;GetPVCs&quot;, &quot;GetPVs&quot;, &quot;GetStatefulSets&quot;, &quot;GetDaemonSets&quot;,
    ... 8 more
]
<h1>Agent gets confused! Which one to use?</h1>
<h1>More tools = worse performance!</h1>
<h1>‚úÖ Good: 5-7 focused tools</h1>
tools = [
    &quot;GetPodStatus&quot;,
    &quot;GetPodLogs&quot;, 
    &quot;DescribePod&quot;,
    &quot;AnalyzeErrors&quot;,
    &quot;CheckResources&quot;
]
<h1>Clear hierarchy, each tool has specific purpose</code></pre></h1>
<h3><strong>Mistake 2: Vague Descriptions</strong></h3>
<pre><code class="language-python"># ‚ùå Bad
Tool(
    name=&quot;CheckPod&quot;,
    description=&quot;Checks pod&quot;
)
<h1>LLM doesn&#039;t know:</h1>
<h1>- What aspect of pod?</h1>
<h1>- When to use vs other tools?</h1>
<h1>- What input format?</h1>
<h1>‚úÖ Good</h1>
Tool(
    name=&quot;GetPodStatus&quot;,
    description=&quot;&quot;&quot;
    Get current status (Running/Pending/Failed/CrashLoopBackOff).
    Use this FIRST when investigating pod issues.
    Input: pod_name (string)
    Output: Status and restart count
    &quot;&quot;&quot;
)</code></pre>
<h3><strong>Mistake 3: No Error Handling</strong></h3>
<pre><code class="language-python"># ‚ùå Bad
def get_logs(pod_name):
    result = subprocess.run([&quot;kubectl&quot;, &quot;logs&quot;, pod_name])
    return result.stdout  # What if it fails?!
<h1>Crashes agent if pod doesn&#039;t exist</h1>
<h1>No helpful error message</h1>
<h1>‚úÖ Good</h1>
def get_logs(pod_name):
    try:
        result = subprocess.run(
            [&quot;kubectl&quot;, &quot;logs&quot;, pod_name],
            capture_output=True,
            timeout=30
        )
        if result.returncode != 0:
            return f&quot;Error: {result.stderr}&quot;
        return result.stdout or &quot;No logs available&quot;
    except Exception as e:
        return f&quot;Failed to get logs: {e}&quot;
<h1>Agent can handle errors gracefully</code></pre></h1>
<p>---</p>
<h2>üìù <strong>Self-Check Questions</strong></h2>
<p>1. <strong>What are the 3 essential parts of a tool?</strong>
   <details>
   <summary>Answer</summary>
   Name (what LLM calls it), Function (what it executes), Description (when/how to use it)
   </details></p>
<p>2. <strong>Why is tool description so important?</strong>
   <details>
   <summary>Answer</summary>
   LLM decides which tool to use based ONLY on descriptions. Bad description = wrong tool selection = wrong diagnosis.
   </details></p>
<p>3. <strong>What happens if a tool doesn't handle errors?</strong>
   <details>
   <summary>Answer</summary>
   Agent crashes or gets stuck. Need try/except, validate inputs, return error messages as strings.
   </details></p>
<p>4. <strong>Should you create a tool for every kubectl command?</strong>
   <details>
   <summary>Answer</summary>
   No! Too many tools confuse the agent. Focus on 5-7 essential tools that cover common diagnostic workflows.
   </details></p>
<p>5. <strong>What's the difference between Tool and StructuredTool?</strong>
   <details>
   <summary>Answer</summary>
   StructuredTool adds Pydantic schema for type-safe inputs. Better for complex tools with multiple parameters.
   </details></p>
<p>---</p>
<h2>üéì <strong>Key Takeaways</strong></h2>
<p>‚úÖ Tools let LLMs interact with real systems  
‚úÖ Every tool needs name, function, and clear description  
‚úÖ Good descriptions = better tool selection  
‚úÖ Always handle errors gracefully  
‚úÖ 5-7 focused tools > 20 generic tools  
‚úÖ Use StructuredTool for type safety</p>
<p>---</p>
<h2>üöÄ <strong>Ready for Hands-On?</strong></h2>
<p>Can you:
<li>Explain what tool calling is?</li>
<li>Write a tool description that helps LLM choose correctly?</li>
<li>Implement error handling in tools?</li></ul></p>
<strong>‚Üí Continue to `10_Hands_On_Exercises.md` to practice!</strong>

    </div>
    

    <div class="module-content" id="module-6">
        <h1>Module 6: Agent Reasoning Loops</h1>
<strong>Study Time</strong>: ~45 minutes  
<strong>Prerequisites</strong>: Module 4 (CoT vs ReAct), Module 5 (Tool Calling)
<p>---</p>
<h2>üéØ <strong>Learning Objectives</strong></h2>
<p>By the end of this module, you'll understand:
1. How agents make decisions in a loop
2. The anatomy of a reasoning iteration
3. How to control and debug agent loops
4. Common loop problems and solutions
5. Best practices for production agents</p>
<p>---</p>
<h2>üîÑ <strong>What is a Reasoning Loop?</strong></h2>
<p>An <strong>agent reasoning loop</strong> is the iterative process where an agent:
1. <strong>Observes</strong> the current state
2. <strong>Thinks</strong> about what to do next
3. <strong>Acts</strong> by calling a tool or giving a final answer
4. <strong>Observes</strong> the result
5. <strong>Repeats</strong> until the task is complete</p>
<p>Think of it like a doctor diagnosing a patient:
<ul><li>Doctor examines patient (observation)</li>
<li>Doctor thinks "could be infection" (reasoning)</li>
<li>Doctor orders blood test (action/tool call)</li>
<li>Doctor sees elevated white blood cells (observation)</li>
<li>Doctor thinks "confirms infection" (reasoning)</li>
<li>Doctor prescribes antibiotics (final answer)</li></p>
<p>---</p>
<h2>üß† <strong>ReAct Loop: Step-by-Step</strong></h2>
<h3><strong>The Pattern</strong></h3>
<pre><code class="language-text">Thought ‚Üí Action ‚Üí Observation ‚Üí Thought ‚Üí Action ‚Üí Observation ‚Üí Final Answer</code></pre>
<h3><strong>Detailed Example: K8s Troubleshooting</strong></h3>
<strong>User Query</strong>: "Why is pod nginx-abc123 crashing?"
<p>#### <strong>Iteration 1</strong>
<pre><code class="language-text">Thought: &quot;I need to check the pod&#039;s current status first&quot;
Action: GetPodStatus(pod_name=&quot;nginx-abc123&quot;, namespace=&quot;default&quot;)
Observation: &quot;Pod nginx-abc123 is in CrashLoopBackOff state. 
             Last restart: 2 minutes ago. Restart count: 15&quot;</code></pre></p>
<p>#### <strong>Iteration 2</strong>
<pre><code class="language-text">Thought: &quot;CrashLoopBackOff means the container is repeatedly crashing. 
         I should check the logs to see why&quot;
Action: GetPodLogs(pod_name=&quot;nginx-abc123&quot;, namespace=&quot;default&quot;, tail=50)
Observation: &quot;Error: Configuration file /etc/nginx/nginx.conf not found
              nginx: [emerg] cannot load configuration file&quot;</code></pre></p>
<p>#### <strong>Iteration 3</strong>
<pre><code class="language-text">Thought: &quot;The pod is crashing because the nginx config file is missing. 
         Let me check the pod&#039;s configuration to see how this file should be mounted&quot;
Action: DescribePod(pod_name=&quot;nginx-abc123&quot;, namespace=&quot;default&quot;)
Observation: &quot;Pod mounts ConfigMap &#039;nginx-config&#039; at /etc/nginx/
              ConfigMap Status: Not Found
              Event: MountVolume.SetUp failed for volume &#039;nginx-config&#039;: 
              configmap &#039;nginx-config&#039; not found&quot;</code></pre></p>
<p>#### <strong>Final Answer</strong>
<pre><code class="language-text">Thought: &quot;I have enough information to provide a complete diagnosis&quot;
Final Answer: &quot;Pod nginx-abc123 is crashing because it&#039;s trying to mount a 
ConfigMap named &#039;nginx-config&#039; that doesn&#039;t exist.</p>
<p>Root Cause: Missing ConfigMap</p>
<p>Fix:
1. Check if ConfigMap was deleted: kubectl get configmap nginx-config
2. Recreate the ConfigMap with the correct nginx configuration
3. Or update the Deployment to use an existing ConfigMap name</p>
<p>The pod will start successfully once the ConfigMap exists.&quot;</code></pre></p>
<strong>Total Iterations</strong>: 3  
<strong>Tools Called</strong>: 3  
<strong>Result</strong>: Complete diagnosis with actionable fix
<p>---</p>
<h2>‚öôÔ∏è <strong>Anatomy of an Iteration</strong></h2>
<p>Each iteration has 3 components:</p>
<h3><strong>1. Thought (Reasoning)</strong></h3>
<p>The agent's internal reasoning about what to do next.</p>
<pre><code class="language-python"># Example thoughts:
&quot;I need to check the pod status first&quot;
&quot;CrashLoopBackOff suggests repeated failures, should check logs&quot;
&quot;The logs show a config error, need to verify the ConfigMap&quot;
&quot;I have enough information to answer now&quot;</code></pre>
<strong>What makes a good thought</strong>:
<li>‚úÖ Refers to previous observations</li>
<li>‚úÖ Explains why taking the next action</li>
<li>‚úÖ Shows logical progression</li>
<li>‚ùå Doesn't repeat the same thought</li>
<li>‚ùå Doesn't contradict previous reasoning</li>
<h3><strong>2. Action (Tool Call)</strong></h3>
<p>The tool the agent decides to call, with parameters.</p>
<pre><code class="language-python"># Good action:
Action: GetPodLogs(pod_name=&quot;nginx-abc123&quot;, namespace=&quot;default&quot;, tail=50)
<h1>Bad action (wrong parameters):</h1>
Action: GetPodLogs(pod_name=&quot;wrong-name&quot;, namespace=&quot;default&quot;)
<h1>Bad action (wrong tool for the situation):</h1>
Action: CheckResources(pod_name=&quot;nginx-abc123&quot;)  # Doesn&#039;t help with config error</code></pre>
<h3><strong>3. Observation (Tool Result)</strong></h3>
<p>The output returned by the tool.</p>
<pre><code class="language-python"># Tool returns this:
Observation: &quot;Error: Configuration file /etc/nginx/nginx.conf not found&quot;
<h1>Agent reads this and uses it in next Thought</code></pre></h1>
<p>---</p>
<h2>üéÆ <strong>Controlling the Loop</strong></h2>
<h3><strong>max_iterations: Prevent Infinite Loops</strong></h3>
<pre><code class="language-python">from langchain.agents import AgentExecutor
<p>agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    max_iterations=5,  # Stop after 5 iterations max
    verbose=True
)</code></pre></p>
<strong>Why this matters</strong>:
<p>‚ùå <strong>Without max_iterations</strong>:
<pre><code class="language-text">Iteration 1: Check pod status
Iteration 2: Check pod logs
Iteration 3: Check pod status (repeating!)
Iteration 4: Check pod logs (repeating!)
Iteration 5: Check pod status (stuck in loop!)
... [infinite loop]</code></pre></p>
<p>‚úÖ <strong>With max_iterations=5</strong>:
<pre><code class="language-text">Iteration 1: Check pod status
Iteration 2: Check pod logs
Iteration 3: Describe pod
Iteration 4: Analyze error
Iteration 5: Give final answer (forced to stop)</code></pre></p>
<strong>Best Practice</strong>: Set `max_iterations=5-10` based on task complexity.
<h3><strong>early_stopping_method: Graceful Exits</strong></h3>
<pre><code class="language-python">agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    max_iterations=5,
    early_stopping_method=&quot;generate&quot;  # or &quot;force&quot;
)</code></pre>
<strong>Options</strong>:
<p>1. <strong>"generate"</strong> (recommended): Agent generates best answer it can when max_iterations is reached
   <pre><code class="language-text">&quot;Based on the logs I checked, the pod is crashing due to a config error. 
   I would need to check the ConfigMap to provide a complete diagnosis.&quot;</code></pre></p>
<p>2. <strong>"force"</strong>: Agent is forced to return what it has immediately
   <pre><code class="language-text">&quot;Agent stopped at max_iterations&quot;  [Not helpful!]</code></pre></p>
<strong>Use "generate"</strong> for better user experience.
<h3><strong>return_intermediate_steps: Debugging</strong></h3>
<pre><code class="language-python">result = agent_executor.invoke(
    {&quot;input&quot;: &quot;Why is pod crashing?&quot;},
    return_intermediate_steps=True  # Get full loop details
)
<h1>Access the reasoning loop:</h1>
for step in result[&quot;intermediate_steps&quot;]:
    action = step[0]  # (tool_name, tool_input)
    observation = step[1]  # tool output
    print(f&quot;Action: {action}&quot;)
    print(f&quot;Observation: {observation}&quot;)</code></pre>
<strong>When to use</strong>:
<li>‚úÖ Debugging agent behavior</li>
<li>‚úÖ Understanding why agent made certain choices</li>
<li>‚úÖ Logging for analytics</li>
<li>‚ùå Production (adds overhead)</li>
<p>---</p>
<h2>üêõ <strong>Common Loop Problems</strong></h2>
<h3><strong>Problem 1: Infinite Loops</strong></h3>
<strong>Symptom</strong>: Agent keeps calling the same tool repeatedly.
<pre><code class="language-text">Iteration 1: GetPodStatus ‚Üí &quot;CrashLoopBackOff&quot;
Iteration 2: GetPodStatus ‚Üí &quot;CrashLoopBackOff&quot;
Iteration 3: GetPodStatus ‚Üí &quot;CrashLoopBackOff&quot;
...</code></pre>
<strong>Causes</strong>:
1. Tool descriptions don't guide agent to next step
2. Temperature too high (random tool selection)
3. No memory (agent forgets what it did)
4. Tool returns ambiguous output
<strong>Solutions</strong>:
<pre><code class="language-python"># 1. Better tool descriptions
@tool
def get_pod_status(pod_name: str) -&gt; str:
    &quot;&quot;&quot;Get pod status.
    
    Use this FIRST to check if pod is Running, Pending, or CrashLoopBackOff.
    If CrashLoopBackOff, use GetPodLogs next to see why it&#039;s crashing.
    &quot;&quot;&quot;
    # ... implementation
    
<h1>2. Lower temperature</h1>
llm = ChatOpenAI(temperature=0.0)  # Deterministic
<h1>3. Add memory</h1>
memory = ConversationBufferWindowMemory(k=10)
<h1>4. Set max_iterations</h1>
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    max_iterations=5  # Force stop
)</code></pre>
<h3><strong>Problem 2: Wrong Tool Selection</strong></h3>
<strong>Symptom</strong>: Agent calls tools that don't help.
<pre><code class="language-text">User: &quot;Pod is out of memory&quot;
Agent calls: GetPodStatus  [Okay]
Agent calls: GetPodLogs    [Okay]
Agent calls: AnalyzeErrors [Doesn&#039;t help for OOM]
Agent calls: GetPodStatus  [Repeating, doesn&#039;t help]</code></pre>
<strong>Causes</strong>:
1. Vague tool descriptions
2. Too many similar tools
3. High temperature
<strong>Solutions</strong>:
<pre><code class="language-python"># 1. Specific descriptions with use cases
@tool
def check_resources(pod_name: str, namespace: str = &quot;default&quot;) -&gt; str:
    &quot;&quot;&quot;Check pod CPU and memory limits/usage.
    
    Use this when:
    - Pod shows OOMKilled error
    - Investigating resource limit issues
    - Pod is stuck in Pending state due to resource constraints
    
    NOT for: Network errors, image pull issues, config problems
    &quot;&quot;&quot;
    # ... implementation
<h1>2. Reduce number of tools (5-7 max)</h1>
<h1>3. Use temperature=0.0</code></pre></h1>
<h3><strong>Problem 3: Premature Exit</strong></h3>
<strong>Symptom</strong>: Agent gives up too early without gathering enough info.
<pre><code class="language-text">User: &quot;Pod nginx-abc is failing&quot;
Agent: &quot;I need more information to help. Please provide pod logs.&quot;
[But agent has GetPodLogs tool it could have used!]</code></pre>
<strong>Causes</strong>:
1. System prompt doesn't encourage tool use
2. max_iterations too low
3. Agent trained to defer to humans
<strong>Solutions</strong>:
<pre><code class="language-python">system_prompt = &quot;&quot;&quot;
You are a K8s troubleshooting assistant with diagnostic tools.
<p>IMPORTANT: Always use your tools to gather information before asking the user.
<li>If you need logs, call GetPodLogs</li>
<li>If you need status, call GetPodStatus</li>
<li>If you need details, call DescribePod</li></p>
<p>Only ask the user for information that your tools cannot provide 
(like business context or expected behavior).
&quot;&quot;&quot;</p>
<h1>Set reasonable max_iterations</h1>
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    max_iterations=10  # Allow enough iterations to investigate
)</code></pre>
<h3><strong>Problem 4: Over-Investigation</strong></h3>
<strong>Symptom</strong>: Agent keeps gathering data even after it has the answer.
<pre><code class="language-text">User: &quot;Is pod nginx-abc running?&quot;
Agent calls: GetPodStatus ‚Üí &quot;Running&quot;
Agent calls: GetPodLogs ‚Üí [unnecessary]
Agent calls: DescribePod ‚Üí [unnecessary]
Agent calls: CheckResources ‚Üí [unnecessary]
Final Answer: &quot;Yes, it&#039;s running&quot;</code></pre>
<strong>Causes</strong>:
1. System prompt encourages over-thoroughness
2. Agent doesn't know when it has enough info
3. Temperature creates randomness
<strong>Solutions</strong>:
<pre><code class="language-python">system_prompt = &quot;&quot;&quot;
You are a K8s troubleshooting assistant.
<p>Guidelines:
<li>Use tools efficiently - only call what you need</li>
<li>If the answer is clear from one tool, provide it immediately</li>
<li>For simple status checks, GetPodStatus is usually sufficient</li>
<li>For crashes/errors, check status ‚Üí logs ‚Üí describe (in that order)</li>
<li>Stop once you have enough information to answer confidently</li>
&quot;&quot;&quot;</p>
<h1>Use temperature=0.0 for deterministic behavior</h1>
llm = ChatOpenAI(temperature=0.0)</code></pre>
<p>---</p>
<h2>üìä <strong>Loop Patterns: Good vs Bad</strong></h2>
<h3><strong>Pattern 1: Linear Investigation (Good)</strong></h3>
<pre><code class="language-text">User: &quot;Pod is crashing&quot;
‚Üì
Iteration 1: GetPodStatus ‚Üí &quot;CrashLoopBackOff&quot;
‚Üì
Iteration 2: GetPodLogs ‚Üí &quot;OOMKilled error&quot;
‚Üì
Iteration 3: CheckResources ‚Üí &quot;Memory limit: 128Mi&quot;
‚Üì
Final Answer: &quot;Pod exceeds 128Mi limit, increase to 256Mi&quot;</code></pre>
<strong>Why good</strong>: Each step builds on the previous, logical progression, efficient.
<h3><strong>Pattern 2: Circular Investigation (Bad)</strong></h3>
<pre><code class="language-text">User: &quot;Pod is crashing&quot;
‚Üì
Iteration 1: GetPodStatus ‚Üí &quot;CrashLoopBackOff&quot;
‚Üì
Iteration 2: GetPodLogs ‚Üí &quot;Error message&quot;
‚Üì
Iteration 3: GetPodStatus ‚Üí &quot;CrashLoopBackOff&quot; [Repeating!]
‚Üì
Iteration 4: GetPodLogs ‚Üí &quot;Error message&quot; [Repeating!]
‚Üì
Max iterations reached, no answer</code></pre>
<strong>Why bad</strong>: Repeating same tools, not progressing, wastes iterations.
<h3><strong>Pattern 3: Shotgun Investigation (Bad)</strong></h3>
<pre><code class="language-text">User: &quot;Pod is crashing&quot;
‚Üì
Iteration 1: CheckResources [Wrong first step]
‚Üì
Iteration 2: AnalyzeErrors [No errors to analyze yet]
‚Üì
Iteration 3: GetPodStatus [Should have been first!]
‚Üì
Iteration 4: DescribePod [Okay but order is wrong]
‚Üì
Iteration 5: GetPodLogs [Should have been second!]</code></pre>
<strong>Why bad</strong>: Random tool order, inefficient, confusing logic.
<p>---</p>
<h2>üéØ <strong>Best Practices for Production</strong></h2>
<h3><strong>1. Design Logical Tool Order</strong></h3>
<p>Guide the agent with tool descriptions:</p>
<pre><code class="language-python">@tool
def get_pod_status(pod_name: str) -&gt; str:
    &quot;&quot;&quot;Step 1: Check pod status (Running, Pending, CrashLoopBackOff).
    
    Use this FIRST for any pod issue. Based on the status:
    - CrashLoopBackOff ‚Üí Use GetPodLogs next
    - Pending ‚Üí Use DescribePod next to see events
    - ImagePullBackOff ‚Üí Use DescribePod next
    &quot;&quot;&quot;
    
@tool
def get_pod_logs(pod_name: str) -&gt; str:
    &quot;&quot;&quot;Step 2: Get pod logs after confirming pod is crashing.
    
    Use this when GetPodStatus shows CrashLoopBackOff or Error state.
    Look for error messages in the logs.
    &quot;&quot;&quot;
<p>@tool
def describe_pod(pod_name: str) -&gt; str:
    &quot;&quot;&quot;Step 3: Get detailed pod info when status and logs aren&#039;t enough.
    
    Use this when:
    - You need to see pod events
    - Investigating Pending state
    - Checking volume mounts or ConfigMap issues
    &quot;&quot;&quot;</code></pre></p>
<h3><strong>2. Monitor Loop Health</strong></h3>
<p>Track metrics:</p>
<pre><code class="language-python">class LoopMonitor:
    def __init__(self):
        self.iteration_count = 0
        self.tools_called = []
        self.repeated_tools = 0
        
    def track_iteration(self, action, observation):
        self.iteration_count += 1
        
        # Detect repeated tools
        if action in self.tools_called:
            self.repeated_tools += 1
            
        self.tools_called.append(action)
        
        # Alert if agent is stuck
        if self.repeated_tools &gt; 2:
            print(&quot;‚ö†Ô∏è Warning: Agent repeating tools, may be stuck&quot;)
            
    def get_stats(self):
        return {
            &quot;total_iterations&quot;: self.iteration_count,
            &quot;unique_tools&quot;: len(set(self.tools_called)),
            &quot;repeated_calls&quot;: self.repeated_tools
        }</code></pre>
<h3><strong>3. Implement Circuit Breakers</strong></h3>
<p>Stop agent if it's clearly stuck:</p>
<pre><code class="language-python">class SmartAgentExecutor:
    def __init__(self, agent_executor, max_repeats=2):
        self.agent_executor = agent_executor
        self.max_repeats = max_repeats
        self.tool_history = []
        
    def invoke(self, input_dict):
        # Custom invoke that tracks tool calls
        for step in self.agent_executor.iter(input_dict):
            action = step.get(&quot;action&quot;)
            
            # Check for repeated tools
            if self.tool_history.count(action) &gt;= self.max_repeats:
                return {
                    &quot;output&quot;: &quot;Agent detected repeating pattern. Stopping to prevent infinite loop.&quot;,
                    &quot;status&quot;: &quot;circuit_breaker_triggered&quot;
                }
                
            self.tool_history.append(action)
            
        return step</code></pre>
<h3><strong>4. Test Different Scenarios</strong></h3>
<p>Create test cases:</p>
<pre><code class="language-python">test_cases = [
    {
        &quot;query&quot;: &quot;Is pod nginx-abc running?&quot;,
        &quot;expected_tools&quot;: [&quot;GetPodStatus&quot;],
        &quot;max_expected_iterations&quot;: 1
    },
    {
        &quot;query&quot;: &quot;Pod nginx-abc is crashing, why?&quot;,
        &quot;expected_tools&quot;: [&quot;GetPodStatus&quot;, &quot;GetPodLogs&quot;, &quot;DescribePod&quot;],
        &quot;max_expected_iterations&quot;: 3
    },
    {
        &quot;query&quot;: &quot;Pod is OOMKilled&quot;,
        &quot;expected_tools&quot;: [&quot;GetPodStatus&quot;, &quot;GetPodLogs&quot;, &quot;CheckResources&quot;],
        &quot;max_expected_iterations&quot;: 3
    }
]
<h1>Run tests</h1>
for test in test_cases:
    result = agent_executor.invoke({&quot;input&quot;: test[&quot;query&quot;]})
    iterations = len(result[&quot;intermediate_steps&quot;])
    
    assert iterations &lt;= test[&quot;max_expected_iterations&quot;], \
        f&quot;Too many iterations: {iterations} &gt; {test[&#039;max_expected_iterations&#039;]}&quot;</code></pre>
<p>---</p>
<h2>üî¨ <strong>Debugging Agent Loops</strong></h2>
<h3><strong>Enable Verbose Mode</strong></h3>
<pre><code class="language-python">agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True  # See all thoughts and actions
)</code></pre>
<strong>Output</strong>:
<pre><code class="language-text">&gt; Entering new AgentExecutor chain...
<p>Thought: I need to check the pod status first
Action: GetPodStatus
Action Input: {&quot;pod_name&quot;: &quot;nginx-abc123&quot;, &quot;namespace&quot;: &quot;default&quot;}
Observation: Pod nginx-abc123 is in CrashLoopBackOff state</p>
<p>Thought: Now I should check the logs to see why it&#039;s crashing
Action: GetPodLogs
Action Input: {&quot;pod_name&quot;: &quot;nginx-abc123&quot;, &quot;namespace&quot;: &quot;default&quot;, &quot;tail&quot;: 50}
Observation: Error: Configuration file not found</p>
<p>Thought: I have enough information to answer
Final Answer: Pod is crashing because the nginx config file is missing...</p>
<p>&gt; Finished chain.</code></pre></p>
<h3><strong>Analyze Intermediate Steps</strong></h3>
<pre><code class="language-python">result = agent_executor.invoke(
    {&quot;input&quot;: &quot;Why is pod crashing?&quot;},
    return_intermediate_steps=True
)
<h1>Check what the agent did</h1>
steps = result[&quot;intermediate_steps&quot;]
print(f&quot;Total iterations: {len(steps)}&quot;)
<p>for i, (action, observation) in enumerate(steps, 1):
    print(f&quot;\n--- Iteration {i} ---&quot;)
    print(f&quot;Tool: {action.tool}&quot;)
    print(f&quot;Input: {action.tool_input}&quot;)
    print(f&quot;Output: {observation[:100]}...&quot;)  # First 100 chars</code></pre></p>
<h3><strong>Log to File for Analysis</strong></h3>
<pre><code class="language-python">import logging
<p>logging.basicConfig(
    filename=&#039;agent_loops.log&#039;,
    level=logging.DEBUG,
    format=&#039;%(asctime)s - %(message)s&#039;
)</p>
<p>class LoggingAgentExecutor:
    def __init__(self, agent_executor):
        self.agent_executor = agent_executor
        
    def invoke(self, input_dict):
        logging.info(f&quot;Query: {input_dict[&#039;input&#039;]}&quot;)
        
        result = self.agent_executor.invoke(
            input_dict,
            return_intermediate_steps=True
        )
        
        # Log each iteration
        for i, (action, observation) in enumerate(result[&quot;intermediate_steps&quot;], 1):
            logging.info(f&quot;Iteration {i}: {action.tool} -&gt; {observation[:200]}&quot;)
            
        logging.info(f&quot;Final Answer: {result[&#039;output&#039;]}&quot;)
        
        return result</code></pre></p>
<p>---</p>
<h2>üéì <strong>Self-Check Questions</strong></h2>
<h3><strong>Question 1</strong>: What are the 3 components of each iteration in a ReAct loop?</h3>
<details>
<summary>Show Answer</summary>
<p>1. <strong>Thought</strong>: The agent's reasoning about what to do next
2. <strong>Action</strong>: The tool call the agent decides to make (with parameters)
3. <strong>Observation</strong>: The result returned by the tool</p>
<p>Example:
<pre><code class="language-text">Thought: &quot;I need to check pod status&quot;
Action: GetPodStatus(pod_name=&quot;nginx-abc&quot;)
Observation: &quot;Pod is CrashLoopBackOff&quot;</code></pre></p>
</details>
<h3><strong>Question 2</strong>: Why should you set max_iterations for production agents?</h3>
<details>
<summary>Show Answer</summary>
<strong>To prevent infinite loops.</strong>
<p>Without max_iterations, an agent might:
<li>Get stuck calling the same tool repeatedly</li>
<li>Loop endlessly if it can't find a solution</li>
<li>Waste compute resources and cost money</li>
<li>Provide poor user experience (long wait times)</li></p>
<strong>Best practice</strong>: Set max_iterations=5-10 based on task complexity, and use `early_stopping_method="generate"` to get the best partial answer if max is reached.
</details>
<h3><strong>Question 3</strong>: What causes an agent to get stuck in an infinite loop?</h3>
<details>
<summary>Show Answer</summary>
<strong>Common causes</strong>:
1. <strong>Vague tool descriptions</strong>: Agent doesn't know what tool to call next
2. <strong>High temperature</strong>: Random tool selection causes repeated calls
3. <strong>No memory</strong>: Agent forgets what it already did
4. <strong>Ambiguous tool outputs</strong>: Agent can't decide what to do with the result
5. <strong>Conflicting tool descriptions</strong>: Multiple tools seem equally valid
<strong>Solutions</strong>:
<li>Use temperature=0.0 for deterministic behavior</li>
<li>Write clear tool descriptions with "Use this when..." guidance</li>
<li>Add memory to track conversation history</li>
<li>Include sequential hints ("Use GetPodStatus FIRST, then GetPodLogs")</li>
<li>Set max_iterations as safety net</li>
</details>
<h3><strong>Question 4</strong>: Design a good reasoning loop for this query: "Pod is OOMKilled"</h3>
<details>
<summary>Show Answer</summary>
<pre><code class="language-text">User Query: &quot;Pod cart-service-xyz is OOMKilled&quot;
<p>Iteration 1:
Thought: &quot;OOMKilled means out of memory. Let me confirm the pod status.&quot;
Action: GetPodStatus(pod_name=&quot;cart-service-xyz&quot;, namespace=&quot;default&quot;)
Observation: &quot;Pod is in CrashLoopBackOff. Last exit code: 137 (OOMKilled)&quot;</p>
<p>Iteration 2:
Thought: &quot;Exit code 137 confirms OOMKilled. I should check resource limits.&quot;
Action: CheckResources(pod_name=&quot;cart-service-xyz&quot;, namespace=&quot;default&quot;)
Observation: &quot;Memory limit: 128Mi, Memory usage at crash: 145Mi&quot;</p>
<p>Iteration 3:
Thought: &quot;Pod is using 145Mi but limit is only 128Mi. I have enough info.&quot;
Final Answer: &quot;Pod cart-service-xyz is being killed because it exceeds its 
128Mi memory limit (using 145Mi).</p>
<p>Fixes:
1. Increase memory limit to 256Mi in the deployment spec
2. Investigate if there&#039;s a memory leak causing high usage
3. Add horizontal pod autoscaling if traffic is increasing&quot;</code></pre></p>
<strong>Why this is good</strong>:
<li>‚úÖ Linear progression (status ‚Üí resources ‚Üí conclusion)</li>
<li>‚úÖ Each step builds on previous observation</li>
<li>‚úÖ Only 2 tool calls needed (efficient)</li>
<li>‚úÖ Clear reasoning at each step</li>
<li>‚úÖ Actionable final answer</li>
</details>
<h3><strong>Question 5</strong>: What's the difference between early_stopping_method "generate" vs "force"?</h3>
<details>
<summary>Show Answer</summary>
<p>When max_iterations is reached:</p>
<strong>"generate"</strong> (recommended):
<li>Agent generates the best answer it can based on what it learned so far</li>
<li>Example: "Based on the logs I checked, the pod is crashing due to a missing config file. I would need to check the ConfigMap to provide a complete solution."</li>
<li>Better user experience</li>
<strong>"force"</strong>:
<li>Agent immediately returns with minimal output</li>
<li>Example: "Agent stopped at max_iterations"</li>
<li>Poor user experience, not helpful</li></ul>
<strong>Always use "generate"</strong> for production systems.
<pre><code class="language-python">agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    max_iterations=5,
    early_stopping_method=&quot;generate&quot;  # ‚úÖ Use this
)</code></pre>
</details>
<p>---</p>
<h2>üöÄ <strong>Key Takeaways</strong></h2>
<p>1. <strong>ReAct loop</strong>: Thought ‚Üí Action ‚Üí Observation ‚Üí repeat
2. <strong>Set max_iterations</strong>: Prevent infinite loops (5-10 is typical)
3. <strong>Use temperature=0.0</strong>: Ensures deterministic tool selection
4. <strong>Guide with descriptions</strong>: Tell agent which tools to use when
5. <strong>Monitor loop health</strong>: Track repeated tools, circular patterns
6. <strong>Test different scenarios</strong>: Ensure agent handles various queries efficiently
7. <strong>Enable verbose mode</strong>: Debug by seeing agent's reasoning
8. <strong>Use "generate" stopping</strong>: Better user experience when max_iterations reached</p>
<p>---</p>
<h2>üîó <strong>Next Module</strong></h2>
<p>Move on to <strong>Module 7: LangChain Components</strong> to understand the framework that powers these agents!</p>
<p>---</p>
<strong>Time to complete this module</strong>: 45 minutes  
<strong>Hands-on practice</strong>: 20 minutes  
<strong>Total</strong>: ~1 hour

    </div>
    

    <div class="module-content" id="module-7">
        <h1>Module 7: LangChain Components Deep Dive</h1>
<strong>Study Time</strong>: ~1 hour  
<strong>Prerequisites</strong>: Modules 1-6
<p>---</p>
<h2>üéØ <strong>Learning Objectives</strong></h2>
<p>By the end of this module, you'll understand:
1. What LangChain is and why it's useful
2. Core components: LLMs, PromptTemplates, Chains, Agents, Tools
3. How to compose components into applications
4. Memory management in LangChain
5. Best practices for your K8s agent</p>
<p>---</p>
<h2>üîó <strong>What is LangChain?</strong></h2>
<strong>LangChain</strong> is a framework for building applications with LLMs. It provides:
<ul><li><strong>Abstractions</strong>: Common patterns wrapped in reusable components</li>
<li><strong>Chains</strong>: Connect multiple LLM calls and logic</li>
<li><strong>Agents</strong>: Autonomous systems that use tools</li>
<li><strong>Memory</strong>: Persist conversation context</li>
<li><strong>Integrations</strong>: Works with OpenAI, Anthropic, Ollama, etc.</li>
<h3><strong>Why Use LangChain?</strong></h3>
<p>#### <strong>Without LangChain</strong> (Raw API calls)
<pre><code class="language-python"># You write everything manually
import openai</p>
<p>def diagnose_pod(pod_name):
    # Call 1: Check status
    status_prompt = f&quot;Check status of pod {pod_name}&quot;
    status = openai.chat.completions.create(...)
    
    # Call 2: Analyze result
    analysis_prompt = f&quot;Analyze this status: {status}&quot;
    analysis = openai.chat.completions.create(...)
    
    # Call 3: Suggest fix
    fix_prompt = f&quot;Based on {analysis}, suggest fix&quot;
    fix = openai.chat.completions.create(...)
    
    # You handle memory, errors, retries manually
    return fix</code></pre></p>
<strong>Problems</strong>:
<li>‚ùå Repetitive code</li>
<li>‚ùå Manual memory management</li>
<li>‚ùå No tool calling framework</li>
<li>‚ùå Hard to maintain</li>
<p>#### <strong>With LangChain</strong>
<pre><code class="language-python">from langchain.agents import create_react_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferWindowMemory</p>
<h1>Framework handles complexity</h1>
llm = ChatOpenAI(temperature=0.0)
memory = ConversationBufferWindowMemory(k=10)
agent = create_react_agent(llm, tools, prompt_template)
agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory)
<h1>One line to execute</h1>
result = agent_executor.invoke({&quot;input&quot;: &quot;Why is pod nginx-abc crashing?&quot;})</code></pre>
<strong>Benefits</strong>:
<li>‚úÖ Less boilerplate code</li>
<li>‚úÖ Built-in memory management</li>
<li>‚úÖ Tool calling framework</li>
<li>‚úÖ Easy to extend</li>
<p>---</p>
<h2>üß± <strong>Core Components</strong></h2>
<h3><strong>1. LLMs and Chat Models</strong></h3>
<strong>LLMs</strong>: Basic text completion models (GPT-3, older models)
<pre><code class="language-python">from langchain.llms import OpenAI
<p>llm = OpenAI(temperature=0.7)
result = llm.invoke(&quot;What is Kubernetes?&quot;)
<h1>Returns: &quot;Kubernetes is a container orchestration platform...&quot;</code></pre></h1></p>
<strong>Chat Models</strong> (Modern, recommended): Designed for conversations
<pre><code class="language-python">from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
<p>chat = ChatOpenAI(model=&quot;gpt-4&quot;, temperature=0.0)</p>
<p>messages = [
    SystemMessage(content=&quot;You are a K8s expert&quot;),
    HumanMessage(content=&quot;What is a pod?&quot;)
]</p>
<p>result = chat.invoke(messages)
<h1>Returns: Message object with .content</code></pre></h1></p>
<strong>Key Difference</strong>:
<li><strong>LLMs</strong>: Simple text in, text out</li>
<li><strong>Chat Models</strong>: Message objects, roles (system/user/assistant), better for conversations</li>
<strong>For your agent</strong>: Use <strong>ChatOpenAI</strong> (chat model)
<h3><strong>2. Prompt Templates</strong></h3>
<p>Reusable prompt structures with variables.</p>
<p>#### <strong>Basic Template</strong>
<pre><code class="language-python">from langchain.prompts import PromptTemplate</p>
<p>template = PromptTemplate(
    input_variables=[&quot;pod_name&quot;, &quot;error&quot;],
    template=&quot;&quot;&quot;
    Diagnose this Kubernetes issue:
    
    Pod: {pod_name}
    Error: {error}
    
    Provide step-by-step diagnosis:
    &quot;&quot;&quot;
)</p>
<h1>Use the template</h1>
prompt = template.format(pod_name=&quot;nginx-abc&quot;, error=&quot;CrashLoopBackOff&quot;)
result = llm.invoke(prompt)</code></pre>
<p>#### <strong>Chat Prompt Template</strong> (Better for conversations)
<pre><code class="language-python">from langchain.prompts import ChatPromptTemplate</p>
<p>chat_template = ChatPromptTemplate.from_messages([
    (&quot;system&quot;, &quot;You are a Kubernetes troubleshooting expert.&quot;),
    (&quot;human&quot;, &quot;Pod {pod_name} is in {status} state. Help me diagnose.&quot;),
    (&quot;ai&quot;, &quot;I&#039;ll help diagnose this issue. Let me check the pod first.&quot;),
    (&quot;human&quot;, &quot;{follow_up}&quot;)
])</p>
<h1>Use it</h1>
messages = chat_template.format_messages(
    pod_name=&quot;nginx-abc&quot;,
    status=&quot;CrashLoopBackOff&quot;,
    follow_up=&quot;What should I check next?&quot;
)
<p>result = chat.invoke(messages)</code></pre></p>
<p>#### <strong>For ReAct Agents</strong>
<pre><code class="language-python">react_prompt = &quot;&quot;&quot;
You are a Kubernetes troubleshooting assistant with access to diagnostic tools.</p>
<p>Available tools:
{tools}</p>
<p>Tool Names: {tool_names}</p>
<p>When troubleshooting:
1. Think step-by-step
2. Use tools to gather real data
3. Provide actionable recommendations</p>
<p>Use this format:</p>
<p>Question: the user&#039;s question
Thought: think about what to do
Action: the tool to use (must be one of [{tool_names}])
Action Input: the input to the tool
Observation: the result of the action
... (repeat Thought/Action/Observation as needed)
Thought: I now know the final answer
Final Answer: the complete diagnosis and fix</p>
<p>Question: {input}
Thought: {agent_scratchpad}
&quot;&quot;&quot;</p>
<p>prompt = PromptTemplate.from_template(react_prompt)</code></pre></p>
<h3><strong>3. Tools</strong></h3>
<p>Functions the agent can call.</p>
<p>#### <strong>Basic Tool</strong>
<pre><code class="language-python">from langchain.tools import tool</p>
<p>@tool
def get_pod_status(pod_name: str, namespace: str = &quot;default&quot;) -&gt; str:
    &quot;&quot;&quot;Get the status of a Kubernetes pod.
    
    Use this tool when you need to check if a pod is Running, Pending, 
    CrashLoopBackOff, or in any other state. This should be your FIRST 
    step when diagnosing pod issues.
    
    Args:
        pod_name: Name of the pod (required)
        namespace: Kubernetes namespace (default: default)
    
    Returns:
        String with pod status and conditions
    &quot;&quot;&quot;
    import subprocess
    
    try:
        result = subprocess.run(
            [&quot;kubectl&quot;, &quot;get&quot;, &quot;pod&quot;, pod_name, &quot;-n&quot;, namespace, &quot;-o&quot;, &quot;wide&quot;],
            capture_output=True,
            text=True,
            timeout=5
        )
        
        if result.returncode != 0:
            return f&quot;Error: Pod {pod_name} not found in namespace {namespace}&quot;
            
        return result.stdout
    except Exception as e:
        return f&quot;Error checking pod status: {str(e)}&quot;</code></pre></p>
<strong>Key parts</strong>:
<li>`@tool` decorator makes it a LangChain tool</li>
<li><strong>Docstring</strong> is critical - agent reads this to decide when to use</li>
<li><strong>Type hints</strong> help agent understand parameters</li>
<li><strong>Error handling</strong> prevents crashes</li>
<p>#### <strong>Structured Tool (Advanced)</strong>
<pre><code class="language-python">from langchain.tools import StructuredTool
from pydantic import BaseModel, Field</p>
<p>class PodStatusInput(BaseModel):
    pod_name: str = Field(description=&quot;Name of the pod to check&quot;)
    namespace: str = Field(default=&quot;default&quot;, description=&quot;Kubernetes namespace&quot;)</p>
<p>def get_pod_status_func(pod_name: str, namespace: str = &quot;default&quot;) -&gt; str:
    # Implementation here
    pass</p>
<p>pod_status_tool = StructuredTool(
    name=&quot;GetPodStatus&quot;,
    func=get_pod_status_func,
    description=&quot;Get status of a Kubernetes pod. Use first when diagnosing issues.&quot;,
    args_schema=PodStatusInput
)</code></pre></p>
<strong>Benefits of StructuredTool</strong>:
<li>‚úÖ Better input validation</li>
<li>‚úÖ Clear parameter descriptions</li>
<li>‚úÖ Type safety</li>
<h3><strong>4. Agents</strong></h3>
<p>Agents decide which tools to call and when.</p>
<p>#### <strong>Create ReAct Agent</strong>
<pre><code class="language-python">from langchain.agents import create_react_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate</p>
<h1>1. Define LLM</h1>
llm = ChatOpenAI(model=&quot;gpt-4&quot;, temperature=0.0)
<h1>2. Define tools</h1>
tools = [get_pod_status, get_pod_logs, describe_pod]
<h1>3. Create prompt</h1>
prompt = PromptTemplate.from_template(react_prompt_template)
<h1>4. Create agent</h1>
agent = create_react_agent(
    llm=llm,
    tools=tools,
    prompt=prompt
)
<h1>5. Create executor (runs the agent)</h1>
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,
    max_iterations=5,
    early_stopping_method=&quot;generate&quot;,
    handle_parsing_errors=True  # Gracefully handle LLM output errors
)</code></pre>
<p>#### <strong>Using the Agent</strong>
<pre><code class="language-python"># Simple query
result = agent_executor.invoke({
    &quot;input&quot;: &quot;Why is pod nginx-abc123 crashing?&quot;
})</p>
<p>print(result[&quot;output&quot;])</code></pre></p>
<p>#### <strong>With Memory</strong>
<pre><code class="language-python">from langchain.memory import ConversationBufferWindowMemory</p>
<p>memory = ConversationBufferWindowMemory(
    k=10,
    memory_key=&quot;chat_history&quot;,
    return_messages=True
)</p>
<p>agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    memory=memory,  # Add memory
    verbose=True
)</p>
<h1>First query</h1>
result1 = agent_executor.invoke({
    &quot;input&quot;: &quot;Check status of pod nginx-abc&quot;
})
<h1>Follow-up (agent remembers we&#039;re talking about nginx-abc)</h1>
result2 = agent_executor.invoke({
    &quot;input&quot;: &quot;What are the logs?&quot;
})</code></pre>
<h3><strong>5. Memory</strong></h3>
<p>Stores conversation history so the agent remembers context.</p>
<p>#### <strong>ConversationBufferMemory</strong> (Simple)
<pre><code class="language-python">from langchain.memory import ConversationBufferMemory</p>
<p>memory = ConversationBufferMemory()</p>
<h1>Manually add to memory</h1>
memory.save_context(
    {&quot;input&quot;: &quot;Hello&quot;}, 
    {&quot;output&quot;: &quot;Hi! How can I help?&quot;}
)
<h1>Load memory</h1>
print(memory.load_memory_variables({}))
<h1>Output: {&quot;history&quot;: &quot;Human: Hello\nAI: Hi! How can I help?&quot;}</code></pre></h1>
<p>#### <strong>ConversationBufferWindowMemory</strong> (Recommended)
<pre><code class="language-python">from langchain.memory import ConversationBufferWindowMemory</p>
<p>memory = ConversationBufferWindowMemory(
    k=10,  # Keep last 10 exchanges
    memory_key=&quot;chat_history&quot;,
    return_messages=True  # Return as Message objects
)</p>
<h1>Use with agent</h1>
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    memory=memory
)</code></pre>
<p>#### <strong>ConversationSummaryMemory</strong> (Advanced)
<pre><code class="language-python">from langchain.memory import ConversationSummaryMemory
from langchain_openai import ChatOpenAI</p>
<p>llm = ChatOpenAI(temperature=0.0)</p>
<p>memory = ConversationSummaryMemory(
    llm=llm,  # Uses LLM to summarize
    memory_key=&quot;chat_history&quot;
)</p>
<h1>Automatically summarizes long conversations</code></pre></h1>
<strong>Comparison</strong>:
<p>| Memory Type | Pros | Cons | Use Case |
|-------------|------|------|----------|
| BufferMemory | Simple, exact history | Can overflow tokens | Demos, short sessions |
| BufferWindowMemory ‚≠ê | Predictable, fixed size | Loses old context | Your K8s agent |
| SummaryMemory | Token efficient | Extra LLM calls, costs | Long sessions |
| SummaryBufferMemory | Best of both | Complex | Production at scale |</p>
<h3><strong>6. Chains</strong> (Legacy, but useful to know)</h3>
<p>Connect multiple components in sequence.</p>
<pre><code class="language-python">from langchain.chains import LLMChain
<h1>Simple chain</h1>
chain = LLMChain(llm=llm, prompt=prompt_template)
result = chain.invoke({&quot;pod_name&quot;: &quot;nginx-abc&quot;, &quot;error&quot;: &quot;CrashLoopBackOff&quot;})</code></pre>
<strong>Note</strong>: Modern LangChain uses LCEL (LangChain Expression Language) instead:
<pre><code class="language-python"># Modern approach with LCEL
chain = prompt_template | llm | output_parser
result = chain.invoke({&quot;pod_name&quot;: &quot;nginx-abc&quot;})</code></pre>
<p>---</p>
<h2>üèóÔ∏è <strong>Building Your K8s Agent</strong></h2>
<h3><strong>Complete Example</strong></h3>
<pre><code class="language-python">from langchain.agents import create_react_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferWindowMemory
from langchain.tools import tool
import subprocess
<h1>1. Define Tools</h1>
@tool
def get_pod_status(pod_name: str, namespace: str = &quot;default&quot;) -&gt; str:
    &quot;&quot;&quot;Get status of a Kubernetes pod. Use FIRST when diagnosing issues.&quot;&quot;&quot;
    result = subprocess.run(
        [&quot;kubectl&quot;, &quot;get&quot;, &quot;pod&quot;, pod_name, &quot;-n&quot;, namespace],
        capture_output=True, text=True, timeout=5
    )
    return result.stdout if result.returncode == 0 else f&quot;Error: {result.stderr}&quot;
<p>@tool
def get_pod_logs(pod_name: str, namespace: str = &quot;default&quot;, tail: int = 50) -&gt; str:
    &quot;&quot;&quot;Get pod logs. Use when pod is crashing to see error messages.&quot;&quot;&quot;
    result = subprocess.run(
        [&quot;kubectl&quot;, &quot;logs&quot;, pod_name, &quot;-n&quot;, namespace, f&quot;--tail={tail}&quot;],
        capture_output=True, text=True, timeout=10
    )
    return result.stdout if result.returncode == 0 else f&quot;Error: {result.stderr}&quot;</p>
<p>@tool
def describe_pod(pod_name: str, namespace: str = &quot;default&quot;) -&gt; str:
    &quot;&quot;&quot;Get detailed pod info and events. Use when status and logs aren&#039;t enough.&quot;&quot;&quot;
    result = subprocess.run(
        [&quot;kubectl&quot;, &quot;describe&quot;, &quot;pod&quot;, pod_name, &quot;-n&quot;, namespace],
        capture_output=True, text=True, timeout=10
    )
    return result.stdout if result.returncode == 0 else f&quot;Error: {result.stderr}&quot;</p>
<p>tools = [get_pod_status, get_pod_logs, describe_pod]</p>
<h1>2. Define Prompt</h1>
react_prompt = &quot;&quot;&quot;
You are a Kubernetes troubleshooting expert with access to diagnostic tools.
<p>Available tools:
{tools}</p>
<p>Use this format:</p>
<p>Question: the user&#039;s question
Thought: think about what to do next
Action: the tool to use (one of [{tool_names}])
Action Input: the input for the tool
Observation: the result from the tool
... (repeat Thought/Action/Observation as needed)
Thought: I now know the final answer
Final Answer: provide complete diagnosis and recommended fixes</p>
<p>Guidelines:
<li>Use GetPodStatus FIRST to check pod state</li>
<li>If CrashLoopBackOff, use GetPodLogs to see errors</li>
<li>If Pending or ImagePullBackOff, use DescribePod for events</li>
<li>Always provide actionable recommendations</li></p>
<p>Question: {input}
Thought: {agent_scratchpad}
&quot;&quot;&quot;</p>
<p>prompt = PromptTemplate.from_template(react_prompt)</p>
<h1>3. Create LLM</h1>
llm = ChatOpenAI(model=&quot;gpt-4&quot;, temperature=0.0)
<h1>4. Create Agent</h1>
agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)
<h1>5. Create Memory</h1>
memory = ConversationBufferWindowMemory(
    k=10,
    memory_key=&quot;chat_history&quot;,
    return_messages=True
)
<h1>6. Create Agent Executor</h1>
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    memory=memory,
    verbose=True,
    max_iterations=5,
    early_stopping_method=&quot;generate&quot;,
    handle_parsing_errors=True
)
<h1>7. Use It!</h1>
if __name__ == &quot;__main__&quot;:
    # First query
    result = agent_executor.invoke({
        &quot;input&quot;: &quot;Check if pod nginx-deployment-abc123 is running&quot;
    })
    print(result[&quot;output&quot;])
    
    # Follow-up query (agent remembers context)
    result = agent_executor.invoke({
        &quot;input&quot;: &quot;What are the logs?&quot;
    })
    print(result[&quot;output&quot;])</code></pre>
<p>---</p>
<h2>üé® <strong>Advanced Patterns</strong></h2>
<h3><strong>Pattern 1: Custom Output Parser</strong></h3>
<p>Parse agent output into structured format:</p>
<pre><code class="language-python">from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
<p>class DiagnosisOutput(BaseModel):
    pod_name: str = Field(description=&quot;Name of the pod&quot;)
    status: str = Field(description=&quot;Current pod status&quot;)
    root_cause: str = Field(description=&quot;Root cause of the issue&quot;)
    fixes: list[str] = Field(description=&quot;List of recommended fixes&quot;)</p>
<p>parser = PydanticOutputParser(pydantic_object=DiagnosisOutput)</p>
<h1>Add to prompt</h1>
format_instructions = parser.get_format_instructions()
prompt_with_format = f&quot;{react_prompt}\n\n{format_instructions}&quot;</code></pre>
<h3><strong>Pattern 2: Fallback LLM</strong></h3>
<p>Use cheaper model, fallback to better one if needed:</p>
<pre><code class="language-python">from langchain.chat_models import ChatOpenAI
from langchain.llms import Ollama
<h1>Primary: Free local model</h1>
primary_llm = Ollama(model=&quot;llama3&quot;)
<h1>Fallback: Paid but better</h1>
fallback_llm = ChatOpenAI(model=&quot;gpt-4&quot;, temperature=0.0)
<h1>Create agent with fallback</h1>
llm_with_fallback = primary_llm.with_fallbacks([fallback_llm])
<p>agent = create_react_agent(
    llm=llm_with_fallback,
    tools=tools,
    prompt=prompt
)</code></pre></p>
<h3><strong>Pattern 3: Streaming Responses</strong></h3>
<p>Stream agent output in real-time:</p>
<pre><code class="language-python">for chunk in agent_executor.stream({&quot;input&quot;: &quot;Why is pod crashing?&quot;}):
    if &quot;output&quot; in chunk:
        print(chunk[&quot;output&quot;], end=&quot;&quot;, flush=True)</code></pre>
<h3><strong>Pattern 4: Async Execution</strong></h3>
<p>Run agent asynchronously:</p>
<pre><code class="language-python">import asyncio
<p>async def diagnose_async(pod_name: str):
    result = await agent_executor.ainvoke({
        &quot;input&quot;: f&quot;Diagnose pod {pod_name}&quot;
    })
    return result[&quot;output&quot;]</p>
<h1>Use it</h1>
result = asyncio.run(diagnose_async(&quot;nginx-abc&quot;))</code></pre>
<p>---</p>
<h2>üö® <strong>Common Pitfalls</strong></h2>
<h3><strong>Pitfall 1: Not Handling Tool Errors</strong></h3>
<p>‚ùå <strong>Bad</strong>:
<pre><code class="language-python">@tool
def get_pod_status(pod_name: str) -&gt; str:
    &quot;&quot;&quot;Get pod status.&quot;&quot;&quot;
    result = subprocess.run([&quot;kubectl&quot;, &quot;get&quot;, &quot;pod&quot;, pod_name])
    return result.stdout  # Might crash if pod doesn&#039;t exist!</code></pre></p>
<p>‚úÖ <strong>Good</strong>:
<pre><code class="language-python">@tool
def get_pod_status(pod_name: str) -&gt; str:
    &quot;&quot;&quot;Get pod status.&quot;&quot;&quot;
    try:
        result = subprocess.run(
            [&quot;kubectl&quot;, &quot;get&quot;, &quot;pod&quot;, pod_name],
            capture_output=True,
            text=True,
            timeout=5
        )
        
        if result.returncode != 0:
            return f&quot;Error: {result.stderr}&quot;
            
        return result.stdout
    except subprocess.TimeoutExpired:
        return &quot;Error: Command timed out&quot;
    except Exception as e:
        return f&quot;Error: {str(e)}&quot;</code></pre></p>
<h3><strong>Pitfall 2: Vague Tool Descriptions</strong></h3>
<p>‚ùå <strong>Bad</strong>:
<pre><code class="language-python">@tool
def check_pod(pod_name: str) -&gt; str:
    &quot;&quot;&quot;Checks pod.&quot;&quot;&quot;  # Too vague!</code></pre></p>
<p>‚úÖ <strong>Good</strong>:
<pre><code class="language-python">@tool
def get_pod_status(pod_name: str) -&gt; str:
    &quot;&quot;&quot;Get the current status of a Kubernetes pod.
    
    Use this tool when you need to check if a pod is:
    - Running (healthy)
    - Pending (waiting to start)
    - CrashLoopBackOff (repeatedly crashing)
    - ImagePullBackOff (can&#039;t pull container image)
    - Error or Unknown state
    
    This should be your FIRST tool when diagnosing any pod issue.
    Based on the status, decide which tool to use next:
    - CrashLoopBackOff ‚Üí Use GetPodLogs to see crash reason
    - Pending ‚Üí Use DescribePod to see why it can&#039;t start
    - ImagePullBackOff ‚Üí Use DescribePod to see image details
    &quot;&quot;&quot;</code></pre></p>
<h3><strong>Pitfall 3: Not Setting max_iterations</strong></h3>
<p>‚ùå <strong>Bad</strong>:
<pre><code class="language-python">agent_executor = AgentExecutor(agent=agent, tools=tools)
<h1>No max_iterations = potential infinite loop!</code></pre></h1></p>
<p>‚úÖ <strong>Good</strong>:
<pre><code class="language-python">agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    max_iterations=5,
    early_stopping_method=&quot;generate&quot;
)</code></pre></p>
<h3><strong>Pitfall 4: Using High Temperature</strong></h3>
<p>‚ùå <strong>Bad</strong>:
<pre><code class="language-python">llm = ChatOpenAI(temperature=1.0)  # Too random for tool calling!</code></pre></p>
<p>‚úÖ <strong>Good</strong>:
<pre><code class="language-python">llm = ChatOpenAI(temperature=0.0)  # Deterministic, consistent</code></pre></p>
<p>---</p>
<h2>üéì <strong>Self-Check Questions</strong></h2>
<h3><strong>Question 1</strong>: What are the 5 core components you need to create a LangChain agent?</h3>
<details>
<summary>Show Answer</summary>
<p>1. <strong>LLM</strong>: The language model (ChatOpenAI, Ollama, etc.)
2. <strong>Tools</strong>: Functions the agent can call
3. <strong>Prompt</strong>: Template that defines agent behavior (ReAct format)
4. <strong>Agent</strong>: Created with `create_react_agent(llm, tools, prompt)`
5. <strong>AgentExecutor</strong>: Runs the agent with configuration (max_iterations, memory, etc.)</p>
<p>Optional but recommended:
<li><strong>Memory</strong>: To maintain conversation context</li></p>
</details>
<h3><strong>Question 2</strong>: What's the difference between ConversationBufferMemory and ConversationBufferWindowMemory?</h3>
<details>
<summary>Show Answer</summary>
<strong>ConversationBufferMemory</strong>:
<li>Stores ALL conversation history</li>
<li>Can overflow token limits in long conversations</li>
<li>Best for: Short sessions, demos</li>
<strong>ConversationBufferWindowMemory</strong>:
<li>Stores only the last `k` exchanges</li>
<li>Fixed memory size, predictable token usage</li>
<li>Loses old context but prevents overflow</li>
<li>Best for: Your K8s agent, multi-turn debugging</li>
<p>Example:
<pre><code class="language-python"># Buffer: Stores everything
memory = ConversationBufferMemory()</p>
<h1>BufferWindow: Only keeps last 10 exchanges</h1>
memory = ConversationBufferWindowMemory(k=10)</code></pre>
</details>
<h3><strong>Question 3</strong>: Why is the tool's docstring so important?</h3>
<details>
<summary>Show Answer</summary>
<p>The <strong>docstring is the tool description</strong> that the agent reads to decide when to use the tool.</p>
<strong>Without good docstring</strong>:
<li>Agent doesn't know when to call the tool</li>
<li>Might call wrong tools</li>
<li>Gets confused with similar tools</li>
<strong>With good docstring</strong>:
<li>Agent knows exactly when to use it</li>
<li>Understands what inputs are needed</li>
<li>Knows what output to expect</li></ul>
<strong>Best practices for docstrings</strong>:
1. Start with what the tool does (one sentence)
2. Explain WHEN to use it ("Use this when...")
3. List valid input parameters
4. Describe the output format
5. Give examples if complex
6. Suggest next steps ("After this, use...")
</details>
<h3><strong>Question 4</strong>: Write the code to create a basic K8s agent with 3 tools</h3>
<details>
<summary>Show Answer</summary>
<pre><code class="language-python">from langchain.agents import create_react_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.tools import tool
import subprocess
<h1>1. Define tools</h1>
@tool
def get_pod_status(pod_name: str) -&gt; str:
    &quot;&quot;&quot;Get pod status. Use FIRST.&quot;&quot;&quot;
    result = subprocess.run(
        [&quot;kubectl&quot;, &quot;get&quot;, &quot;pod&quot;, pod_name],
        capture_output=True, text=True
    )
    return result.stdout
<p>@tool
def get_pod_logs(pod_name: str) -&gt; str:
    &quot;&quot;&quot;Get pod logs. Use when pod is crashing.&quot;&quot;&quot;
    result = subprocess.run(
        [&quot;kubectl&quot;, &quot;logs&quot;, pod_name, &quot;--tail=50&quot;],
        capture_output=True, text=True
    )
    return result.stdout</p>
<p>@tool
def describe_pod(pod_name: str) -&gt; str:
    &quot;&quot;&quot;Get detailed pod info. Use for events and config.&quot;&quot;&quot;
    result = subprocess.run(
        [&quot;kubectl&quot;, &quot;describe&quot;, &quot;pod&quot;, pod_name],
        capture_output=True, text=True
    )
    return result.stdout</p>
<p>tools = [get_pod_status, get_pod_logs, describe_pod]</p>
<h1>2. Define prompt</h1>
react_prompt = &quot;&quot;&quot;
Answer the following question using these tools: {tools}
<p>Use this format:
Question: {input}
Thought: [your reasoning]
Action: [tool name]
Action Input: [tool input]
Observation: [tool output]
... (repeat as needed)
Final Answer: [complete answer]</p>
<p>Question: {input}
{agent_scratchpad}
&quot;&quot;&quot;</p>
<h1>3. Create agent</h1>
llm = ChatOpenAI(model=&quot;gpt-4&quot;, temperature=0.0)
prompt = PromptTemplate.from_template(react_prompt)
agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)
<h1>4. Create executor</h1>
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    max_iterations=5,
    verbose=True
)
<h1>5. Use it</h1>
result = agent_executor.invoke({
    &quot;input&quot;: &quot;Why is pod nginx-abc crashing?&quot;
})
print(result[&quot;output&quot;])</code></pre>
</details>
<h3><strong>Question 5</strong>: What's the purpose of handle_parsing_errors=True in AgentExecutor?</h3>
<details>
<summary>Show Answer</summary>
<strong>handle_parsing_errors=True</strong> makes the agent more robust by handling cases where the LLM generates malformed output.
<strong>Without it</strong>:
<pre><code class="language-python"># LLM outputs invalid format:
&quot;Action: GetPodStatus nginx-abc&quot;  # Missing proper JSON format
<h1>Result: Agent crashes with parsing error</code></pre></h1>
<strong>With it</strong>:
<pre><code class="language-python">agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    handle_parsing_errors=True  # ‚úÖ Graceful handling
)
<h1>LLM outputs invalid format ‚Üí Agent catches it and asks LLM to retry</h1>
<h1>&quot;Please format your response correctly using Action: ... and Action Input: ...&quot;</code></pre></h1>
<strong>Always use this in production</strong> to prevent crashes from occasional LLM formatting mistakes.
</details>
<p>---</p>
<h2>üöÄ <strong>Key Takeaways</strong></h2>
<p>1. <strong>LangChain provides abstractions</strong>: Less boilerplate, more functionality
2. <strong>Core components</strong>: LLM, Tools, Prompt, Agent, AgentExecutor, Memory
3. <strong>Tools need excellent docstrings</strong>: Agent uses them to decide when to call
4. <strong>Use ChatOpenAI with temperature=0.0</strong>: Deterministic tool calling
5. <strong>BufferWindowMemory is best</strong>: For your K8s agent (k=10)
6. <strong>Set max_iterations</strong>: Prevent infinite loops (5-10 typical)
7. <strong>Handle errors gracefully</strong>: Try/except in tools, handle_parsing_errors=True
8. <strong>Test incrementally</strong>: Build tools ‚Üí test individually ‚Üí combine into agent</p>
<p>---</p>
<h2>üîó <strong>Next Module</strong></h2>
<p>Move on to <strong>Module 8: Memory Types & Context Management</strong> for deeper understanding of conversation memory!</p>
<p>---</p>
<strong>Time to complete this module</strong>: 1 hour  
<strong>Hands-on practice</strong>: 30 minutes  
<strong>Total</strong>: ~1.5 hours

    </div>
    

    <div class="module-content" id="module-8">
        <h1>Module 8: Memory Types & Context Management</h1>
<strong>Study Time</strong>: ~45 minutes  
<strong>Prerequisites</strong>: Module 7 (LangChain Components)
<p>---</p>
<h2>üéØ <strong>Learning Objectives</strong></h2>
<p>By the end of this module, you'll understand:
1. Why memory is critical for conversational agents
2. Different memory types and when to use each
3. How to manage token limits with memory
4. Best practices for production memory management
5. How to choose the right memory for your K8s agent</p>
<p>---</p>
<h2>üß† <strong>Why Memory Matters</strong></h2>
<h3><strong>Without Memory</strong></h3>
<pre><code class="language-python">agent = AgentExecutor(agent=agent, tools=tools)  # No memory
<h1>Conversation 1</h1>
user: &quot;Check pod nginx-abc status&quot;
agent: [calls GetPodStatus] &quot;Pod is CrashLoopBackOff&quot;
<h1>Conversation 2</h1>
user: &quot;What are the logs?&quot;
agent: &quot;What pod? Please specify pod name.&quot;  ‚ùå</code></pre>
<strong>Problem</strong>: Agent has amnesia - doesn't remember we're talking about `nginx-abc`.
<h3><strong>With Memory</strong></h3>
<pre><code class="language-python">memory = ConversationBufferWindowMemory(k=10)
agent = AgentExecutor(agent=agent, tools=tools, memory=memory)
<h1>Conversation 1</h1>
user: &quot;Check pod nginx-abc status&quot;
agent: [calls GetPodStatus] &quot;Pod is CrashLoopBackOff&quot;
<h1>Conversation 2</h1>
user: &quot;What are the logs?&quot;
agent: [calls GetPodLogs for nginx-abc] &quot;Error: Config file not found&quot; ‚úÖ</code></pre>
<strong>Benefit</strong>: Agent remembers context from previous messages.
<p>---</p>
<h2>üìö <strong>Memory Types Overview</strong></h2>
<p>| Memory Type | Storage Strategy | Token Usage | Best For |
|-------------|------------------|-------------|----------|
| <strong>BufferMemory</strong> | All messages | Grows unbounded | Demos, short sessions |
| <strong>BufferWindowMemory</strong> ‚≠ê | Last N messages | Fixed, predictable | Your K8s agent |
| <strong>SummaryMemory</strong> | Summarized history | Low, but extra LLM calls | Long sessions |
| <strong>SummaryBufferMemory</strong> | Hybrid: recent + summary | Efficient | Production at scale |
| <strong>EntityMemory</strong> | Key facts about entities | Very efficient | Customer service |
| <strong>VectorStoreMemory</strong> | Semantic search | Flexible | Large knowledge bases |</p>
<p>---</p>
<h2>1Ô∏è‚É£ <strong>ConversationBufferMemory</strong></h2>
<h3><strong>How It Works</strong></h3>
<p>Stores <strong>all</strong> messages in order.</p>
<pre><code class="language-python">from langchain.memory import ConversationBufferMemory
<p>memory = ConversationBufferMemory()</p>
<h1>Message 1</h1>
memory.save_context(
    {&quot;input&quot;: &quot;Check pod nginx-abc&quot;},
    {&quot;output&quot;: &quot;Pod is Running&quot;}
)
<h1>Message 2</h1>
memory.save_context(
    {&quot;input&quot;: &quot;What are the logs?&quot;},
    {&quot;output&quot;: &quot;No errors found&quot;}
)
<h1>View history</h1>
print(memory.load_memory_variables({}))</code></pre>
<strong>Output</strong>:
<pre><code class="language-text">{
  &quot;history&quot;: &quot;&quot;&quot;
  Human: Check pod nginx-abc
  AI: Pod is Running
  Human: What are the logs?
  AI: No errors found
  &quot;&quot;&quot;
}</code></pre>
<h3><strong>Pros & Cons</strong></h3>
<p>‚úÖ <strong>Pros</strong>:
<ul><li>Simple to understand</li>
<li>Complete conversation history</li>
<li>No information loss</li></p>
<p>‚ùå <strong>Cons</strong>:
<li><strong>Token overflow</strong>: Long conversations exceed context window</li>
<li><strong>Cost</strong>: More tokens = more expensive</li>
<li><strong>Performance</strong>: Large history slows down responses</li></p>
<h3><strong>When to Use</strong></h3>
<li>‚úÖ Short demos (5-10 exchanges)</li>
<li>‚úÖ Testing and debugging</li>
<li>‚ùå Production (risk of token overflow)</li>
<li>‚ùå Long troubleshooting sessions</li>
<h3><strong>Example</strong></h3>
<pre><code class="language-python">from langchain.agents import AgentExecutor
from langchain.memory import ConversationBufferMemory
<p>memory = ConversationBufferMemory(
    memory_key=&quot;chat_history&quot;,
    return_messages=True  # Return as Message objects
)</p>
<p>agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    memory=memory,
    verbose=True
)</p>
<h1>Use it</h1>
result = agent_executor.invoke({&quot;input&quot;: &quot;Check pod nginx-abc&quot;})
<h1>Memory stores: &quot;Human: Check pod nginx-abc\nAI: [response]&quot;</code></pre></h1>
<p>---</p>
<h2>2Ô∏è‚É£ <strong>ConversationBufferWindowMemory</strong> ‚≠ê</h2>
<h3><strong>How It Works</strong></h3>
<p>Stores only the <strong>last N exchanges</strong> (sliding window).</p>
<pre><code class="language-python">from langchain.memory import ConversationBufferWindowMemory
<p>memory = ConversationBufferWindowMemory(
    k=3  # Keep only last 3 exchanges
)</p>
<h1>Exchanges 1-3 (all stored)</h1>
memory.save_context({&quot;input&quot;: &quot;Check pod nginx-abc&quot;}, {&quot;output&quot;: &quot;Running&quot;})
memory.save_context({&quot;input&quot;: &quot;Check pod redis-xyz&quot;}, {&quot;output&quot;: &quot;Crashing&quot;})
memory.save_context({&quot;input&quot;: &quot;What are redis logs?&quot;}, {&quot;output&quot;: &quot;OOMKilled&quot;})
<h1>Exchange 4 (oldest is dropped)</h1>
memory.save_context({&quot;input&quot;: &quot;How to fix?&quot;}, {&quot;output&quot;: &quot;Increase memory&quot;})
<h1>View memory (only last 3)</h1>
print(memory.load_memory_variables({}))</code></pre>
<strong>Output</strong>:
<pre><code class="language-text">{
  &quot;history&quot;: &quot;&quot;&quot;
  Human: Check pod redis-xyz
  AI: Crashing
  Human: What are redis logs?
  AI: OOMKilled
  Human: How to fix?
  AI: Increase memory
  &quot;&quot;&quot;
}</code></pre>
<strong>Note</strong>: First exchange ("Check pod nginx-abc") is dropped!
<h3><strong>Pros & Cons</strong></h3>
<p>‚úÖ <strong>Pros</strong>:
<li><strong>Fixed token usage</strong>: Never exceeds context window</li>
<li><strong>Predictable</strong>: Always uses exactly N exchanges</li>
<li><strong>Simple</strong>: Easy to reason about</li>
<li><strong>Cost-effective</strong>: Caps token usage</li></p>
<p>‚ùå <strong>Cons</strong>:
<li><strong>Loses old context</strong>: Early conversation is forgotten</li>
<li><strong>No long-term memory</strong>: Can't remember things from 20 messages ago</li></p>
<h3><strong>When to Use</strong></h3>
<li>‚úÖ <strong>Your K8s agent</strong> (perfect for debugging sessions)</li>
<li>‚úÖ Multi-turn troubleshooting (5-15 exchanges)</li>
<li>‚úÖ Production with moderate sessions</li>
<li>‚ùå Very long sessions where early context matters</li>
<h3><strong>Choosing k (Window Size)</strong></h3>
<pre><code class="language-python"># Small window (k=5): 5 exchanges = 10 messages
<h1>- User asks 5 questions</h1>
<h1>- Agent responds 5 times</h1>
<h1>- Total: 10 messages stored</h1>
<h1>Medium window (k=10): 10 exchanges = 20 messages</h1>
<h1>- Good balance for most use cases</h1>
<h1>‚≠ê Recommended for your K8s agent</h1>
<h1>Large window (k=20): 20 exchanges = 40 messages</h1>
<h1>- For complex multi-step debugging</h1>
<h1>- Watch out for token limits (especially with long tool outputs)</code></pre></h1>
<strong>Best Practice</strong>: Start with `k=10`, adjust based on your use case.
<h3><strong>Example for K8s Agent</strong></h3>
<pre><code class="language-python">from langchain.memory import ConversationBufferWindowMemory
<p>memory = ConversationBufferWindowMemory(
    k=10,  # Last 10 exchanges (20 messages)
    memory_key=&quot;chat_history&quot;,
    return_messages=True,
    input_key=&quot;input&quot;,  # Key for user input in invoke()
    output_key=&quot;output&quot;  # Key for agent output
)</p>
<p>agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    memory=memory,
    max_iterations=5
)</p>
<h1>Conversation</h1>
result1 = agent_executor.invoke({&quot;input&quot;: &quot;Check pod nginx-abc&quot;})
<h1>Memory: [User: &quot;Check pod nginx-abc&quot;, AI: &quot;Running&quot;]</h1>
<p>result2 = agent_executor.invoke({&quot;input&quot;: &quot;What are the logs?&quot;})
<h1>Memory: [User: &quot;Check pod nginx-abc&quot;, AI: &quot;Running&quot;, </h1>
<h1>         User: &quot;What are the logs?&quot;, AI: &quot;[logs]&quot;]</h1>
<h1>Agent knows we&#039;re talking about nginx-abc!</code></pre></h1></p>
<p>---</p>
<h2>3Ô∏è‚É£ <strong>ConversationSummaryMemory</strong></h2>
<h3><strong>How It Works</strong></h3>
<p>Uses an LLM to <strong>summarize</strong> old messages, keeping only the summary.</p>
<pre><code class="language-python">from langchain.memory import ConversationSummaryMemory
from langchain_openai import ChatOpenAI
<p>llm = ChatOpenAI(temperature=0.0)</p>
<p>memory = ConversationSummaryMemory(
    llm=llm,  # LLM creates summaries
    memory_key=&quot;chat_history&quot;
)</p>
<h1>Long conversation</h1>
memory.save_context(
    {&quot;input&quot;: &quot;Check pod nginx-abc status&quot;},
    {&quot;output&quot;: &quot;Pod nginx-abc is in CrashLoopBackOff state. Exit code 1.&quot;}
)
memory.save_context(
    {&quot;input&quot;: &quot;What are the logs?&quot;},
    {&quot;output&quot;: &quot;Logs show: Error: Cannot connect to database at db-service:5432&quot;}
)
memory.save_context(
    {&quot;input&quot;: &quot;Is the database pod running?&quot;},
    {&quot;output&quot;: &quot;Yes, database pod is Running and healthy.&quot;}
)
<h1>View memory (summarized)</h1>
print(memory.load_memory_variables({}))</code></pre>
<strong>Output</strong> (summarized by LLM):
<pre><code class="language-text">{
  &quot;history&quot;: &quot;User is troubleshooting pod nginx-abc which is crashing due to 
  inability to connect to database at db-service:5432. Database pod itself 
  is healthy and running.&quot;
}</code></pre>
<h3><strong>Pros & Cons</strong></h3>
<p>‚úÖ <strong>Pros</strong>:
<li><strong>Token efficient</strong>: Summary is much shorter than full history</li>
<li><strong>Retains key info</strong>: Important details preserved</li>
<li><strong>No context window overflow</strong>: Can handle very long sessions</li></p>
<p>‚ùå <strong>Cons</strong>:
<li><strong>Extra LLM calls</strong>: Every save_context calls LLM (costs money, slower)</li>
<li><strong>Information loss</strong>: Summary might miss nuances</li>
<li><strong>Less predictable</strong>: Summary quality varies</li></p>
<h3><strong>When to Use</strong></h3>
<li>‚úÖ Very long sessions (50+ exchanges)</li>
<li>‚úÖ When token budget is critical</li>
<li>‚ùå Your K8s agent (BufferWindow is simpler and sufficient)</li>
<li>‚ùå When you need exact message history</li>
<h3><strong>Example</strong></h3>
<pre><code class="language-python">from langchain.memory import ConversationSummaryMemory
from langchain_openai import ChatOpenAI
<p>llm = ChatOpenAI(temperature=0.0)</p>
<p>memory = ConversationSummaryMemory(
    llm=llm,
    memory_key=&quot;chat_history&quot;,
    return_messages=True
)</p>
<p>agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    memory=memory
)</p>
<h1>Use it (LLM automatically summarizes after each exchange)</h1>
result = agent_executor.invoke({&quot;input&quot;: &quot;Check pod nginx-abc&quot;})</code></pre>
<p>---</p>
<h2>4Ô∏è‚É£ <strong>ConversationSummaryBufferMemory</strong></h2>
<h3><strong>How It Works</strong></h3>
<p>Hybrid approach: Keep recent messages verbatim + summarize older ones.</p>
<pre><code class="language-python">from langchain.memory import ConversationSummaryBufferMemory
from langchain_openai import ChatOpenAI
<p>llm = ChatOpenAI(temperature=0.0)</p>
<p>memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=200,  # When history exceeds 200 tokens, summarize oldest
    memory_key=&quot;chat_history&quot;
)</p>
<h1>Messages 1-5 (under 200 tokens, kept verbatim)</h1>
for i in range(5):
    memory.save_context(
        {&quot;input&quot;: f&quot;Question {i}&quot;},
        {&quot;output&quot;: f&quot;Answer {i}&quot;}
    )
<h1>Message 6 (exceeds 200 tokens)</h1>
<h1>‚Üí Oldest messages summarized, recent ones kept verbatim</h1>
memory.save_context(
    {&quot;input&quot;: &quot;Question 6&quot;},
    {&quot;output&quot;: &quot;Answer 6&quot;}
)</code></pre>
<strong>Memory contents</strong>:
<pre><code class="language-text">Summary: &quot;User asked questions 1-3 about pod status. Agent confirmed pods are running.&quot;
Human: Question 4
AI: Answer 4
Human: Question 5
AI: Answer 5
Human: Question 6
AI: Answer 6</code></pre>
<h3><strong>Pros & Cons</strong></h3>
<p>‚úÖ <strong>Pros</strong>:
<li><strong>Best of both worlds</strong>: Recent context verbatim, old context summarized</li>
<li><strong>Token efficient</strong>: Doesn't overflow</li>
<li><strong>Retains recent details</strong>: Last N messages are exact</li></p>
<p>‚ùå <strong>Cons</strong>:
<li><strong>Most complex</strong>: Harder to understand and debug</li>
<li><strong>Extra LLM calls</strong>: For summarization (costs money)</li>
<li><strong>Overkill for simple use cases</strong></li></p>
<h3><strong>When to Use</strong></h3>
<li>‚úÖ Production at scale (thousands of users, long sessions)</li>
<li>‚úÖ Customer service bots (need full history)</li>
<li>‚ùå Your K8s agent demo (BufferWindow is sufficient)</li>
<p>---</p>
<h2>üéØ <strong>Choosing Memory for Your K8s Agent</strong></h2>
<h3><strong>Recommendation: ConversationBufferWindowMemory</strong> ‚≠ê</h3>
<pre><code class="language-python">from langchain.memory import ConversationBufferWindowMemory
<p>memory = ConversationBufferWindowMemory(
    k=10,  # Last 10 exchanges (20 messages)
    memory_key=&quot;chat_history&quot;,
    return_messages=True
)</code></pre></p>
<h3><strong>Why?</strong></h3>
<p>1. <strong>Debugging sessions are short</strong>: Typical troubleshooting is 5-15 exchanges
2. <strong>Need recent context</strong>: "What are the logs?" requires remembering which pod
3. <strong>Predictable</strong>: Fixed token usage, won't overflow
4. <strong>Simple</strong>: Easy to understand and debug
5. <strong>No extra LLM calls</strong>: Unlike SummaryMemory (no added cost/latency)</p>
<h3><strong>Interview Answer</strong></h3>
<p>*"I chose ConversationBufferWindowMemory with k=10 for my K8s agent because:*</p>
<p>1. *Debugging sessions typically involve 5-15 exchanges, so a window of 10 is sufficient*
2. *It prevents token overflow while maintaining recent context*
3. *Fixed token usage makes performance predictable*
4. *Simpler than SummaryMemory (no extra LLM calls)*
5. *If I needed to support longer sessions in production, I'd upgrade to SummaryBufferMemory"*</p>
<p>---</p>
<h2>üîß <strong>Advanced Memory Patterns</strong></h2>
<h3><strong>Pattern 1: Separate Memory per Session</strong></h3>
<pre><code class="language-python">from uuid import uuid4
<h1>Store memories per user session</h1>
session_memories = {}
<p>def get_or_create_memory(session_id: str):
    if session_id not in session_memories:
        session_memories[session_id] = ConversationBufferWindowMemory(k=10)
    return session_memories[session_id]</p>
<h1>Use it</h1>
session_id = str(uuid4())  # Generate for new user
memory = get_or_create_memory(session_id)
<p>agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory)</code></pre></p>
<h3><strong>Pattern 2: Custom Memory with Redis</strong></h3>
<pre><code class="language-python">import redis
import json
<p>class RedisMemory:
    def __init__(self, session_id: str, redis_client):
        self.session_id = session_id
        self.redis = redis_client
        self.memory_key = f&quot;memory:{session_id}&quot;
        
    def save_context(self, inputs, outputs):
        # Load existing history
        history = self.load_memory_variables({}).get(&quot;history&quot;, [])
        
        # Append new message
        history.append({&quot;human&quot;: inputs[&quot;input&quot;], &quot;ai&quot;: outputs[&quot;output&quot;]})
        
        # Keep only last 10
        history = history[-10:]
        
        # Save to Redis
        self.redis.set(self.memory_key, json.dumps(history))
        
    def load_memory_variables(self, inputs):
        # Load from Redis
        data = self.redis.get(self.memory_key)
        if data:
            history = json.loads(data)
            formatted = &quot;\n&quot;.join([
                f&quot;Human: {msg[&#039;human&#039;]}\nAI: {msg[&#039;ai&#039;]}&quot;
                for msg in history
            ])
            return {&quot;history&quot;: formatted}
        return {&quot;history&quot;: &quot;&quot;}</p>
<h1>Use it</h1>
redis_client = redis.Redis(host=&#039;localhost&#039;, port=6379)
memory = RedisMemory(session_id=&quot;user123&quot;, redis_client=redis_client)</code></pre>
<h3><strong>Pattern 3: Memory with Context Reset</strong></h3>
<pre><code class="language-python">class ResettableMemory:
    def __init__(self, k=10):
        self.memory = ConversationBufferWindowMemory(k=k)
        
    def clear(self):
        &quot;&quot;&quot;Reset memory when switching topics&quot;&quot;&quot;
        self.memory.clear()
        
    def invoke_with_reset_check(self, agent_executor, user_input):
        # Check if user wants to change topic
        if &quot;new pod&quot; in user_input.lower() or &quot;different pod&quot; in user_input.lower():
            print(&quot;Detected topic change, clearing memory...&quot;)
            self.clear()
            
        return agent_executor.invoke({&quot;input&quot;: user_input})
<h1>Use it</h1>
memory_manager = ResettableMemory(k=10)
agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory_manager.memory)
<h1>User switches pods</h1>
result = memory_manager.invoke_with_reset_check(
    agent_executor,
    &quot;Now check a different pod: redis-xyz&quot;
)</code></pre>
<p>---</p>
<h2>üìä <strong>Token Management</strong></h2>
<h3><strong>Calculating Memory Token Usage</strong></h3>
<pre><code class="language-python">import tiktoken
<p>def count_tokens(text: str, model: str = &quot;gpt-4&quot;) -&gt; int:
    &quot;&quot;&quot;Count tokens in text&quot;&quot;&quot;
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))</p>
<h1>Check memory size</h1>
memory_text = memory.load_memory_variables({})[&quot;history&quot;]
tokens = count_tokens(memory_text)
<p>print(f&quot;Current memory: {tokens} tokens&quot;)</p>
<h1>Warn if approaching limit</h1>
if tokens &gt; 6000:  # GPT-4 has 8K context
    print(&quot;‚ö†Ô∏è Memory approaching token limit!&quot;)</code></pre>
<h3><strong>Managing Token Overflow</strong></h3>
<pre><code class="language-python">class TokenAwareMemory:
    def __init__(self, max_tokens=2000, model=&quot;gpt-4&quot;):
        self.max_tokens = max_tokens
        self.model = model
        self.memory = ConversationBufferMemory()
        
    def save_context(self, inputs, outputs):
        # Save to memory
        self.memory.save_context(inputs, outputs)
        
        # Check token count
        history = self.memory.load_memory_variables({})[&quot;history&quot;]
        tokens = count_tokens(history, self.model)
        
        # If exceeds max, trim oldest messages
        if tokens &gt; self.max_tokens:
            print(f&quot;Memory exceeded {self.max_tokens} tokens, trimming...&quot;)
            # Switch to window memory or summarize
            self._trim_memory()
            
    def _trim_memory(self):
        # Implementation: Remove oldest messages
        pass</code></pre>
<p>---</p>
<h2>üéì <strong>Self-Check Questions</strong></h2>
<h3><strong>Question 1</strong>: What's the key difference between BufferMemory and BufferWindowMemory?</h3>
<details>
<summary>Show Answer</summary>
<strong>ConversationBufferMemory</strong>:
<li>Stores ALL messages</li>
<li>Token usage grows unbounded</li>
<li>Can overflow context window in long conversations</li>
<strong>ConversationBufferWindowMemory</strong>:
<li>Stores only last `k` exchanges (sliding window)</li>
<li>Fixed token usage (predictable)</li>
<li>Loses old context but prevents overflow</li>
<strong>Example</strong>:
<pre><code class="language-python"># BufferMemory: Stores all 100 exchanges (might overflow!)
buffer = ConversationBufferMemory()
<h1>BufferWindowMemory: Stores only last 10 exchanges (safe)</h1>
window = ConversationBufferWindowMemory(k=10)</code></pre>
<strong>For your K8s agent</strong>: Use BufferWindowMemory with k=10.
</details>
<h3><strong>Question 2</strong>: When would you use ConversationSummaryMemory?</h3>
<details>
<summary>Show Answer</summary>
<strong>Use ConversationSummaryMemory when</strong>:
<li>‚úÖ Very long sessions (50+ exchanges)</li>
<li>‚úÖ Token budget is critical</li>
<li>‚úÖ Need to retain key facts from old messages</li>
<li>‚úÖ Willing to accept extra LLM calls for summarization</li>
<strong>Don't use when</strong>:
<li>‚ùå Short sessions (BufferWindow is simpler)</li>
<li>‚ùå Need exact message history</li>
<li>‚ùå Want to minimize LLM calls (each save triggers summarization)</li>
<strong>For your K8s agent</strong>: BufferWindowMemory is sufficient since debugging sessions are typically short (5-15 exchanges).
</details>
<h3><strong>Question 3</strong>: Why is k=10 a good choice for BufferWindowMemory in a K8s agent?</h3>
<details>
<summary>Show Answer</summary>
<strong>k=10 means</strong>:
<li>Last 10 exchanges</li>
<li>= 20 total messages (10 user + 10 agent)</li>
<li>‚âà 2000-3000 tokens (depending on message length)</li>
<strong>Why it's good for K8s troubleshooting</strong>:
1. <strong>Typical debugging flow</strong>: 5-10 exchanges
   - "Check pod status" ‚Üí "Get logs" ‚Üí "Describe pod" ‚Üí "Fix recommendation"
   
2. <strong>Maintains context</strong>: Agent remembers which pod we're discussing
<p>3. <strong>Won't overflow</strong>: Even with long tool outputs, stays under context window</p>
<p>4. <strong>Efficient</strong>: Not storing unnecessary old context</p>
<strong>If sessions were longer</strong>: Use k=20 or upgrade to SummaryBufferMemory.
</details>
<h3><strong>Question 4</strong>: What happens if you don't use any memory with an agent?</h3>
<details>
<summary>Show Answer</summary>
<strong>Without memory</strong>:
<pre><code class="language-python">agent_executor = AgentExecutor(agent=agent, tools=tools)  # No memory
<h1>Exchange 1</h1>
user: &quot;Check pod nginx-abc&quot;
agent: &quot;Pod is Running&quot; ‚úÖ
<h1>Exchange 2</h1>
user: &quot;What are the logs?&quot;
agent: &quot;Which pod? Please specify.&quot; ‚ùå  &lt;-- Forgot nginx-abc!</code></pre>
<strong>Problems</strong>:
<li>Agent has no context from previous messages</li>
<li>User must repeat information every time</li>
<li>Poor user experience</li>
<li>Can't do multi-turn debugging</li></ul>
<strong>Solution</strong>: Always add memory for conversational agents.
<pre><code class="language-python">memory = ConversationBufferWindowMemory(k=10)
agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory)</code></pre>
</details>
<h3><strong>Question 5</strong>: How would you clear memory when a user wants to troubleshoot a different pod?</h3>
<details>
<summary>Show Answer</summary>
<strong>Option 1: Manual clear</strong>
<pre><code class="language-python">memory.clear()  # Wipes all history
<h1>Now start fresh with new pod</h1>
result = agent_executor.invoke({&quot;input&quot;: &quot;Check pod redis-xyz&quot;})</code></pre>
<strong>Option 2: Detect topic change</strong>
<pre><code class="language-python">def invoke_with_topic_detection(user_input):
    # Keywords suggesting new topic
    if any(word in user_input.lower() for word in [&quot;new pod&quot;, &quot;different pod&quot;, &quot;switch to&quot;]):
        print(&quot;Detected new topic, clearing memory...&quot;)
        memory.clear()
    
    return agent_executor.invoke({&quot;input&quot;: user_input})
<h1>Use it</h1>
result = invoke_with_topic_detection(&quot;Now check a different pod: redis-xyz&quot;)</code></pre>
<strong>Option 3: Explicit reset command</strong>
<pre><code class="language-python">if user_input.strip() == &quot;/reset&quot;:
    memory.clear()
    return &quot;Memory cleared. Ready for new topic!&quot;
<p>result = agent_executor.invoke({&quot;input&quot;: user_input})</code></pre></p>
<strong>Best practice</strong>: Make it explicit - add a "clear history" button or command.
</details>
<p>---</p>
<h2>üöÄ <strong>Key Takeaways</strong></h2>
<p>1. <strong>Memory is essential</strong>: Enables multi-turn conversations
2. <strong>BufferWindowMemory is best</strong>: For your K8s agent (k=10)
3. <strong>Fixed window prevents overflow</strong>: Predictable token usage
4. <strong>SummaryMemory for long sessions</strong>: But adds complexity and cost
5. <strong>Monitor token usage</strong>: Especially with long tool outputs
6. <strong>Clear memory when switching topics</strong>: Better user experience
7. <strong>Use return_messages=True</strong>: For better integration with agents</p>
<p>---</p>
<h2>üîó <strong>Next Module</strong></h2>
<p>Move on to <strong>Module 9: Output Parsers & Structured Outputs</strong> to learn how to get consistent, parseable responses!</p>
<p>---</p>
<strong>Time to complete this module</strong>: 45 minutes  
<strong>Hands-on practice</strong>: 20 minutes  
<strong>Total</strong>: ~1 hour

    </div>
    

    <div class="module-content" id="module-9">
        <h1>Module 9: Output Parsers & Structured Outputs</h1>
<strong>Study Time</strong>: ~30 minutes  
<strong>Prerequisites</strong>: Module 7 (LangChain Components)
<p>---</p>
<h2>üéØ <strong>Learning Objectives</strong></h2>
<p>By the end of this module, you'll understand:
1. Why structured outputs matter
2. Different output parser types
3. How to enforce JSON/structured responses
4. Error handling for malformed outputs
5. Best practices for production systems</p>
<p>---</p>
<h2>üì§ <strong>Why Structured Outputs?</strong></h2>
<h3><strong>Problem: Unpredictable Text</strong></h3>
<p>LLMs return free-form text by default:</p>
<pre><code class="language-python">user: &quot;Diagnose pod nginx-abc&quot;
agent: &quot;Well, the pod nginx-abc seems to be experiencing some issues. 
        It&#039;s in CrashLoopBackOff state, which means... [long paragraph]&quot;</code></pre>
<strong>Issues</strong>:
<ul><li>‚ùå Hard to parse programmatically</li>
<li>‚ùå Inconsistent format</li>
<li>‚ùå Can't easily extract specific fields</li>
<li>‚ùå Difficult to display in UI (tables, cards, etc.)</li>
<h3><strong>Solution: Structured Outputs</strong></h3>
<pre><code class="language-python">user: &quot;Diagnose pod nginx-abc&quot;
agent: {
  &quot;pod_name&quot;: &quot;nginx-abc&quot;,
  &quot;status&quot;: &quot;CrashLoopBackOff&quot;,
  &quot;root_cause&quot;: &quot;Missing ConfigMap &#039;nginx-config&#039;&quot;,
  &quot;severity&quot;: &quot;high&quot;,
  &quot;recommended_fixes&quot;: [
    &quot;Create ConfigMap &#039;nginx-config&#039; with nginx.conf&quot;,
    &quot;Or update Deployment to reference existing ConfigMap&quot;
  ]
}</code></pre>
<strong>Benefits</strong>:
<li>‚úÖ Easy to parse (JSON)</li>
<li>‚úÖ Consistent structure</li>
<li>‚úÖ Can extract specific fields</li>
<li>‚úÖ Easy to display in UI</li>
<li>‚úÖ Type-safe (if using Pydantic)</li>
<p>---</p>
<h2>üîß <strong>Output Parser Types</strong></h2>
<h3><strong>1. StrOutputParser (Default)</strong></h3>
<p>Returns plain string.</p>
<pre><code class="language-python">from langchain_core.output_parsers import StrOutputParser
<p>parser = StrOutputParser()</p>
<h1>LLM output: &quot;The pod is crashing&quot;</h1>
result = parser.parse(&quot;The pod is crashing&quot;)
print(result)  # &quot;The pod is crashing&quot;</code></pre>
<strong>Use when</strong>: You want free-form text (explanations, summaries).
<h3><strong>2. JSONOutputParser</strong></h3>
<p>Parses JSON from LLM output.</p>
<pre><code class="language-python">from langchain.output_parsers import ResponseSchema, StructuredOutputParser
<h1>Define expected fields</h1>
response_schemas = [
    ResponseSchema(name=&quot;pod_name&quot;, description=&quot;Name of the pod&quot;),
    ResponseSchema(name=&quot;status&quot;, description=&quot;Current pod status&quot;),
    ResponseSchema(name=&quot;root_cause&quot;, description=&quot;Why pod is failing&quot;)
]
<p>parser = StructuredOutputParser.from_response_schemas(response_schemas)</p>
<h1>Get format instructions</h1>
format_instructions = parser.get_format_instructions()
print(format_instructions)</code></pre>
<strong>Output</strong> (format instructions for LLM):
<pre><code class="language-text">The output should be a markdown code snippet formatted in the following schema:</code></pre>json
{
    "pod_name": string  // Name of the pod
    "status": string  // Current pod status
    "root_cause": string  // Why pod is failing
}
<pre><code class="language-text"></code></pre>
<strong>Usage</strong>:
<pre><code class="language-python">from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
<h1>Create prompt with format instructions</h1>
prompt = PromptTemplate(
    template=&quot;Diagnose this pod: {pod_name}\n\n{format_instructions}&quot;,
    input_variables=[&quot;pod_name&quot;],
    partial_variables={&quot;format_instructions&quot;: parser.get_format_instructions()}
)
<h1>Create chain</h1>
llm = ChatOpenAI(temperature=0.0)
chain = prompt | llm | parser
<h1>Invoke</h1>
result = chain.invoke({&quot;pod_name&quot;: &quot;nginx-abc&quot;})
print(result)
<h1>Output: {&quot;pod_name&quot;: &quot;nginx-abc&quot;, &quot;status&quot;: &quot;CrashLoopBackOff&quot;, &quot;root_cause&quot;: &quot;...&quot;}</code></pre></h1>
<h3><strong>3. PydanticOutputParser</strong> ‚≠ê</h3>
<p>Type-safe structured outputs using Pydantic models.</p>
<pre><code class="language-python">from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from typing import List
<h1>Define output schema</h1>
class PodDiagnosis(BaseModel):
    pod_name: str = Field(description=&quot;Name of the pod&quot;)
    namespace: str = Field(description=&quot;Kubernetes namespace&quot;)
    status: str = Field(description=&quot;Pod status (Running, CrashLoopBackOff, etc.)&quot;)
    root_cause: str = Field(description=&quot;Root cause of the issue&quot;)
    severity: str = Field(description=&quot;Severity: low, medium, high, critical&quot;)
    recommended_fixes: List[str] = Field(description=&quot;List of recommended fixes&quot;)
<h1>Create parser</h1>
parser = PydanticOutputParser(pydantic_object=PodDiagnosis)
<h1>Get format instructions</h1>
format_instructions = parser.get_format_instructions()</code></pre>
<strong>Format instructions output</strong>:
<pre><code class="language-text">The output should be formatted as a JSON instance that conforms to the JSON schema below.
<p>{
  &quot;properties&quot;: {
    &quot;pod_name&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Name of the pod&quot;},
    &quot;namespace&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Kubernetes namespace&quot;},
    &quot;status&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Pod status&quot;},
    &quot;root_cause&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Root cause&quot;},
    &quot;severity&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Severity: low, medium, high, critical&quot;},
    &quot;recommended_fixes&quot;: {&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;string&quot;}}
  },
  &quot;required&quot;: [&quot;pod_name&quot;, &quot;namespace&quot;, &quot;status&quot;, &quot;root_cause&quot;, &quot;severity&quot;, &quot;recommended_fixes&quot;]
}</code></pre></p>
<strong>Usage</strong>:
<pre><code class="language-python">from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
<p>prompt = PromptTemplate(
    template=&quot;&quot;&quot;
    You are a Kubernetes troubleshooting expert.
    Diagnose this pod: {pod_name}
    
    {format_instructions}
    &quot;&quot;&quot;,
    input_variables=[&quot;pod_name&quot;],
    partial_variables={&quot;format_instructions&quot;: parser.get_format_instructions()}
)</p>
<p>llm = ChatOpenAI(model=&quot;gpt-4&quot;, temperature=0.0)
chain = prompt | llm | parser</p>
<h1>Invoke</h1>
result: PodDiagnosis = chain.invoke({&quot;pod_name&quot;: &quot;nginx-abc&quot;})
<h1>Type-safe access</h1>
print(result.pod_name)  # &quot;nginx-abc&quot;
print(result.status)  # &quot;CrashLoopBackOff&quot;
print(result.severity)  # &quot;high&quot;
print(result.recommended_fixes[0])  # First fix</code></pre>
<strong>Benefits</strong>:
<li>‚úÖ Type safety (IDE autocomplete, type checking)</li>
<li>‚úÖ Validation (Pydantic validates types)</li>
<li>‚úÖ Clear schema definition</li>
<li>‚úÖ Easy to extend</li>
<h3><strong>4. OutputFixingParser</strong></h3>
<p>Automatically fixes malformed outputs.</p>
<pre><code class="language-python">from langchain.output_parsers import OutputFixingParser
from langchain_openai import ChatOpenAI
<h1>Original parser</h1>
base_parser = PydanticOutputParser(pydantic_object=PodDiagnosis)
<h1>Wrap with fixing parser</h1>
fixing_parser = OutputFixingParser.from_llm(
    parser=base_parser,
    llm=ChatOpenAI(model=&quot;gpt-4&quot;, temperature=0.0)
)
<h1>If LLM returns malformed JSON, fixing parser uses another LLM call to fix it</h1>
malformed_output = &#039;{&quot;pod_name&quot;: &quot;nginx-abc&quot;, &quot;status&quot;: &quot;CrashLoopBackOff&quot;&#039;  # Missing closing }
<p>try:
    result = base_parser.parse(malformed_output)
except Exception as e:
    print(f&quot;Base parser failed: {e}&quot;)
    
    # Fixing parser tries to fix it
    result = fixing_parser.parse(malformed_output)
    print(f&quot;Fixed! {result}&quot;)</code></pre></p>
<strong>Trade-off</strong>:
<li>‚úÖ More robust (handles malformed outputs)</li>
<li>‚ùå Extra LLM call (costs money, adds latency)</li>
<p>---</p>
<h2>üé® <strong>Complete Example: Structured K8s Diagnosis</strong></h2>
<pre><code class="language-python">from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field
from typing import List, Literal
<h1>1. Define output schema</h1>
class PodDiagnosis(BaseModel):
    pod_name: str = Field(description=&quot;Name of the pod being diagnosed&quot;)
    namespace: str = Field(default=&quot;default&quot;, description=&quot;Kubernetes namespace&quot;)
    current_status: Literal[&quot;Running&quot;, &quot;Pending&quot;, &quot;CrashLoopBackOff&quot;, &quot;Error&quot;, &quot;Unknown&quot;] = \
        Field(description=&quot;Current pod status&quot;)
    root_cause: str = Field(description=&quot;Root cause of the issue&quot;)
    severity: Literal[&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;critical&quot;] = \
        Field(description=&quot;Issue severity&quot;)
    affected_components: List[str] = Field(
        description=&quot;List of affected components (containers, volumes, etc.)&quot;
    )
    recommended_fixes: List[str] = Field(
        description=&quot;Ordered list of recommended fixes, most likely first&quot;
    )
    kubectl_commands: List[str] = Field(
        description=&quot;Kubectl commands to apply fixes&quot;
    )
<h1>2. Create parser</h1>
parser = PydanticOutputParser(pydantic_object=PodDiagnosis)
<h1>3. Create prompt</h1>
prompt = PromptTemplate(
    template=&quot;&quot;&quot;
    You are a Kubernetes expert. Diagnose the following pod issue:
    
    Pod Name: {pod_name}
    Namespace: {namespace}
    Status: {status}
    Logs: {logs}
    Events: {events}
    
    Provide a complete diagnosis.
    
    {format_instructions}
    &quot;&quot;&quot;,
    input_variables=[&quot;pod_name&quot;, &quot;namespace&quot;, &quot;status&quot;, &quot;logs&quot;, &quot;events&quot;],
    partial_variables={&quot;format_instructions&quot;: parser.get_format_instructions()}
)
<h1>4. Create chain</h1>
llm = ChatOpenAI(model=&quot;gpt-4&quot;, temperature=0.0)
diagnosis_chain = prompt | llm | parser
<h1>5. Use it</h1>
result: PodDiagnosis = diagnosis_chain.invoke({
    &quot;pod_name&quot;: &quot;nginx-abc123&quot;,
    &quot;namespace&quot;: &quot;production&quot;,
    &quot;status&quot;: &quot;CrashLoopBackOff&quot;,
    &quot;logs&quot;: &quot;Error: Config file /etc/nginx/nginx.conf not found&quot;,
    &quot;events&quot;: &quot;MountVolume.SetUp failed: configmap &#039;nginx-config&#039; not found&quot;
})
<h1>6. Access structured data</h1>
print(f&quot;Pod: {result.pod_name}&quot;)
print(f&quot;Status: {result.current_status}&quot;)
print(f&quot;Severity: {result.severity}&quot;)
print(f&quot;Root Cause: {result.root_cause}&quot;)
print(&quot;\nRecommended Fixes:&quot;)
for i, fix in enumerate(result.recommended_fixes, 1):
    print(f&quot;  {i}. {fix}&quot;)
print(&quot;\nCommands:&quot;)
for cmd in enumerate(result.kubectl_commands):
    print(f&quot;  $ {cmd}&quot;)</code></pre>
<strong>Example Output</strong>:
<pre><code class="language-text">Pod: nginx-abc123
Status: CrashLoopBackOff
Severity: high
Root Cause: Pod is trying to mount ConfigMap &#039;nginx-config&#039; which doesn&#039;t exist
<p>Recommended Fixes:
  1. Create the missing ConfigMap &#039;nginx-config&#039; with required nginx configuration
  2. Update Deployment to reference an existing ConfigMap
  3. Verify ConfigMap name matches what&#039;s specified in pod spec</p>
<p>Commands:
  $ kubectl create configmap nginx-config --from-file=nginx.conf
  $ kubectl get configmap -n production
  $ kubectl describe deployment nginx -n production</code></pre></p>
<p>---</p>
<h2>üîß <strong>Using with Agents</strong></h2>
<h3><strong>Challenge</strong></h3>
<p>Agents use ReAct pattern (Thought ‚Üí Action ‚Üí Observation), which conflicts with structured output.</p>
<h3><strong>Solution: Separate Final Answer Parsing</strong></h3>
<pre><code class="language-python">from langchain.agents import create_react_agent, AgentExecutor
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
<p>class DiagnosisResult(BaseModel):
    root_cause: str = Field(description=&quot;Root cause&quot;)
    fixes: List[str] = Field(description=&quot;Recommended fixes&quot;)</p>
<h1>Create agent as usual</h1>
agent = create_react_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools)
<h1>Get agent output</h1>
agent_output = agent_executor.invoke({&quot;input&quot;: &quot;Diagnose pod nginx-abc&quot;})
<h1>Parse final answer into structured format</h1>
parser = PydanticOutputParser(pydantic_object=DiagnosisResult)
<p>parse_prompt = f&quot;&quot;&quot;
Extract structured information from this diagnosis:</p>
<p>{agent_output[&#039;output&#039;]}</p>
<p>{parser.get_format_instructions()}
&quot;&quot;&quot;</p>
<p>structured_result = (llm | parser).invoke(parse_prompt)
print(structured_result.root_cause)
print(structured_result.fixes)</code></pre></p>
<strong>Pattern</strong>: Let agent work naturally, then structure the final output.
<p>---</p>
<h2>‚ö†Ô∏è <strong>Handling Parsing Errors</strong></h2>
<h3><strong>Problem</strong></h3>
<p>LLM might not follow format perfectly:</p>
<pre><code class="language-python"># Expected:
{&quot;pod_name&quot;: &quot;nginx-abc&quot;, &quot;status&quot;: &quot;Running&quot;}
<h1>Actual LLM output:</h1>
&quot;The pod nginx-abc is Running&quot;  # Not JSON!</code></pre>
<h3><strong>Solution 1: OutputFixingParser</strong></h3>
<pre><code class="language-python">from langchain.output_parsers import OutputFixingParser
<p>base_parser = PydanticOutputParser(pydantic_object=PodDiagnosis)
fixing_parser = OutputFixingParser.from_llm(
    parser=base_parser,
    llm=ChatOpenAI(model=&quot;gpt-4&quot;)
)</p>
<h1>Automatically fixes malformed output</h1>
result = fixing_parser.parse(malformed_output)</code></pre>
<h3><strong>Solution 2: RetryOutputParser</strong></h3>
<pre><code class="language-python">from langchain.output_parsers import RetryWithErrorOutputParser
<p>base_parser = PydanticOutputParser(pydantic_object=PodDiagnosis)</p>
<h1>If parsing fails, retry with error message</h1>
retry_parser = RetryWithErrorOutputParser.from_llm(
    parser=base_parser,
    llm=ChatOpenAI(model=&quot;gpt-4&quot;)
)
<h1>Parse with original prompt</h1>
try:
    result = base_parser.parse(llm_output)
except Exception as e:
    # Retry with error context
    result = retry_parser.parse_with_prompt(llm_output, original_prompt)</code></pre>
<h3><strong>Solution 3: Try-Except with Fallback</strong></h3>
<pre><code class="language-python">def safe_parse(output: str, parser: PydanticOutputParser):
    try:
        return parser.parse(output)
    except Exception as e:
        print(f&quot;Parsing failed: {e}&quot;)
        # Return default structure
        return PodDiagnosis(
            pod_name=&quot;unknown&quot;,
            namespace=&quot;default&quot;,
            current_status=&quot;Unknown&quot;,
            root_cause=&quot;Failed to parse diagnosis&quot;,
            severity=&quot;low&quot;,
            affected_components=[],
            recommended_fixes=[&quot;Manual investigation required&quot;],
            kubectl_commands=[]
        )
<p>result = safe_parse(llm_output, parser)</code></pre></p>
<p>---</p>
<h2>üéØ <strong>Best Practices</strong></h2>
<h3><strong>1. Keep Schemas Simple</strong></h3>
<p>‚ùå <strong>Bad</strong> (too complex):
<pre><code class="language-python">class OverlyComplexDiagnosis(BaseModel):
    pod: PodInfo
    containers: List[ContainerInfo]
    volumes: List[VolumeInfo]
    network: NetworkInfo
    resource_usage: ResourceMetrics
    historical_data: List[HistoricalEvent]
    # ... 20 more fields</code></pre></p>
<p>‚úÖ <strong>Good</strong> (focused):
<pre><code class="language-python">class SimpleDiagnosis(BaseModel):
    pod_name: str
    status: str
    root_cause: str
    fixes: List[str]</code></pre></p>
<strong>Why</strong>: Simpler schemas have higher LLM compliance rates.
<h3><strong>2. Use Descriptive Field Names</strong></h3>
<p>‚ùå <strong>Bad</strong>:
<pre><code class="language-python">class Diagnosis(BaseModel):
    n: str  # What is &#039;n&#039;?
    s: str  # What is &#039;s&#039;?
    f: List[str]  # What is &#039;f&#039;?</code></pre></p>
<p>‚úÖ <strong>Good</strong>:
<pre><code class="language-python">class Diagnosis(BaseModel):
    pod_name: str = Field(description=&quot;Name of the pod&quot;)
    status: str = Field(description=&quot;Current status&quot;)
    fixes: List[str] = Field(description=&quot;Recommended fixes&quot;)</code></pre></p>
<h3><strong>3. Provide Examples in Descriptions</strong></h3>
<pre><code class="language-python">class Diagnosis(BaseModel):
    severity: str = Field(
        description=&quot;Severity level. Must be one of: low, medium, high, critical. &quot;
                    &quot;Example: &#039;high&#039; for CrashLoopBackOff, &#039;low&#039; for minor resource issues.&quot;
    )</code></pre>
<h3><strong>4. Use Enums for Limited Choices</strong></h3>
<pre><code class="language-python">from enum import Enum
<p>class PodStatus(str, Enum):
    RUNNING = &quot;Running&quot;
    PENDING = &quot;Pending&quot;
    CRASH_LOOP = &quot;CrashLoopBackOff&quot;
    ERROR = &quot;Error&quot;</p>
<p>class Diagnosis(BaseModel):
    status: PodStatus  # LLM must use one of these</code></pre></p>
<h3><strong>5. Set Defaults</strong></h3>
<pre><code class="language-python">class Diagnosis(BaseModel):
    pod_name: str
    namespace: str = Field(default=&quot;default&quot;)  # Default value
    severity: str = Field(default=&quot;medium&quot;)</code></pre>
<p>---</p>
<h2>üéì <strong>Self-Check Questions</strong></h2>
<h3><strong>Question 1</strong>: Why use structured outputs instead of free-form text?</h3>
<details>
<summary>Show Answer</summary>
<strong>Structured outputs provide</strong>:
1. <strong>Parseable data</strong>: Easy to extract specific fields
2. <strong>Consistency</strong>: Same format every time
3. <strong>Type safety</strong>: With Pydantic, compile-time checking
4. <strong>UI integration</strong>: Easy to display in tables, cards, dashboards
5. <strong>Downstream processing</strong>: Can feed into other systems
<strong>Example</strong>:
<pre><code class="language-python"># Free-form: Hard to parse
&quot;The pod nginx-abc is crashing because config is missing. Fix: create ConfigMap.&quot;
<h1>Structured: Easy to use</h1>
{
  &quot;pod_name&quot;: &quot;nginx-abc&quot;,
  &quot;root_cause&quot;: &quot;Missing ConfigMap&quot;,
  &quot;fixes&quot;: [&quot;Create ConfigMap nginx-config&quot;]
}</code></pre>
</details>
<h3><strong>Question 2</strong>: What's the difference between JSONOutputParser and PydanticOutputParser?</h3>
<details>
<summary>Show Answer</summary>
<strong>JSONOutputParser</strong>:
<li>Returns dict (untyped)</li>
<li>Basic validation (is it valid JSON?)</li>
<li>Lightweight</li>
<strong>PydanticOutputParser</strong>:
<li>Returns Pydantic model (typed)</li>
<li>Full validation (types, required fields, constraints)</li>
<li>IDE autocomplete and type checking</li>
<li>More robust</li>
<strong>Example</strong>:
<pre><code class="language-python"># JSONOutputParser
result = {&quot;pod_name&quot;: &quot;nginx-abc&quot;}  # Just a dict
print(result[&quot;pod_name&quot;])  # No autocomplete
<h1>PydanticOutputParser</h1>
result: PodDiagnosis = ...  # Typed model
print(result.pod_name)  # IDE autocomplete! ‚úÖ
print(result.status)    # Type checking! ‚úÖ</code></pre>
<strong>For production</strong>: Use PydanticOutputParser for type safety.
</details>
<h3><strong>Question 3</strong>: When would you use OutputFixingParser?</h3>
<details>
<summary>Show Answer</summary>
<strong>Use OutputFixingParser when</strong>:
<li>LLM occasionally returns malformed JSON</li>
<li>You need high reliability (can't have parsing failures)</li>
<li>Willing to accept extra LLM call for fixing</li>
<strong>How it works</strong>:
<pre><code class="language-python">fixing_parser = OutputFixingParser.from_llm(
    parser=base_parser,
    llm=ChatOpenAI(model=&quot;gpt-4&quot;)
)
<h1>If LLM returns: &#039;{&quot;pod_name&quot;: &quot;nginx-abc&quot;, &quot;status&quot;: &quot;Running&quot;&#039;  (missing })</h1>
<h1>OutputFixingParser makes another LLM call to fix it</h1>
result = fixing_parser.parse(malformed_output)  # Fixed!</code></pre>
<strong>Trade-off</strong>:
<li>‚úÖ More robust</li>
<li>‚ùå Extra cost (additional LLM call)</li>
<li>‚ùå Extra latency</li>
<strong>Alternative</strong>: Use try-except with sensible defaults.
</details>
<h3><strong>Question 4</strong>: How do you use structured outputs with ReAct agents?</h3>
<details>
<summary>Show Answer</summary>
<strong>Problem</strong>: ReAct agents output "Thought ‚Üí Action ‚Üí Observation" format, not structured JSON.
<strong>Solution</strong>: Parse final answer separately.
<pre><code class="language-python"># 1. Let agent work naturally
agent_executor = AgentExecutor(agent=agent, tools=tools)
result = agent_executor.invoke({&quot;input&quot;: &quot;Diagnose pod nginx-abc&quot;})
<h1>2. Parse the final answer into structured format</h1>
parse_prompt = f&quot;&quot;&quot;
Extract structured info from this diagnosis:
<p>{result[&#039;output&#039;]}</p>
<p>{parser.get_format_instructions()}
&quot;&quot;&quot;</p>
<h1>3. Get structured output</h1>
structured = (llm | parser).invoke(parse_prompt)
<h1>4. Use typed result</h1>
print(structured.pod_name)
print(structured.fixes)</code></pre>
<strong>Pattern</strong>: Agent does reasoning ‚Üí Extract structure from final answer.
</details>
<h3><strong>Question 5</strong>: Write a Pydantic model for K8s pod diagnosis with 5 fields</h3>
<details>
<summary>Show Answer</summary>
<pre><code class="language-python">from pydantic import BaseModel, Field
from typing import List, Literal
<p>class PodDiagnosis(BaseModel):
    &quot;&quot;&quot;Structured diagnosis of a Kubernetes pod issue&quot;&quot;&quot;
    
    pod_name: str = Field(
        description=&quot;Name of the pod being diagnosed&quot;
    )
    
    status: Literal[&quot;Running&quot;, &quot;Pending&quot;, &quot;CrashLoopBackOff&quot;, &quot;Error&quot;] = Field(
        description=&quot;Current pod status&quot;
    )
    
    root_cause: str = Field(
        description=&quot;Root cause of the issue (one sentence)&quot;
    )
    
    severity: Literal[&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;critical&quot;] = Field(
        description=&quot;Issue severity&quot;
    )
    
    fixes: List[str] = Field(
        description=&quot;Ordered list of recommended fixes (2-5 items)&quot;
    )</p>
<h1>Usage</h1>
parser = PydanticOutputParser(pydantic_object=PodDiagnosis)</code></pre>
<strong>Key features</strong>:
<li>‚úÖ Clear field descriptions</li>
<li>‚úÖ Type hints (str, List, Literal)</li>
<li>‚úÖ Enums for limited choices (Literal)</li>
<li>‚úÖ Descriptive names</li>
<li>‚úÖ Docstring for the class</li></ul>
</details>
<p>---</p>
<h2>üöÄ <strong>Key Takeaways</strong></h2>
<p>1. <strong>Structured outputs enable programmatic use</strong>: JSON > free text
2. <strong>PydanticOutputParser is best</strong>: Type-safe, validated, IDE support
3. <strong>Simple schemas have higher compliance</strong>: Don't overcomplicate
4. <strong>Use fixing parsers for robustness</strong>: But understand the cost
5. <strong>Descriptive field names and examples help</strong>: LLM follows better
6. <strong>Agents + structured outputs</strong>: Let agent reason, then parse final answer
7. <strong>Always handle parsing errors</strong>: try-except or OutputFixingParser</p>
<p>---</p>
<h2>üîó <strong>Next Module</strong></h2>
<p>Move on to <strong>Module 10: RAG Pattern & Vector Databases</strong> to learn about retrieval-augmented generation!</p>
<p>---</p>
<strong>Time to complete this module</strong>: 30 minutes  
<strong>Hands-on practice</strong>: 15 minutes  
<strong>Total</strong>: ~45 minutes

    </div>
    

    <div class="module-content" id="module-10">
        <h1>Module 10: RAG Pattern & Vector Databases</h1>
<strong>Study Time</strong>: ~45 minutes  
<strong>Prerequisites</strong>: Module 2 (Tokens & Embeddings)
<p>---</p>
<h2>üéØ <strong>Learning Objectives</strong></h2>
<p>By the end of this module, you'll understand:
1. What RAG (Retrieval-Augmented Generation) is and why it's powerful
2. When to use RAG vs other patterns
3. How vector databases enable semantic search
4. Complete RAG architecture
5. Why RAG might NOT be needed for your K8s agent v1</p>
<p>---</p>
<h2>üîç <strong>What is RAG?</strong></h2>
<strong>RAG = Retrieval-Augmented Generation</strong>
<p>A pattern where you:
1. <strong>Retrieve</strong> relevant documents from a knowledge base
2. <strong>Augment</strong> the LLM prompt with those documents
3. <strong>Generate</strong> an answer based on retrieved context</p>
<h3><strong>The Problem RAG Solves</strong></h3>
<strong>LLMs have limitations</strong>:
<ul><li>‚ùå Training data cutoff (can't know latest info)</li>
<li>‚ùå No access to private/internal docs</li>
<li>‚ùå Hallucinate facts they don't know</li>
<li>‚ùå Limited context window (can't fit all docs)</li>
<strong>Example</strong>:
<pre><code class="language-python">user: &quot;What&#039;s our company&#039;s policy on Kubernetes resource limits?&quot;
llm: &quot;I don&#039;t have access to your company&#039;s internal policies.&quot; ‚ùå</code></pre>
<strong>With RAG</strong>:
<pre><code class="language-python">user: &quot;What&#039;s our company&#039;s policy on Kubernetes resource limits?&quot;
<h1>System retrieves relevant docs from company knowledge base</h1>
retrieved_docs = [
    &quot;Engineering Policy Doc: All production pods must have memory limits...&quot;,
    &quot;Best Practices: CPU limits should be 2x requests...&quot;
]
<h1>LLM generates answer using retrieved context</h1>
llm: &quot;According to your Engineering Policy Doc, all production pods must have 
memory limits set to prevent OOM issues. CPU limits should be 2x requests...&quot; ‚úÖ</code></pre>
<p>---</p>
<h2>üèóÔ∏è <strong>RAG Architecture</strong></h2>
<pre><code class="language-text">User Query: &quot;How do I fix OOMKilled pods?&quot;
        ‚Üì
    [1. Embed Query]
        ‚Üì
    Query Embedding: [0.234, -0.512, 0.891, ...]
        ‚Üì
    [2. Search Vector DB]
        ‚Üì
    Retrieved Docs:
    - &quot;OOMKilled means pod exceeded memory limit...&quot;
    - &quot;To fix: increase memory limit in deployment...&quot;
    - &quot;Common causes: memory leaks, undersized limits...&quot;
        ‚Üì
    [3. Augment Prompt]
        ‚Üì
    Prompt = &quot;Based on these docs: [retrieved docs]
              Answer: How do I fix OOMKilled pods?&quot;
        ‚Üì
    [4. Generate Answer]
        ‚Üì
    LLM: &quot;OOMKilled occurs when your pod exceeds its memory limit.
          Based on our docs, you should:
          1. Increase memory limit in deployment
          2. Check for memory leaks
          ...&quot;</code></pre>
<p>---</p>
<h2>üß© <strong>Components of RAG</strong></h2>
<h3><strong>1. Document Store</strong></h3>
<p>Where you keep your knowledge base:
<li>Company runbooks</li>
<li>K8s troubleshooting guides</li>
<li>Internal documentation</li>
<li>Support tickets (historical)</li>
<li>Configuration examples</li></p>
<strong>Storage options</strong>:
<li>Files (PDF, Markdown, HTML)</li>
<li>Databases (PostgreSQL, MongoDB)</li>
<li>Cloud storage (S3, Google Drive)</li>
<h3><strong>2. Embeddings</strong></h3>
<p>Convert text into vectors for semantic search.</p>
<pre><code class="language-python">from langchain_openai import OpenAIEmbeddings
<p>embeddings = OpenAIEmbeddings()</p>
<h1>Convert text to vector</h1>
doc_text = &quot;Pods with OOMKilled status need increased memory limits&quot;
doc_vector = embeddings.embed_query(doc_text)
<p>print(doc_vector[:5])  # [0.234, -0.512, 0.891, -0.123, 0.456, ...]
print(len(doc_vector))  # 1536 dimensions</code></pre></p>
<strong>Key concept</strong>: Similar texts ‚Üí similar vectors
<pre><code class="language-python"># These will have similar vectors:
&quot;Pod is out of memory&quot; ‚Üí [0.23, -0.51, 0.89, ...]
&quot;Container exceeded memory limit&quot; ‚Üí [0.25, -0.49, 0.87, ...]
<h1>These will have different vectors:</h1>
&quot;Pod is out of memory&quot; ‚Üí [0.23, -0.51, 0.89, ...]
&quot;Image pull failed&quot; ‚Üí [-0.61, 0.82, -0.34, ...]</code></pre>
<h3><strong>3. Vector Database</strong></h3>
<p>Stores embeddings and enables fast similarity search.</p>
<strong>Popular options</strong>:
<li><strong>Chroma</strong> (local, open-source) ‚≠ê Best for demos</li>
<li><strong>Pinecone</strong> (managed, scalable)</li>
<li><strong>Weaviate</strong> (open-source, production-ready)</li>
<li><strong>Qdrant</strong> (high-performance)</li>
<li><strong>FAISS</strong> (Facebook AI, local)</li>
<h3><strong>4. Retriever</strong></h3>
<p>Finds relevant documents for a query.</p>
<pre><code class="language-python">from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
<h1>Create vector store</h1>
vectorstore = Chroma(
    embedding_function=OpenAIEmbeddings(),
    persist_directory=&quot;./chroma_db&quot;
)
<h1>Create retriever</h1>
retriever = vectorstore.as_retriever(
    search_type=&quot;similarity&quot;,
    search_kwargs={&quot;k&quot;: 3}  # Return top 3 most relevant docs
)
<h1>Search</h1>
query = &quot;Pod is OOMKilled&quot;
relevant_docs = retriever.get_relevant_documents(query)
<p>for doc in relevant_docs:
    print(doc.page_content)</code></pre></p>
<h3><strong>5. Generator</strong></h3>
<p>LLM that generates the final answer.</p>
<pre><code class="language-python">from langchain_openai import ChatOpenAI
<p>llm = ChatOpenAI(model=&quot;gpt-4&quot;, temperature=0.0)</code></pre></p>
<p>---</p>
<h2>üî® <strong>Building a RAG System</strong></h2>
<h3><strong>Step 1: Load Documents</strong></h3>
<pre><code class="language-python">from langchain.document_loaders import DirectoryLoader, TextLoader
<h1>Load all markdown files from a directory</h1>
loader = DirectoryLoader(
    &quot;./runbooks&quot;,
    glob=&quot;**/*.md&quot;,
    loader_cls=TextLoader
)
<p>documents = loader.load()
print(f&quot;Loaded {len(documents)} documents&quot;)</code></pre></p>
<h3><strong>Step 2: Split Documents</strong></h3>
<p>Break large documents into chunks (LLMs have token limits).</p>
<pre><code class="language-python">from langchain.text_splitter import RecursiveCharacterTextSplitter
<p>text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # 1000 characters per chunk
    chunk_overlap=200,  # 200 character overlap between chunks
    length_function=len
)</p>
<p>chunks = text_splitter.split_documents(documents)
print(f&quot;Split into {len(chunks)} chunks&quot;)</code></pre></p>
<strong>Why overlap?</strong>: Prevents cutting in the middle of important context.
<h3><strong>Step 3: Create Embeddings & Vector Store</strong></h3>
<pre><code class="language-python">from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
<h1>Create embeddings</h1>
embeddings = OpenAIEmbeddings()
<h1>Create vector store</h1>
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory=&quot;./chroma_db&quot;
)
<p>print(&quot;Vector store created!&quot;)</code></pre></p>
<h3><strong>Step 4: Create Retriever</strong></h3>
<pre><code class="language-python">retriever = vectorstore.as_retriever(
    search_type=&quot;similarity&quot;,
    search_kwargs={&quot;k&quot;: 5}  # Return top 5 chunks
)</code></pre>
<h3><strong>Step 5: Create RAG Chain</strong></h3>
<pre><code class="language-python">from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
<p>llm = ChatOpenAI(model=&quot;gpt-4&quot;, temperature=0.0)</p>
<p>qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type=&quot;stuff&quot;,  # &quot;stuff&quot; all docs into prompt
    retriever=retriever,
    return_source_documents=True  # Show which docs were used
)</code></pre></p>
<h3><strong>Step 6: Use It!</strong></h3>
<pre><code class="language-python">query = &quot;How do I fix a pod that&#039;s OOMKilled?&quot;
<p>result = qa_chain.invoke({&quot;query&quot;: query})</p>
<p>print(&quot;Answer:&quot;)
print(result[&quot;result&quot;])</p>
<p>print(&quot;\nSource Documents:&quot;)
for i, doc in enumerate(result[&quot;source_documents&quot;], 1):
    print(f&quot;\n{i}. {doc.page_content[:200]}...&quot;)</code></pre></p>
<p>---</p>
<h2>üîß <strong>Complete RAG Example</strong></h2>
<pre><code class="language-python">from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
<h1>1. Load documents</h1>
loader = DirectoryLoader(
    &quot;./k8s_runbooks&quot;,
    glob=&quot;**/*.md&quot;,
    loader_cls=TextLoader
)
documents = loader.load()
<h1>2. Split into chunks</h1>
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)
<h1>3. Create vector store</h1>
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory=&quot;./chroma_db&quot;
)
<h1>4. Create retriever</h1>
retriever = vectorstore.as_retriever(
    search_type=&quot;similarity&quot;,
    search_kwargs={&quot;k&quot;: 5}
)
<h1>5. Create RAG chain</h1>
llm = ChatOpenAI(model=&quot;gpt-4&quot;, temperature=0.0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type=&quot;stuff&quot;,
    retriever=retriever,
    return_source_documents=True
)
<h1>6. Query</h1>
result = qa_chain.invoke({
    &quot;query&quot;: &quot;Pod is stuck in ImagePullBackOff. How do I fix it?&quot;
})
<p>print(result[&quot;result&quot;])</code></pre></p>
<p>---</p>
<h2>üÜö <strong>RAG vs ReAct Agents</strong></h2>
<h3><strong>RAG (Retrieval-Augmented Generation)</strong></h3>
<strong>Best for</strong>:
<li>‚úÖ Large knowledge bases (100+ documents)</li>
<li>‚úÖ Static information (docs, policies, guides)</li>
<li>‚úÖ Questions that need context from docs</li>
<li>‚úÖ Citing sources</li>
<strong>Not good for</strong>:
<li>‚ùå Real-time data (cluster state)</li>
<li>‚ùå Actions (can't run kubectl)</li>
<li>‚ùå Multi-step reasoning with tools</li>
<strong>Example</strong>:
<pre><code class="language-text">User: &quot;What&#039;s our company policy on resource limits?&quot;
RAG: [Retrieves policy doc] &quot;According to the policy, all pods must have limits...&quot;</code></pre>
<h3><strong>ReAct Agents</strong></h3>
<strong>Best for</strong>:
<li>‚úÖ Real-time data (kubectl commands)</li>
<li>‚úÖ Actions (restart pod, check logs)</li>
<li>‚úÖ Multi-step troubleshooting</li>
<li>‚úÖ Tool calling</li>
<strong>Not good for</strong>:
<li>‚ùå Large knowledge bases (can't fit in context)</li>
<li>‚ùå Document-heavy questions</li>
<strong>Example</strong>:
<pre><code class="language-text">User: &quot;Why is pod nginx-abc crashing?&quot;
Agent: [Calls GetPodStatus] [Calls GetPodLogs] &quot;Pod is crashing because...&quot;</code></pre>
<h3><strong>RAG + Agents (Best of Both)</strong></h3>
<p>Combine them for powerful system:</p>
<pre><code class="language-python"># RAG tool for documentation
@tool
def search_runbooks(query: str) -&gt; str:
    &quot;&quot;&quot;Search K8s troubleshooting runbooks.
    
    Use this when you need information from documentation,
    policies, or historical troubleshooting guides.
    &quot;&quot;&quot;
    result = qa_chain.invoke({&quot;query&quot;: query})
    return result[&quot;result&quot;]
<h1>Agent with RAG + kubectl tools</h1>
tools = [
    get_pod_status,
    get_pod_logs,
    describe_pod,
    search_runbooks  # ‚≠ê RAG as a tool!
]
<p>agent = create_react_agent(llm, tools, prompt)</code></pre></p>
<strong>Example flow</strong>:
<pre><code class="language-text">User: &quot;Pod is OOMKilled. What should I do?&quot;
<p>Agent Thought: &quot;I should check runbooks for OOMKilled guidance&quot;
Agent Action: search_runbooks(&quot;OOMKilled pod fix&quot;)
Observation: &quot;Runbook says to check memory limits and usage...&quot;</p>
<p>Agent Thought: &quot;Now let me check this pod&#039;s actual memory limit&quot;
Agent Action: CheckResources(pod_name=&quot;nginx-abc&quot;)
Observation: &quot;Memory limit: 128Mi, usage at crash: 145Mi&quot;</p>
<p>Agent Final Answer: &quot;Based on runbook and current pod state, 
increase memory limit from 128Mi to 256Mi&quot;</code></pre></p>
<p>---</p>
<h2>üéØ <strong>For Your K8s Agent</strong></h2>
<h3><strong>Version 1 (Demo): Skip RAG</strong> ‚≠ê</h3>
<strong>Why</strong>:
<li>‚úÖ ReAct with kubectl tools is sufficient for live troubleshooting</li>
<li>‚úÖ Simpler architecture (faster to build)</li>
<li>‚úÖ No need for document ingestion pipeline</li>
<li>‚úÖ Can always add RAG later</li>
<strong>Use ReAct agent with tools</strong>:
<li>GetPodStatus</li>
<li>GetPodLogs</li>
<li>DescribePod</li>
<li>CheckResources</li>
<li>AnalyzeErrors</li>
<h3><strong>Version 2 (Production): Add RAG</strong></h3>
<strong>When to add</strong>:
<li>‚úÖ Have large runbook library (50+ docs)</li>
<li>‚úÖ Historical troubleshooting data</li>
<li>‚úÖ Company-specific policies</li>
<li>‚úÖ Want to cite sources</li>
<strong>Architecture</strong>:
<pre><code class="language-text">User Query
    ‚Üì
ReAct Agent
    ‚îú‚îÄ Kubectl Tools (get real cluster data)
    ‚îú‚îÄ RAG Tool (search runbooks)
    ‚îî‚îÄ Analysis Tools
    ‚Üì
Comprehensive Answer (live data + documented knowledge)</code></pre>
<p>---</p>
<h2>üìä <strong>RAG Best Practices</strong></h2>
<h3><strong>1. Chunk Size Matters</strong></h3>
<pre><code class="language-python"># Too small: Loses context
text_splitter = RecursiveCharacterTextSplitter(chunk_size=100)  ‚ùå
<h1>Too large: Exceeds token limits</h1>
text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000)  ‚ùå
<h1>Just right: 500-1500 characters</h1>
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)  ‚úÖ</code></pre>
<h3><strong>2. Use Metadata</strong></h3>
<pre><code class="language-python">from langchain.schema import Document
<h1>Add metadata to documents</h1>
doc = Document(
    page_content=&quot;OOMKilled means pod exceeded memory limit...&quot;,
    metadata={
        &quot;source&quot;: &quot;k8s-troubleshooting-guide.md&quot;,
        &quot;section&quot;: &quot;Memory Issues&quot;,
        &quot;category&quot;: &quot;OOM&quot;,
        &quot;last_updated&quot;: &quot;2025-01-01&quot;
    }
)
<h1>Filter by metadata</h1>
retriever = vectorstore.as_retriever(
    search_kwargs={
        &quot;k&quot;: 5,
        &quot;filter&quot;: {&quot;category&quot;: &quot;OOM&quot;}  # Only search OOM docs
    }
)</code></pre>
<h3><strong>3. Rerank Results</strong></h3>
<p>Not all retrieved docs are equally relevant:</p>
<pre><code class="language-python">from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
<h1>Base retriever</h1>
base_retriever = vectorstore.as_retriever(search_kwargs={&quot;k&quot;: 10})
<h1>Compressor (reranks using LLM)</h1>
compressor = LLMChainExtractor.from_llm(llm)
<h1>Compression retriever</h1>
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=base_retriever
)
<h1>Returns only most relevant chunks</h1>
docs = compression_retriever.get_relevant_documents(query)</code></pre>
<h3><strong>4. Monitor Retrieval Quality</strong></h3>
<pre><code class="language-python">def evaluate_retrieval(query: str, expected_doc_ids: list):
    &quot;&quot;&quot;Check if retriever finds expected documents&quot;&quot;&quot;
    retrieved = retriever.get_relevant_documents(query)
    retrieved_ids = [doc.metadata[&quot;id&quot;] for doc in retrieved]
    
    # Calculate recall
    found = len(set(expected_doc_ids) &amp; set(retrieved_ids))
    recall = found / len(expected_doc_ids)
    
    print(f&quot;Recall: {recall:.2%}&quot;)
    return recall
<h1>Test</h1>
evaluate_retrieval(
    query=&quot;OOMKilled pod&quot;,
    expected_doc_ids=[&quot;doc_memory_001&quot;, &quot;doc_oom_guide_002&quot;]
)</code></pre>
<p>---</p>
<h2>üéì <strong>Self-Check Questions</strong></h2>
<h3><strong>Question 1</strong>: What problem does RAG solve?</h3>
<details>
<summary>Show Answer</summary>
<p>RAG solves the problem of <strong>LLMs not having access to specific/updated knowledge</strong>:</p>
<p>1. <strong>Training data cutoff</strong>: LLMs don't know anything after their training date
2. <strong>No private data</strong>: Can't access your company's internal docs
3. <strong>Hallucinations</strong>: LLMs make up facts they don't know
4. <strong>Limited context</strong>: Can't fit all docs in prompt</p>
<strong>RAG solution</strong>: Retrieve relevant docs from knowledge base ‚Üí Augment prompt ‚Üí Generate answer based on actual docs
<strong>Example</strong>:
<li>Without RAG: "I don't know your company's K8s policy"</li>
<li>With RAG: [Retrieves policy doc] "According to your policy..."</li>
</details>
<h3><strong>Question 2</strong>: What are the 5 components of a RAG system?</h3>
<details>
<summary>Show Answer</summary>
<p>1. <strong>Document Store</strong>: Where knowledge base is kept (files, DB, cloud)
2. <strong>Embeddings</strong>: Convert text to vectors for semantic search
3. <strong>Vector Database</strong>: Store embeddings, enable fast similarity search (Chroma, Pinecone)
4. <strong>Retriever</strong>: Find relevant documents for a query
5. <strong>Generator</strong>: LLM that generates final answer using retrieved docs</p>
<strong>Flow</strong>:
<pre><code class="language-text">Query ‚Üí Embed ‚Üí Search Vector DB ‚Üí Retrieve Docs ‚Üí Generate Answer</code></pre>
</details>
<h3><strong>Question 3</strong>: When should you use RAG vs ReAct agents?</h3>
<details>
<summary>Show Answer</summary>
<strong>Use RAG when</strong>:
<li>‚úÖ Large knowledge base (100+ docs)</li>
<li>‚úÖ Static information (policies, guides, runbooks)</li>
<li>‚úÖ Need to cite sources</li>
<li>‚ùå But can't take actions or get real-time data</li>
<strong>Use ReAct agents when</strong>:
<li>‚úÖ Need real-time data (kubectl commands)</li>
<li>‚úÖ Need to take actions (restart pod, scale deployment)</li>
<li>‚úÖ Multi-step troubleshooting</li>
<li>‚ùå But don't have large document collection</li>
<strong>Use BOTH when</strong>:
<li>‚úÖ Need real-time troubleshooting + documented knowledge</li>
<li>‚úÖ Make RAG a tool that agent can call</li></ul>
<strong>Your K8s agent v1</strong>: ReAct only (sufficient for live troubleshooting)  
<strong>Your K8s agent v2</strong>: ReAct + RAG tool (adds runbook knowledge)
</details>
<h3><strong>Question 4</strong>: Why is chunk overlap important in document splitting?</h3>
<details>
<summary>Show Answer</summary>
<strong>Without overlap</strong>:
<pre><code class="language-text">Chunk 1: &quot;...pods with memory limits. OOM&quot;
Chunk 2: &quot;Killed pods need increased limits...&quot;</code></pre>
‚Üí Splits "OOMKilled" across chunks, loses context!
<strong>With overlap (200 chars)</strong>:
<pre><code class="language-text">Chunk 1: &quot;...pods with memory limits. OOMKilled pods need increased...&quot;
Chunk 2: &quot;...OOMKilled pods need increased limits...&quot;</code></pre>
‚Üí Both chunks have full context!
<strong>Best practice</strong>: chunk_overlap = 20% of chunk_size
<pre><code class="language-python">text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200  # 20% overlap
)</code></pre>
</details>
<h3><strong>Question 5</strong>: How would you add RAG as a tool in your K8s agent?</h3>
<details>
<summary>Show Answer</summary>
<pre><code class="language-python">from langchain.tools import tool
from langchain.chains import RetrievalQA
from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
<h1>1. Create RAG chain</h1>
vectorstore = Chroma(
    embedding_function=OpenAIEmbeddings(),
    persist_directory=&quot;./runbooks_db&quot;
)
<p>qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model=&quot;gpt-4&quot;, temperature=0.0),
    retriever=vectorstore.as_retriever(search_kwargs={&quot;k&quot;: 3})
)</p>
<h1>2. Wrap RAG as a tool</h1>
@tool
def search_runbooks(query: str) -&gt; str:
    &quot;&quot;&quot;Search Kubernetes troubleshooting runbooks and documentation.
    
    Use this tool when you need information about:
    - Company policies and best practices
    - Historical troubleshooting procedures
    - Common error patterns and solutions
    - Configuration examples
    
    Do NOT use for real-time cluster data - use kubectl tools instead.
    
    Args:
        query: What to search for in runbooks
        
    Returns:
        Relevant information from documentation
    &quot;&quot;&quot;
    result = qa_chain.invoke({&quot;query&quot;: query})
    return result[&quot;result&quot;]
<h1>3. Add to agent tools</h1>
tools = [
    get_pod_status,      # Real-time data
    get_pod_logs,        # Real-time data
    describe_pod,        # Real-time data
    search_runbooks      # ‚≠ê RAG tool for documentation
]
<h1>4. Create agent</h1>
agent = create_react_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools)
<h1>5. Agent can now use both!</h1>
result = agent_executor.invoke({
    &quot;input&quot;: &quot;Pod is OOMKilled. What does our runbook say?&quot;
})</code></pre>
<strong>Agent flow</strong>:
1. Calls `search_runbooks("OOMKilled")` ‚Üí Gets documented solution
2. Calls `check_resources(pod_name)` ‚Üí Gets actual pod limits
3. Combines both ‚Üí Gives answer based on runbook + real data
</details>
<p>---</p>
<h2>üöÄ <strong>Key Takeaways</strong></h2>
<p>1. <strong>RAG = Retrieval + Augmentation + Generation</strong>: Gives LLMs access to knowledge bases
2. <strong>Vector databases enable semantic search</strong>: Similar text ‚Üí similar vectors
3. <strong>RAG is for static knowledge</strong>: Docs, policies, runbooks
4. <strong>ReAct agents are for real-time data</strong>: kubectl, APIs, actions
5. <strong>Combine RAG + Agents</strong>: Make RAG a tool the agent can call
6. <strong>For your K8s agent v1</strong>: Skip RAG (ReAct is sufficient)
7. <strong>For production v2</strong>: Add RAG for runbook knowledge</p>
<p>---</p>
<h2>üîó <strong>Next Module</strong></h2>
<p>Move on to <strong>Module 11: Error Handling & Production Patterns</strong> to learn how to build robust production systems!</p>
<p>---</p>
<strong>Time to complete this module</strong>: 45 minutes  
<strong>Hands-on practice</strong>: 30 minutes  
<strong>Total</strong>: ~1 hour 15 minutes

    </div>
    

    <div class="module-content" id="module-11">
        <h1>Module 11: Error Handling & Production Patterns</h1>
<strong>Study Time</strong>: ~45 minutes  
<strong>Prerequisites</strong>: Modules 1-10
<p>---</p>
<h2>üéØ <strong>Learning Objectives</strong></h2>
<p>By the end of this module, you'll understand:
1. Common failure modes in AI systems
2. Error handling patterns for LLMs and agents
3. Retry logic and circuit breakers
4. Logging and observability
5. Production-ready code patterns
6. How to make your K8s agent robust</p>
<p>---</p>
<h2>‚ö†Ô∏è <strong>Common Failure Modes</strong></h2>
<h3><strong>1. API Errors</strong></h3>
<strong>Problem</strong>: OpenAI/API rate limits, timeouts, network issues
<pre><code class="language-python"># ‚ùå No error handling
response = client.chat.completions.create(
    model=&quot;gpt-4&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Help&quot;}]
)
<h1>Crashes if API is down!</code></pre></h1>
<strong>Solution</strong>: Try-except with retries
<pre><code class="language-python">import time
from openai import OpenAI, APIError, RateLimitError
<p>client = OpenAI()</p>
<p>def call_llm_with_retry(messages, max_retries=3):
    &quot;&quot;&quot;Call LLM with exponential backoff retry&quot;&quot;&quot;
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=&quot;gpt-4&quot;,
                messages=messages,
                timeout=30  # 30 second timeout
            )
            return response.choices[0].message.content
            
        except RateLimitError:
            if attempt &lt; max_retries - 1:
                wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s, 4s
                print(f&quot;Rate limited. Waiting {wait_time}s...&quot;)
                time.sleep(wait_time)
            else:
                return &quot;Error: API rate limit exceeded. Try again later.&quot;
                
        except APIError as e:
            if attempt &lt; max_retries - 1:
                print(f&quot;API error: {e}. Retrying...&quot;)
                time.sleep(1)
            else:
                return f&quot;Error: API unavailable. {str(e)}&quot;
                
        except Exception as e:
            return f&quot;Unexpected error: {str(e)}&quot;
    
    return &quot;Error: Max retries exceeded&quot;</p>
<h1>Use it</h1>
result = call_llm_with_retry([
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Why is pod crashing?&quot;}
])</code></pre>
<h3><strong>2. Tool Execution Errors</strong></h3>
<strong>Problem</strong>: kubectl command fails, pod doesn't exist, timeout
<pre><code class="language-python"># ‚ùå No error handling
@tool
def get_pod_status(pod_name: str) -&gt; str:
    &quot;&quot;&quot;Get pod status&quot;&quot;&quot;
    result = subprocess.run([&quot;kubectl&quot;, &quot;get&quot;, &quot;pod&quot;, pod_name])
    return result.stdout  # Crashes if pod doesn&#039;t exist!</code></pre>
<strong>Solution</strong>: Comprehensive error handling
<pre><code class="language-python">import subprocess
from langchain.tools import tool
<p>@tool
def get_pod_status(pod_name: str, namespace: str = &quot;default&quot;) -&gt; str:
    &quot;&quot;&quot;Get status of a Kubernetes pod.
    
    Args:
        pod_name: Name of the pod
        namespace: K8s namespace (default: default)
    
    Returns:
        Pod status or error message
    &quot;&quot;&quot;
    try:
        result = subprocess.run(
            [&quot;kubectl&quot;, &quot;get&quot;, &quot;pod&quot;, pod_name, &quot;-n&quot;, namespace, &quot;-o&quot;, &quot;wide&quot;],
            capture_output=True,
            text=True,
            timeout=10  # 10 second timeout
        )
        
        if result.returncode != 0:
            # Command failed
            if &quot;not found&quot; in result.stderr.lower():
                return f&quot;Pod &#039;{pod_name}&#039; not found in namespace &#039;{namespace}&#039;&quot;
            else:
                return f&quot;Error: {result.stderr.strip()}&quot;
        
        # Success
        return result.stdout.strip()
        
    except subprocess.TimeoutExpired:
        return f&quot;Error: kubectl command timed out after 10 seconds&quot;
        
    except FileNotFoundError:
        return &quot;Error: kubectl not found. Is it installed?&quot;
        
    except Exception as e:
        return f&quot;Unexpected error: {str(e)}&quot;</p>
<h1>All errors return string messages, agent can handle gracefully</code></pre></h1>
<h3><strong>3. Parsing Errors</strong></h3>
<strong>Problem</strong>: Agent output doesn't match expected format
<pre><code class="language-python"># ‚ùå Agent might return malformed output
agent_executor = AgentExecutor(agent=agent, tools=tools)
<h1>Crashes if LLM returns invalid format!</code></pre></h1>
<strong>Solution</strong>: handle_parsing_errors
<pre><code class="language-python">agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    handle_parsing_errors=True,  # ‚úÖ Catch parsing errors
    handle_parsing_errors_message=&quot;Invalid format. Please use:\nThought: ...\nAction: ...\nAction Input: ...&quot;
)</code></pre>
<h3><strong>4. Infinite Loops</strong></h3>
<strong>Problem</strong>: Agent repeats same action forever
<pre><code class="language-python"># ‚ùå No iteration limit
agent_executor = AgentExecutor(agent=agent, tools=tools)
<h1>Might loop forever!</code></pre></h1>
<strong>Solution</strong>: max_iterations
<pre><code class="language-python">agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    max_iterations=5,  # ‚úÖ Stop after 5 iterations
    early_stopping_method=&quot;generate&quot;  # ‚úÖ Generate best answer when limit hit
)</code></pre>
<h3><strong>5. Token Overflow</strong></h3>
<strong>Problem</strong>: Conversation history exceeds context window
<pre><code class="language-python"># ‚ùå Unbounded memory
memory = ConversationBufferMemory()
<h1>Grows forever, eventually crashes!</code></pre></h1>
<strong>Solution</strong>: Window memory
<pre><code class="language-python">memory = ConversationBufferWindowMemory(
    k=10,  # ‚úÖ Only keep last 10 exchanges
    memory_key=&quot;chat_history&quot;
)</code></pre>
<p>---</p>
<h2>üîÑ <strong>Retry Patterns</strong></h2>
<h3><strong>Pattern 1: Simple Retry</strong></h3>
<pre><code class="language-python">def retry_on_failure(func, max_attempts=3):
    &quot;&quot;&quot;Simple retry wrapper&quot;&quot;&quot;
    for attempt in range(max_attempts):
        try:
            return func()
        except Exception as e:
            if attempt &lt; max_attempts - 1:
                print(f&quot;Attempt {attempt + 1} failed: {e}. Retrying...&quot;)
                continue
            else:
                print(f&quot;All {max_attempts} attempts failed&quot;)
                raise</code></pre>
<h3><strong>Pattern 2: Exponential Backoff</strong></h3>
<pre><code class="language-python">import time
<p>def exponential_backoff_retry(func, max_attempts=5, base_delay=1):
    &quot;&quot;&quot;Retry with exponential backoff: 1s, 2s, 4s, 8s, 16s&quot;&quot;&quot;
    for attempt in range(max_attempts):
        try:
            return func()
        except Exception as e:
            if attempt &lt; max_attempts - 1:
                delay = base_delay * (2 ** attempt)
                print(f&quot;Attempt {attempt + 1} failed. Waiting {delay}s...&quot;)
                time.sleep(delay)
            else:
                raise</p>
<h1>Use it</h1>
result = exponential_backoff_retry(
    lambda: agent_executor.invoke({&quot;input&quot;: &quot;Diagnose pod&quot;})
)</code></pre>
<h3><strong>Pattern 3: Retry with Different Strategy</strong></h3>
<pre><code class="language-python">def retry_with_fallback(primary_func, fallback_func, max_attempts=3):
    &quot;&quot;&quot;Try primary, fallback to secondary on failure&quot;&quot;&quot;
    for attempt in range(max_attempts):
        try:
            return primary_func()
        except Exception as e:
            print(f&quot;Primary failed: {e}&quot;)
            if attempt &lt; max_attempts - 1:
                continue
            else:
                print(&quot;Falling back to secondary strategy...&quot;)
                return fallback_func()
<h1>Example: Try GPT-4, fallback to GPT-3.5</h1>
result = retry_with_fallback(
    primary_func=lambda: call_gpt4(query),
    fallback_func=lambda: call_gpt35(query)
)</code></pre>
<p>---</p>
<h2>üõ°Ô∏è <strong>Circuit Breaker Pattern</strong></h2>
<p>Prevent cascading failures by "opening the circuit" when errors exceed threshold.</p>
<pre><code class="language-python">from datetime import datetime, timedelta
<p>class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout  # seconds
        self.failures = 0
        self.last_failure_time = None
        self.state = &quot;CLOSED&quot;  # CLOSED, OPEN, HALF_OPEN
        
    def call(self, func):
        &quot;&quot;&quot;Execute function with circuit breaker protection&quot;&quot;&quot;
        
        # Check if circuit is open
        if self.state == &quot;OPEN&quot;:
            if self._should_attempt_reset():
                self.state = &quot;HALF_OPEN&quot;
                print(&quot;Circuit breaker HALF_OPEN, attempting call...&quot;)
            else:
                raise Exception(&quot;Circuit breaker is OPEN. Service unavailable.&quot;)
        
        # Try to call function
        try:
            result = func()
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise
    
    def _on_success(self):
        &quot;&quot;&quot;Reset circuit on success&quot;&quot;&quot;
        self.failures = 0
        self.state = &quot;CLOSED&quot;
        print(&quot;Circuit breaker CLOSED&quot;)
    
    def _on_failure(self):
        &quot;&quot;&quot;Record failure&quot;&quot;&quot;
        self.failures += 1
        self.last_failure_time = datetime.now()
        
        if self.failures &gt;= self.failure_threshold:
            self.state = &quot;OPEN&quot;
            print(f&quot;Circuit breaker OPEN after {self.failures} failures&quot;)
    
    def _should_attempt_reset(self):
        &quot;&quot;&quot;Check if timeout has passed&quot;&quot;&quot;
        if self.last_failure_time is None:
            return True
        return datetime.now() - self.last_failure_time &gt; timedelta(seconds=self.timeout)</p>
<h1>Use it</h1>
breaker = CircuitBreaker(failure_threshold=3, timeout=30)
<p>try:
    result = breaker.call(lambda: agent_executor.invoke({&quot;input&quot;: query}))
except Exception as e:
    print(f&quot;Request failed: {e}&quot;)</code></pre></p>
<p>---</p>
<h2>üìä <strong>Logging & Observability</strong></h2>
<h3><strong>Pattern 1: Structured Logging</strong></h3>
<pre><code class="language-python">import logging
import json
from datetime import datetime
<h1>Configure logging</h1>
logging.basicConfig(
    level=logging.INFO,
    format=&#039;%(message)s&#039;
)
logger = logging.getLogger(__name__)
<p>def log_agent_call(query, result, duration, error=None):
    &quot;&quot;&quot;Log agent invocation with structured data&quot;&quot;&quot;
    log_entry = {
        &quot;timestamp&quot;: datetime.utcnow().isoformat(),
        &quot;query&quot;: query,
        &quot;success&quot;: error is None,
        &quot;duration_ms&quot;: duration * 1000,
        &quot;error&quot;: str(error) if error else None,
        &quot;result_length&quot;: len(result) if result else 0
    }
    logger.info(json.dumps(log_entry))</p>
<h1>Use it</h1>
import time
<p>query = &quot;Why is pod crashing?&quot;
start_time = time.time()</p>
<p>try:
    result = agent_executor.invoke({&quot;input&quot;: query})
    duration = time.time() - start_time
    log_agent_call(query, result[&quot;output&quot;], duration)
except Exception as e:
    duration = time.time() - start_time
    log_agent_call(query, None, duration, error=e)
    raise</code></pre></p>
<h3><strong>Pattern 2: Agent Step Logging</strong></h3>
<pre><code class="language-python">def log_agent_steps(agent_executor, query):
    &quot;&quot;&quot;Log each step of agent reasoning&quot;&quot;&quot;
    result = agent_executor.invoke(
        {&quot;input&quot;: query},
        return_intermediate_steps=True
    )
    
    logger.info(f&quot;Query: {query}&quot;)
    
    for i, (action, observation) in enumerate(result[&quot;intermediate_steps&quot;], 1):
        logger.info(f&quot;Step {i}:&quot;)
        logger.info(f&quot;  Tool: {action.tool}&quot;)
        logger.info(f&quot;  Input: {action.tool_input}&quot;)
        logger.info(f&quot;  Output: {observation[:200]}...&quot;)  # First 200 chars
    
    logger.info(f&quot;Final Answer: {result[&#039;output&#039;]}&quot;)
    
    return result
<h1>Use it</h1>
result = log_agent_steps(agent_executor, &quot;Diagnose pod nginx-abc&quot;)</code></pre>
<h3><strong>Pattern 3: Metrics Tracking</strong></h3>
<pre><code class="language-python">from collections import defaultdict
from datetime import datetime
<p>class AgentMetrics:
    def __init__(self):
        self.call_count = 0
        self.success_count = 0
        self.error_count = 0
        self.total_duration = 0
        self.tool_usage = defaultdict(int)
        
    def record_call(self, success, duration, tools_used):
        &quot;&quot;&quot;Record metrics for an agent call&quot;&quot;&quot;
        self.call_count += 1
        
        if success:
            self.success_count += 1
        else:
            self.error_count += 1
        
        self.total_duration += duration
        
        for tool in tools_used:
            self.tool_usage[tool] += 1
    
    def get_stats(self):
        &quot;&quot;&quot;Get current metrics&quot;&quot;&quot;
        return {
            &quot;total_calls&quot;: self.call_count,
            &quot;success_rate&quot;: self.success_count / self.call_count if self.call_count &gt; 0 else 0,
            &quot;avg_duration_ms&quot;: (self.total_duration / self.call_count * 1000) if self.call_count &gt; 0 else 0,
            &quot;most_used_tools&quot;: sorted(self.tool_usage.items(), key=lambda x: x[1], reverse=True)[:5]
        }</p>
<h1>Use it</h1>
metrics = AgentMetrics()
<p>start = time.time()
try:
    result = agent_executor.invoke({&quot;input&quot;: query}, return_intermediate_steps=True)
    duration = time.time() - start
    tools_used = [step[0].tool for step in result[&quot;intermediate_steps&quot;]]
    metrics.record_call(success=True, duration=duration, tools_used=tools_used)
except Exception:
    duration = time.time() - start
    metrics.record_call(success=False, duration=duration, tools_used=[])</p>
<h1>Print stats</h1>
print(json.dumps(metrics.get_stats(), indent=2))</code></pre>
<p>---</p>
<h2>üèóÔ∏è <strong>Production-Ready Agent</strong></h2>
<p>Complete example with all error handling patterns:</p>
<pre><code class="language-python">import logging
import time
import subprocess
from typing import Optional
from langchain.agents import create_react_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferWindowMemory
from langchain.tools import tool
from openai import RateLimitError, APIError
<h1>Setup logging</h1>
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
<h1>Define tools with error handling</h1>
@tool
def get_pod_status(pod_name: str, namespace: str = &quot;default&quot;) -&gt; str:
    &quot;&quot;&quot;Get status of a Kubernetes pod with comprehensive error handling.&quot;&quot;&quot;
    try:
        result = subprocess.run(
            [&quot;kubectl&quot;, &quot;get&quot;, &quot;pod&quot;, pod_name, &quot;-n&quot;, namespace, &quot;-o&quot;, &quot;wide&quot;],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        if result.returncode != 0:
            if &quot;not found&quot; in result.stderr.lower():
                return f&quot;Pod &#039;{pod_name}&#039; not found in namespace &#039;{namespace}&#039;&quot;
            return f&quot;Error: {result.stderr.strip()}&quot;
        
        return result.stdout.strip()
        
    except subprocess.TimeoutExpired:
        return f&quot;Error: Command timed out after 10 seconds&quot;
    except FileNotFoundError:
        return &quot;Error: kubectl not installed&quot;
    except Exception as e:
        logger.error(f&quot;get_pod_status error: {e}&quot;)
        return f&quot;Unexpected error: {str(e)}&quot;
<p>tools = [get_pod_status]  # Add more tools...</p>
<h1>Production-ready agent class</h1>
class ProductionK8sAgent:
    def __init__(
        self,
        model: str = &quot;gpt-4&quot;,
        max_retries: int = 3,
        max_iterations: int = 5
    ):
        self.max_retries = max_retries
        self.call_count = 0
        self.error_count = 0
        
        # Create LLM with retry
        self.llm = ChatOpenAI(
            model=model,
            temperature=0.0,
            request_timeout=30
        )
        
        # Create memory
        self.memory = ConversationBufferWindowMemory(
            k=10,
            memory_key=&quot;chat_history&quot;,
            return_messages=True
        )
        
        # Create agent
        agent = create_react_agent(self.llm, tools, prompt_template)
        
        # Create executor with error handling
        self.agent_executor = AgentExecutor(
            agent=agent,
            tools=tools,
            memory=self.memory,
            max_iterations=max_iterations,
            early_stopping_method=&quot;generate&quot;,
            handle_parsing_errors=True,
            verbose=True
        )
    
    def diagnose(self, query: str) -&gt; dict:
        &quot;&quot;&quot;Diagnose with full error handling and retry logic&quot;&quot;&quot;
        self.call_count += 1
        start_time = time.time()
        
        for attempt in range(self.max_retries):
            try:
                logger.info(f&quot;Attempt {attempt + 1}/{self.max_retries}: {query}&quot;)
                
                result = self.agent_executor.invoke(
                    {&quot;input&quot;: query},
                    return_intermediate_steps=True
                )
                
                duration = time.time() - start_time
                
                # Log success
                logger.info(f&quot;Success in {duration:.2f}s&quot;)
                
                return {
                    &quot;success&quot;: True,
                    &quot;output&quot;: result[&quot;output&quot;],
                    &quot;duration&quot;: duration,
                    &quot;attempts&quot;: attempt + 1,
                    &quot;steps&quot;: len(result.get(&quot;intermediate_steps&quot;, []))
                }
                
            except RateLimitError as e:
                logger.warning(f&quot;Rate limited on attempt {attempt + 1}&quot;)
                if attempt &lt; self.max_retries - 1:
                    wait_time = 2 ** attempt
                    time.sleep(wait_time)
                else:
                    self.error_count += 1
                    return {
                        &quot;success&quot;: False,
                        &quot;error&quot;: &quot;API rate limit exceeded&quot;,
                        &quot;duration&quot;: time.time() - start_time
                    }
                    
            except APIError as e:
                logger.error(f&quot;API error on attempt {attempt + 1}: {e}&quot;)
                if attempt &lt; self.max_retries - 1:
                    time.sleep(1)
                else:
                    self.error_count += 1
                    return {
                        &quot;success&quot;: False,
                        &quot;error&quot;: f&quot;API unavailable: {str(e)}&quot;,
                        &quot;duration&quot;: time.time() - start_time
                    }
                    
            except Exception as e:
                logger.error(f&quot;Unexpected error: {e}&quot;)
                self.error_count += 1
                return {
                    &quot;success&quot;: False,
                    &quot;error&quot;: str(e),
                    &quot;duration&quot;: time.time() - start_time
                }
        
        # Should never reach here
        return {
            &quot;success&quot;: False,
            &quot;error&quot;: &quot;Max retries exceeded&quot;,
            &quot;duration&quot;: time.time() - start_time
        }
    
    def get_stats(self):
        &quot;&quot;&quot;Get agent statistics&quot;&quot;&quot;
        return {
            &quot;total_calls&quot;: self.call_count,
            &quot;errors&quot;: self.error_count,
            &quot;success_rate&quot;: (self.call_count - self.error_count) / self.call_count 
                           if self.call_count &gt; 0 else 0
        }
<h1>Use it</h1>
agent = ProductionK8sAgent(model=&quot;gpt-4&quot;, max_retries=3, max_iterations=5)
<h1>Diagnose</h1>
result = agent.diagnose(&quot;Why is pod nginx-abc crashing?&quot;)
<p>if result[&quot;success&quot;]:
    print(f&quot;‚úÖ Diagnosis: {result[&#039;output&#039;]}&quot;)
    print(f&quot;‚è±Ô∏è  Duration: {result[&#039;duration&#039;]:.2f}s&quot;)
else:
    print(f&quot;‚ùå Error: {result[&#039;error&#039;]}&quot;)</p>
<h1>Check stats</h1>
print(agent.get_stats())</code></pre>
<p>---</p>
<h2>üéì <strong>Self-Check Questions</strong></h2>
<h3><strong>Question 1</strong>: What are the 5 common failure modes in AI agents?</h3>
<details>
<summary>Show Answer</summary>
<p>1. <strong>API Errors</strong>: Rate limits, timeouts, network issues
2. <strong>Tool Execution Errors</strong>: kubectl fails, pod doesn't exist
3. <strong>Parsing Errors</strong>: Agent output doesn't match expected format
4. <strong>Infinite Loops</strong>: Agent repeats same action forever
5. <strong>Token Overflow</strong>: Conversation history exceeds context window</p>
<strong>Solutions</strong>:
1. Retry with exponential backoff
2. Try-except in all tools, return error strings
3. handle_parsing_errors=True
4. max_iterations=5
5. ConversationBufferWindowMemory(k=10)
</details>
<h3><strong>Question 2</strong>: What is exponential backoff and when should you use it?</h3>
<details>
<summary>Show Answer</summary>
<strong>Exponential backoff</strong> is a retry strategy where wait time increases exponentially: 1s, 2s, 4s, 8s, 16s...
<strong>Formula</strong>: `wait_time = base_delay * (2 ** attempt)`
<strong>When to use</strong>:
<ul><li>‚úÖ API rate limits (give server time to recover)</li>
<li>‚úÖ Network timeouts (transient issues)</li>
<li>‚úÖ Database connection errors</li>
<strong>Why it works</strong>:
<li>Reduces load on failing service</li>
<li>Gives service time to recover</li>
<li>Prevents thundering herd problem</li>
<pre><code class="language-python">for attempt in range(max_retries):
    try:
        return func()
    except Exception:
        wait = 2 ** attempt  # 1s, 2s, 4s, 8s
        time.sleep(wait)</code></pre>
</details>
<h3><strong>Question 3</strong>: What's the purpose of a circuit breaker?</h3>
<details>
<summary>Show Answer</summary>
<strong>Circuit breaker</strong> prevents cascading failures by "opening" when error rate exceeds threshold.
<strong>States</strong>:
1. <strong>CLOSED</strong>: Normal operation, requests pass through
2. <strong>OPEN</strong>: Too many failures, all requests fail immediately (don't even try)
3. <strong>HALF_OPEN</strong>: After timeout, try one request to test if service recovered
<strong>Purpose</strong>:
<li>Prevent wasting resources on failing service</li>
<li>Give failing service time to recover</li>
<li>Fail fast instead of hanging</li>
<li>Protect dependent services</li>
<strong>Example</strong>:
<pre><code class="language-python"># After 5 failures, circuit opens
<h1>All requests fail immediately for 60 seconds</h1>
<h1>After 60s, try one request</h1>
<h1>If success ‚Üí circuit closes</h1>
<h1>If failure ‚Üí circuit stays open another 60s</code></pre></h1>
</details>
<h3><strong>Question 4</strong>: How do you make kubectl tools robust against errors?</h3>
<details>
<summary>Show Answer</summary>
<pre><code class="language-python">@tool
def get_pod_status(pod_name: str, namespace: str = &quot;default&quot;) -&gt; str:
    &quot;&quot;&quot;Get pod status with comprehensive error handling&quot;&quot;&quot;
    try:
        result = subprocess.run(
            [&quot;kubectl&quot;, &quot;get&quot;, &quot;pod&quot;, pod_name, &quot;-n&quot;, namespace],
            capture_output=True,  # ‚úÖ Capture stderr
            text=True,            # ‚úÖ Decode as text
            timeout=10            # ‚úÖ Prevent hanging
        )
        
        if result.returncode != 0:
            # ‚úÖ Check specific errors
            if &quot;not found&quot; in result.stderr.lower():
                return f&quot;Pod not found: {pod_name}&quot;
            return f&quot;Error: {result.stderr}&quot;
        
        return result.stdout
        
    except subprocess.TimeoutExpired:
        # ‚úÖ Handle timeout
        return &quot;Error: Command timed out&quot;
        
    except FileNotFoundError:
        # ‚úÖ Handle missing kubectl
        return &quot;Error: kubectl not installed&quot;
        
    except Exception as e:
        # ‚úÖ Catch-all for unexpected errors
        return f&quot;Unexpected error: {str(e)}&quot;</code></pre>
<strong>Key points</strong>:
<li>Always use timeout</li>
<li>Capture stderr</li>
<li>Check return code</li>
<li>Handle specific errors (not found, timeout, missing binary)</li>
<li>Return error strings (not raise exceptions)</li>
<li>Agent can handle error messages gracefully</li>
</details>
<h3><strong>Question 5</strong>: What should you log for production observability?</h3>
<details>
<summary>Show Answer</summary>
<strong>Essential logs</strong>:
<p>1. <strong>Request/Response</strong>:
   - Query
   - Result
   - Duration
   - Success/failure</p>
<p>2. <strong>Agent Steps</strong>:
   - Tools called
   - Tool inputs
   - Tool outputs
   - Reasoning (thoughts)</p>
<p>3. <strong>Errors</strong>:
   - Error type
   - Error message
   - Stack trace
   - Retry attempts</p>
<p>4. <strong>Metrics</strong>:
   - Total calls
   - Success rate
   - Average duration
   - Tool usage frequency
   - Error rate by type</p>
<strong>Example</strong>:
<pre><code class="language-python">log_entry = {
    &quot;timestamp&quot;: &quot;2025-12-09T10:30:00Z&quot;,
    &quot;query&quot;: &quot;Why is pod crashing?&quot;,
    &quot;success&quot;: True,
    &quot;duration_ms&quot;: 2340,
    &quot;tools_used&quot;: [&quot;GetPodStatus&quot;, &quot;GetPodLogs&quot;],
    &quot;iterations&quot;: 2,
    &quot;result_length&quot;: 450
}</code></pre>
<strong>Why</strong>:
<li>Debug issues</li>
<li>Monitor performance</li>
<li>Identify patterns</li>
<li>Optimize tool usage</li>
<li>Track success rates</li></ul>
</details>
<p>---</p>
<h2>üöÄ <strong>Key Takeaways</strong></h2>
<p>1. <strong>Always handle errors</strong>: API, tools, parsing, loops, tokens
2. <strong>Use retries with exponential backoff</strong>: For transient failures
3. <strong>Implement circuit breakers</strong>: For cascading failures
4. <strong>Log everything</strong>: Queries, results, steps, errors, metrics
5. <strong>Set timeouts</strong>: On API calls and subprocess commands
6. <strong>Return error strings from tools</strong>: Don't raise exceptions
7. <strong>Use max_iterations</strong>: Prevent infinite loops
8. <strong>Monitor metrics</strong>: Success rate, duration, tool usage</p>
<p>---</p>
<h2>üîó <strong>Next Module</strong></h2>
<p>Move on to <strong>Module 12: ML System Design & Best Practices</strong> for high-level architecture patterns!</p>
<p>---</p>
<strong>Time to complete this module</strong>: 45 minutes  
<strong>Hands-on practice</strong>: 30 minutes  
<strong>Total</strong>: ~1 hour 15 minutes

    </div>
    

    <div class="module-content" id="module-12">
        <h1>Module 12: ML System Design & Best Practices</h1>
<strong>Study Time</strong>: ~30 minutes  
<strong>Prerequisites</strong>: All previous modules
<p>---</p>
<h2>üéØ <strong>Learning Objectives</strong></h2>
<p>By the end of this module, you'll understand:
1. How to design scalable ML systems
2. Common architecture patterns
3. Development workflow (prototype ‚Üí production)
4. Evaluation and testing strategies
5. Cost optimization
6. How to talk about system design in interviews</p>
<p>---</p>
<h2>üèõÔ∏è <strong>ML System Architecture Layers</strong></h2>
<h3><strong>Layer 1: Presentation</strong></h3>
<ul><li>User interface (Web, CLI, API)</li>
<li>Input validation</li>
<li>Output formatting</li>
<h3><strong>Layer 2: Application Logic</strong></h3>
<li>Request routing</li>
<li>Session management</li>
<li>Business rules</li>
<h3><strong>Layer 3: ML/AI Layer</strong></h3>
<li>Agents</li>
<li>LLM calls</li>
<li>Tool execution</li>
<li>Memory management</li>
<h3><strong>Layer 4: Data Layer</strong></h3>
<li>Vector databases</li>
<li>Document stores</li>
<li>Caching</li>
<li>External APIs</li>
<h3><strong>Layer 5: Infrastructure</strong></h3>
<li>Kubernetes (deployment, scaling)</li>
<li>Monitoring & logging</li>
<li>Security & RBAC</li>
<p>---</p>
<h2>üé® <strong>Common ML Architecture Patterns</strong></h2>
<h3><strong>Pattern 1: Simple API Gateway</strong></h3>
<pre><code class="language-text">User ‚Üí FastAPI ‚Üí LLM ‚Üí Response</code></pre>
<strong>Use case</strong>: Simple Q&A, no tools, no memory
<strong>Example</strong>:
<pre><code class="language-python">from fastapi import FastAPI
from langchain_openai import ChatOpenAI
<p>app = FastAPI()
llm = ChatOpenAI(model=&quot;gpt-4&quot;, temperature=0.0)</p>
<p>@app.post(&quot;/ask&quot;)
async def ask(query: str):
    response = llm.invoke(query)
    return {&quot;answer&quot;: response.content}</code></pre></p>
<strong>Pros</strong>: ‚úÖ Simple, fast to build  
<strong>Cons</strong>: ‚ùå No state, no tools, limited use
<p>---</p>
<h3><strong>Pattern 2: Agent with Tools</strong></h3>
<pre><code class="language-text">User ‚Üí FastAPI ‚Üí Agent ‚Üí Tools ‚Üí Response
                    ‚Üì
                 Memory</code></pre>
<strong>Use case</strong>: Your K8s troubleshooting agent
<strong>Example</strong>:
<pre><code class="language-python">from fastapi import FastAPI
from langchain.agents import AgentExecutor
from langchain.memory import ConversationBufferWindowMemory
<p>app = FastAPI()</p>
<h1>Global agent (in production, use session management)</h1>
memory = ConversationBufferWindowMemory(k=10)
agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory)
<p>@app.post(&quot;/diagnose&quot;)
async def diagnose(query: str):
    result = agent_executor.invoke({&quot;input&quot;: query})
    return {
        &quot;diagnosis&quot;: result[&quot;output&quot;],
        &quot;success&quot;: True
    }</code></pre></p>
<strong>Pros</strong>: ‚úÖ Can take actions, maintains context  
<strong>Cons</strong>: ‚ùå Need session management for multi-user
<p>---</p>
<h3><strong>Pattern 3: RAG System</strong></h3>
<pre><code class="language-text">User ‚Üí FastAPI ‚Üí RAG Chain ‚Üí Vector DB ‚Üí LLM ‚Üí Response</code></pre>
<strong>Use case</strong>: Document Q&A, knowledge base
<strong>Example</strong>:
<pre><code class="language-python">from fastapi import FastAPI
from langchain.chains import RetrievalQA
from langchain.vectorstores import Chroma
<p>app = FastAPI()</p>
<h1>Load vector store</h1>
vectorstore = Chroma(persist_directory=&quot;./db&quot;)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever()
)
<p>@app.post(&quot;/search&quot;)
async def search(query: str):
    result = qa_chain.invoke({&quot;query&quot;: query})
    return {
        &quot;answer&quot;: result[&quot;result&quot;],
        &quot;sources&quot;: result.get(&quot;source_documents&quot;, [])
    }</code></pre></p>
<strong>Pros</strong>: ‚úÖ Grounded in docs, cite sources  
<strong>Cons</strong>: ‚ùå Need document ingestion pipeline
<p>---</p>
<h3><strong>Pattern 4: Agent + RAG (Hybrid)</strong></h3>
<pre><code class="language-text">User ‚Üí FastAPI ‚Üí Agent ‚Üí [Kubectl Tools, RAG Tool] ‚Üí Response
                    ‚Üì
                 Memory</code></pre>
<strong>Use case</strong>: Your K8s agent v2 (production)
<strong>Architecture</strong>:
<pre><code class="language-python">from fastapi import FastAPI
from langchain.agents import AgentExecutor
from langchain.tools import tool
<h1>RAG as a tool</h1>
@tool
def search_runbooks(query: str) -&gt; str:
    &quot;&quot;&quot;Search K8s runbooks&quot;&quot;&quot;
    return qa_chain.invoke({&quot;query&quot;: query})[&quot;result&quot;]
<h1>Agent with kubectl + RAG tools</h1>
tools = [
    get_pod_status,
    get_pod_logs,
    describe_pod,
    search_runbooks  # ‚≠ê RAG tool
]
<p>agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory)</p>
<p>@app.post(&quot;/diagnose&quot;)
async def diagnose(query: str, session_id: str):
    # Get or create session memory
    memory = get_session_memory(session_id)
    
    result = agent_executor.invoke({&quot;input&quot;: query})
    
    return {
        &quot;diagnosis&quot;: result[&quot;output&quot;],
        &quot;session_id&quot;: session_id
    }</code></pre></p>
<strong>Pros</strong>: ‚úÖ Best of both worlds (real-time + docs)  
<strong>Cons</strong>: ‚ùå More complex, higher cost
<p>---</p>
<h2>üìä <strong>Your K8s Agent: 3-Tier Architecture</strong></h2>
<h3><strong>Tier 1: Demo/MVP (What you're building)</strong></h3>
<pre><code class="language-text">‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Terminal  ‚îÇ  (User)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FastAPI App  ‚îÇ
‚îÇ  Port 8000    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LangChain Agent       ‚îÇ
‚îÇ  - GetPodStatus        ‚îÇ
‚îÇ  - GetPodLogs          ‚îÇ
‚îÇ  - DescribePod         ‚îÇ
‚îÇ  - CheckResources      ‚îÇ
‚îÇ  - AnalyzeErrors       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   kubectl   ‚îÇ  (Local K8s)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
<strong>Characteristics</strong>:
<li>‚úÖ Simple, monolithic</li>
<li>‚úÖ Local deployment (minikube)</li>
<li>‚úÖ No database needed</li>
<li>‚úÖ 5 tools, no RAG</li>
<li>‚úÖ BufferWindowMemory (in-process)</li>
<strong>Suitable for</strong>: Demo, interview, proof of concept
<p>---</p>
<h3><strong>Tier 2: Production-Ready</strong></h3>
<pre><code class="language-text">‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Web UI      ‚îÇ  (React/Streamlit)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  API Gateway (FastAPI)   ‚îÇ
‚îÇ  - Auth                  ‚îÇ
‚îÇ  - Rate limiting         ‚îÇ
‚îÇ  - Session management    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Agent Service (K8s Pod)  ‚îÇ
‚îÇ  - LangChain Agent        ‚îÇ
‚îÇ  - Memory (Redis)         ‚îÇ
‚îÇ  - Tools (kubectl + RAG)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Vector DB   ‚îÇ  K8s Cluster‚îÇ
‚îÇ  (Chroma)    ‚îÇ  (kubectl)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
<strong>Additions</strong>:
<li>‚úÖ Web UI</li>
<li>‚úÖ Authentication</li>
<li>‚úÖ Session management (Redis)</li>
<li>‚úÖ RAG for runbooks</li>
<li>‚úÖ Deployed in K8s</li>
<li>‚úÖ Monitoring & logs</li>
<strong>Suitable for</strong>: Internal tooling, small team
<p>---</p>
<h3><strong>Tier 3: Enterprise Scale</strong></h3>
<pre><code class="language-text">‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Load Balancer                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  API Gateway (Kong/NGINX)               ‚îÇ
‚îÇ  - Auth (OAuth)                         ‚îÇ
‚îÇ  - Rate limiting                        ‚îÇ
‚îÇ  - Caching                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Agent Service (Auto-scaling Pods)      ‚îÇ
‚îÇ  - Multiple replicas                    ‚îÇ
‚îÇ  - Load balanced                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Redis ‚îÇ ‚îÇ Vector DB   ‚îÇ
‚îÇ Cluster‚îÇ ‚îÇ (Pinecone) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Observability       ‚îÇ
‚îÇ  - Prometheus        ‚îÇ
‚îÇ  - Grafana           ‚îÇ
‚îÇ  - ELK Stack         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
<strong>Additions</strong>:
<li>‚úÖ Horizontal scaling</li>
<li>‚úÖ High availability</li>
<li>‚úÖ Managed services (Pinecone, Redis Cloud)</li>
<li>‚úÖ Advanced observability</li>
<li>‚úÖ Multi-cluster support</li>
<li>‚úÖ Cost tracking & optimization</li>
<strong>Suitable for</strong>: Large enterprise, thousands of users
<p>---</p>
<h2>üöÄ <strong>Development Workflow</strong></h2>
<h3><strong>Phase 1: Prototype (1-2 days)</strong></h3>
<strong>Goal</strong>: Prove the concept works
<pre><code class="language-text">1. Define problem ‚úÖ (K8s troubleshooting)
2. Build simplest version ‚úÖ (5 tools, local)
3. Test with 10 scenarios ‚úÖ
4. Demo to stakeholders ‚úÖ</code></pre>
<strong>Tools</strong>:
<li>Python script or Jupyter notebook</li>
<li>Local LLM (Ollama) or API (OpenAI)</li>
<li>Hardcoded configs</li>
<li>Print statements for debugging</li>
<strong>Output</strong>: Working demo, validated approach
<p>---</p>
<h3><strong>Phase 2: MVP (3-5 days)</strong></h3>
<strong>Goal</strong>: Production-ready v1
<pre><code class="language-text">1. Add error handling ‚úÖ
2. Create FastAPI service ‚úÖ
3. Add logging ‚úÖ
4. Write tests ‚úÖ
5. Deploy to K8s ‚úÖ
6. Document API ‚úÖ</code></pre>
<strong>Tools</strong>:
<li>FastAPI</li>
<li>Docker</li>
<li>Kubernetes manifests</li>
<li>pytest for testing</li>
<strong>Output</strong>: Deployable service, basic monitoring
<p>---</p>
<h3><strong>Phase 3: Production (1-2 weeks)</strong></h3>
<strong>Goal</strong>: Scale and polish
<pre><code class="language-text">1. Add RAG for runbooks ‚úÖ
2. Implement session management ‚úÖ
3. Build web UI ‚úÖ
4. Add authentication ‚úÖ
5. Set up monitoring (Prometheus) ‚úÖ
6. Load testing &amp; optimization ‚úÖ
7. Write runbooks for ops ‚úÖ</code></pre>
<strong>Tools</strong>:
<li>Redis for sessions</li>
<li>Chroma/Pinecone for RAG</li>
<li>React/Streamlit for UI</li>
<li>Prometheus + Grafana for monitoring</li>
<strong>Output</strong>: Production-ready system
<p>---</p>
<h2>üìê <strong>Design Principles</strong></h2>
<h3><strong>1. Start Simple, Add Complexity Later</strong></h3>
<p>‚ùå <strong>Don't start here</strong>:
<pre><code class="language-text">Agent + RAG + Multi-model + Fine-tuning + Advanced memory + 
Vector DB + Kubernetes + Monitoring + ...</code></pre></p>
<p>‚úÖ <strong>Start here</strong>:
<pre><code class="language-text">Agent + 5 tools + Local deployment</code></pre></p>
<p>Then add:
<li>More tools ‚Üí RAG ‚Üí Better memory ‚Üí Production deployment ‚Üí Monitoring</li></p>
<h3><strong>2. Optimize for Iteration Speed</strong></h3>
<strong>Early stages</strong>: Use APIs (OpenAI, Anthropic)
<li>Fast to test</li>
<li>High quality</li>
<li>Don't worry about cost yet</li>
<strong>Later</strong>: Optimize costs
<li>Self-hosted models (Llama 3)</li>
<li>Caching</li>
<li>Cheaper models for simple tasks</li>
<h3><strong>3. Measure Before Optimizing</strong></h3>
<strong>Collect data first</strong>:
<li>Which tools are used most?</li>
<li>What queries take longest?</li>
<li>What's the error rate?</li>
<li>What's the cost per query?</li>
<strong>Then optimize</strong>:
<li>Cache frequent tool outputs</li>
<li>Use cheaper LLM for simple queries</li>
<li>Remove unused tools</li>
<h3><strong>4. Build for Observability</strong></h3>
<strong>From day 1</strong>:
<li>Log every agent call</li>
<li>Track tool usage</li>
<li>Monitor errors</li>
<li>Measure latency</li>
<strong>Benefits</strong>:
<li>Debug issues faster</li>
<li>Understand usage patterns</li>
<li>Optimize based on data</li>
<p>---</p>
<h2>üß™ <strong>Testing & Evaluation</strong></h2>
<h3><strong>Unit Tests</strong></h3>
<p>Test individual tools:</p>
<pre><code class="language-python">def test_get_pod_status():
    &quot;&quot;&quot;Test pod status tool&quot;&quot;&quot;
    # Happy path
    result = get_pod_status(&quot;nginx-abc&quot;, &quot;default&quot;)
    assert &quot;nginx-abc&quot; in result.lower()
    
    # Pod not found
    result = get_pod_status(&quot;nonexistent-pod&quot;, &quot;default&quot;)
    assert &quot;not found&quot; in result.lower()
    
    # Invalid namespace
    result = get_pod_status(&quot;nginx-abc&quot;, &quot;invalid-ns&quot;)
    assert &quot;error&quot; in result.lower()</code></pre>
<h3><strong>Integration Tests</strong></h3>
<p>Test agent end-to-end:</p>
<pre><code class="language-python">def test_agent_diagnose_crashloop():
    &quot;&quot;&quot;Test agent can diagnose CrashLoopBackOff&quot;&quot;&quot;
    agent = ProductionK8sAgent()
    
    result = agent.diagnose(&quot;Why is pod nginx-abc crashing?&quot;)
    
    assert result[&quot;success&quot;] == True
    assert &quot;crashloop&quot; in result[&quot;output&quot;].lower() or &quot;crash&quot; in result[&quot;output&quot;].lower()
    assert result[&quot;steps&quot;] &gt;= 2  # Should call multiple tools</code></pre>
<h3><strong>Evaluation Set</strong></h3>
<p>Create test cases:</p>
<pre><code class="language-python">test_cases = [
    {
        &quot;query&quot;: &quot;Pod nginx-abc is CrashLoopBackOff&quot;,
        &quot;expected_tools&quot;: [&quot;GetPodStatus&quot;, &quot;GetPodLogs&quot;],
        &quot;expected_keywords&quot;: [&quot;crash&quot;, &quot;error&quot;, &quot;fix&quot;],
        &quot;max_iterations&quot;: 3
    },
    {
        &quot;query&quot;: &quot;Pod is OOMKilled&quot;,
        &quot;expected_tools&quot;: [&quot;GetPodStatus&quot;, &quot;GetPodLogs&quot;, &quot;CheckResources&quot;],
        &quot;expected_keywords&quot;: [&quot;memory&quot;, &quot;limit&quot;, &quot;increase&quot;],
        &quot;max_iterations&quot;: 3
    },
    # ... 20-50 test cases
]
<p>def evaluate_agent(agent, test_cases):
    &quot;&quot;&quot;Run agent on all test cases&quot;&quot;&quot;
    results = []
    
    for test in test_cases:
        result = agent.diagnose(test[&quot;query&quot;])
        
        # Check expected tools were called
        tools_called = extract_tools_from_result(result)
        tools_match = all(tool in tools_called for tool in test[&quot;expected_tools&quot;])
        
        # Check expected keywords in output
        output_lower = result[&quot;output&quot;].lower()
        keywords_match = any(kw in output_lower for kw in test[&quot;expected_keywords&quot;])
        
        # Check iterations
        iterations_ok = result[&quot;steps&quot;] &lt;= test[&quot;max_iterations&quot;]
        
        results.append({
            &quot;query&quot;: test[&quot;query&quot;],
            &quot;success&quot;: result[&quot;success&quot;],
            &quot;tools_match&quot;: tools_match,
            &quot;keywords_match&quot;: keywords_match,
            &quot;iterations_ok&quot;: iterations_ok,
            &quot;score&quot;: sum([result[&quot;success&quot;], tools_match, keywords_match, iterations_ok]) / 4
        })
    
    # Calculate metrics
    avg_score = sum(r[&quot;score&quot;] for r in results) / len(results)
    print(f&quot;Average Score: {avg_score:.2%}&quot;)
    
    return results</code></pre></p>
<p>---</p>
<h2>üí∞ <strong>Cost Optimization</strong></h2>
<h3><strong>1. Model Selection</strong></h3>
<pre><code class="language-python"># Expensive: GPT-4 for everything
llm = ChatOpenAI(model=&quot;gpt-4&quot;)  # $0.03/1K input tokens
<h1>Optimized: GPT-3.5 for simple, GPT-4 for complex</h1>
def get_llm(query: str):
    if is_complex_query(query):
        return ChatOpenAI(model=&quot;gpt-4&quot;)
    else:
        return ChatOpenAI(model=&quot;gpt-3.5-turbo&quot;)  # $0.0015/1K tokens (20x cheaper!)</code></pre>
<h3><strong>2. Caching</strong></h3>
<pre><code class="language-python">from functools import lru_cache
import hashlib
<h1>Cache tool outputs</h1>
@lru_cache(maxsize=100)
def get_pod_status_cached(pod_name: str, namespace: str) -&gt; str:
    &quot;&quot;&quot;Cached version - pod status doesn&#039;t change every second&quot;&quot;&quot;
    return get_pod_status(pod_name, namespace)
    
<h1>Cache LLM responses</h1>
class LLMCache:
    def __init__(self):
        self.cache = {}
    
    def get_or_call(self, prompt: str, llm):
        key = hashlib.md5(prompt.encode()).hexdigest()
        
        if key in self.cache:
            return self.cache[key]
        
        result = llm.invoke(prompt)
        self.cache[key] = result
        return result</code></pre>
<h3><strong>3. Prompt Optimization</strong></h3>
<pre><code class="language-python"># ‚ùå Verbose prompt (wastes tokens)
prompt = f&quot;&quot;&quot;
You are an expert Kubernetes administrator with 10 years of experience...
[500 words of instructions]
<p>Question: {query}
&quot;&quot;&quot;</p>
<h1>‚úÖ Concise prompt</h1>
prompt = f&quot;&quot;&quot;
K8s troubleshooting assistant. Diagnose issues using tools.
<p>Question: {query}
&quot;&quot;&quot;</code></pre></p>
<h3><strong>4. Batch Processing</strong></h3>
<pre><code class="language-python"># ‚ùå One request per query
for query in queries:
    result = llm.invoke(query)
<h1>‚úÖ Batch multiple queries</h1>
batch_prompt = &quot;\n\n&quot;.join([
    f&quot;Query {i+1}: {q}&quot; for i, q in enumerate(queries)
])
result = llm.invoke(batch_prompt)</code></pre>
<p>---</p>
<h2>üé§ <strong>Interview Talking Points</strong></h2>
<h3><strong>"Walk me through your system design"</strong></h3>
<p>*"I designed a 3-tier architecture:*</p>
<p>1. <strong>*Tier 1 (Demo)</strong>*: Monolithic FastAPI app with LangChain agent, 5 kubectl tools, in-process memory. Deployed locally with minikube. Simple, fast to build, proves concept.</p>
<p>2. <strong>*Tier 2 (Production)</strong>*: Separate agent service in Kubernetes, Redis for session management, added RAG tool for runbooks. Includes monitoring, error handling, API authentication.</p>
<p>3. <strong>*Tier 3 (Scale)</strong>*: Horizontal scaling with multiple agent replicas, managed services (Pinecone, Redis Cloud), advanced observability with Prometheus/Grafana, multi-cluster support.</p>
<p>*I started with Tier 1 for the demo, with clear path to Tier 2/3 for production."*</p>
<p>---</p>
<h3><strong>"How did you choose your tech stack?"</strong></h3>
<p>*"I evaluated options across 3 dimensions:*</p>
<strong>LLM</strong>: GPT-4 vs Llama 3
<li>*GPT-4*: Better quality, code-aware (GitHub Copilot)</li>
<li>*Llama 3*: Free, can fine-tune</li>
<li><strong>Choice</strong>: GPT-4 for demo (quality matters), plan to add Llama 3 fallback</li>
<strong>Agent Framework</strong>: LangChain vs custom
<li>*LangChain*: Mature, well-documented, less boilerplate</li>
<li>*Custom*: Full control, lighter weight</li>
<li><strong>Choice</strong>: LangChain (faster development)</li>
<strong>Memory</strong>: Buffer vs Window vs Summary
<li>*BufferWindow*: Fixed size, predictable</li>
<li>*Summary*: Token-efficient but extra LLM calls</li>
<li><strong>Choice</strong>: BufferWindowMemory(k=10) - debugging sessions are short</li>
<p>*Every choice balances speed, quality, and cost."*</p>
<p>---</p>
<h3><strong>"How would you improve this for production?"</strong></h3>
<p>*"5 key improvements:*</p>
<p>1. <strong>RAG Integration</strong>: Add vector DB with runbooks so agent can reference documented solutions</p>
<p>2. <strong>Session Management</strong>: Redis-backed memory for multi-user support</p>
<p>3. <strong>Observability</strong>: Prometheus metrics, structured logging, distributed tracing</p>
<p>4. <strong>Cost Optimization</strong>: Cache tool outputs, use GPT-3.5 for simple queries, batch processing</p>
<p>5. <strong>Safety</strong>: Add approval workflows for write operations, limit to specific namespaces, RBAC</p>
<p>*I'd also add evaluation suite with 50+ test cases to measure accuracy before/after changes."*</p>
<p>---</p>
<h2>üéì <strong>Self-Check: Can You Explain?</strong></h2>
<p>Before the interview, make sure you can answer:</p>
<p>‚úÖ <strong>Architecture</strong>:
<li>Why 3 tiers? (demo ‚Üí prod ‚Üí scale)</li>
<li>Why FastAPI? (async, modern, fast)</li>
<li>Why LangChain? (less boilerplate, mature)</li></p>
<p>‚úÖ <strong>Design Choices</strong>:
<li>Why these 5 tools? (cover 80% of issues)</li>
<li>Why BufferWindowMemory? (short sessions)</li>
<li>Why temperature=0.0? (deterministic)</li></p>
<p>‚úÖ <strong>Trade-offs</strong>:
<li>GPT-4 vs Llama 3? (quality vs cost)</li>
<li>ReAct vs RAG? (real-time vs docs)</li>
<li>BufferWindow vs Summary? (simple vs efficient)</li></p>
<p>‚úÖ <strong>Production</strong>:
<li>How to scale? (K8s replicas, Redis)</li>
<li>How to monitor? (Prometheus, logs)</li>
<li>How to optimize cost? (caching, cheaper models)</li></p>
<p>‚úÖ <strong>Improvement Path</strong>:
<li>v1 ‚Üí v2 ‚Üí v3 roadmap</li>
<li>What to add next? (RAG, auth, UI)</li>
<li>How to evaluate? (test cases, metrics)</li></p>
<p>---</p>
<h2>üöÄ <strong>Key Takeaways</strong></h2>
<p>1. <strong>Start simple</strong>: Agent + tools ‚Üí Add complexity later
2. <strong>3-tier architecture</strong>: Demo ‚Üí Production ‚Üí Scale
3. <strong>Optimize for iteration</strong>: Fast feedback loop
4. <strong>Measure everything</strong>: Logs, metrics, costs
5. <strong>Test systematically</strong>: Unit ‚Üí integration ‚Üí evaluation
6. <strong>Plan for scale</strong>: But don't over-engineer early
7. <strong>Know trade-offs</strong>: Explain every design decision</p>
<p>---</p>
<h2>üéâ <strong>Congratulations!</strong></h2>
<p>You've completed all 12 core modules! You now have:
<li>‚úÖ Understanding of LLMs, tokens, embeddings</li>
<li>‚úÖ Knowledge of ReAct, tool calling, agents</li>
<li>‚úÖ Mastery of LangChain components</li>
<li>‚úÖ Production patterns for error handling</li>
<li>‚úÖ System design principles</li></ul></p>
<strong>Next</strong>: Review QUICK_REFERENCE.md and INTERVIEW_DEMO_PREP.md, then build your project!
<p>---</p>
<strong>Time to complete this module</strong>: 30 minutes  
<strong>Total learning time (all 12 modules)</strong>: ~8-10 hours  
<strong>You're ready to build!</strong> üöÄ

    </div>
    

    <div class="module-content" id="module-13">
        <h1>Hands-On Exercises: Build Your Skills</h1>
<strong>Total Time</strong>: ~2-3 hours  
<strong>Prerequisites</strong>: Complete Modules 1-12
<p>---</p>
<h2>üéØ <strong>Overview</strong></h2>
<p>These exercises will help you:
1. Practice what you learned
2. Build muscle memory for coding patterns
3. Gain confidence before the project
4. Identify knowledge gaps</p>
<strong>Approach</strong>: Try each exercise yourself first, then check the solution.
<p>---</p>
<h2>üèãÔ∏è <strong>Exercise 1: Call an LLM (30 minutes)</strong></h2>
<h3><strong>Objective</strong></h3>
Learn to make basic LLM API calls with different parameters.
<h3><strong>Task</strong></h3>
Create a Python script that:
1. Calls OpenAI API (or Ollama locally)
2. Tests different temperatures (0.0, 0.5, 1.0)
3. Compares outputs
<h3><strong>Starter Code</strong></h3>
<pre><code class="language-python">from langchain_openai import ChatOpenAI
<p>def test_llm_temperatures():
    &quot;&quot;&quot;Test LLM with different temperatures&quot;&quot;&quot;
    query = &quot;List 3 common Kubernetes pod issues&quot;
    
    for temp in [0.0, 0.5, 1.0]:
        print(f&quot;\n{&#039;=&#039;*50}&quot;)
        print(f&quot;Temperature: {temp}&quot;)
        print(f&quot;{&#039;=&#039;*50}&quot;)
        
        # TODO: Create LLM with this temperature
        # TODO: Call LLM with query
        # TODO: Print response
        
if __name__ == &quot;__main__&quot;:
    test_llm_temperatures()</code></pre></p>
<h3><strong>Your Solution</strong></h3>
<pre><code class="language-python"># Write your code here</code></pre>
<details>
<summary>Show Solution</summary>
<pre><code class="language-python">from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
<p>def test_llm_temperatures():
    &quot;&quot;&quot;Test LLM with different temperatures&quot;&quot;&quot;
    query = &quot;List 3 common Kubernetes pod issues&quot;
    
    for temp in [0.0, 0.5, 1.0]:
        print(f&quot;\n{&#039;=&#039;*50}&quot;)
        print(f&quot;Temperature: {temp}&quot;)
        print(f&quot;{&#039;=&#039;*50}&quot;)
        
        # Create LLM with temperature
        llm = ChatOpenAI(
            model=&quot;gpt-4&quot;,
            temperature=temp
        )
        
        # Call LLM
        response = llm.invoke([HumanMessage(content=query)])
        
        # Print response
        print(response.content)
        print()</p>
<p>if __name__ == &quot;__main__&quot;:
    test_llm_temperatures()</code></pre></p>
<strong>Expected Output</strong>:
<ul><li>Temperature 0.0: Same output every run</li>
<li>Temperature 0.5: Slight variations</li>
<li>Temperature 1.0: More creative, varied outputs</li>
</details>
<h3><strong>Bonus Challenge</strong></h3>
Modify to use a local Ollama model instead of OpenAI.
<p>---</p>
<h2>üîß <strong>Exercise 2: Create a Tool (45 minutes)</strong></h2>
<h3><strong>Objective</strong></h3>
Build a kubectl tool with proper error handling.
<h3><strong>Task</strong></h3>
Create a `check_pod_resources` tool that:
1. Runs `kubectl top pod <pod_name>`
2. Handles errors (pod not found, kubectl missing, timeout)
3. Returns CPU and memory usage as a string
<h3><strong>Starter Code</strong></h3>
<pre><code class="language-python">from langchain.tools import tool
import subprocess
<p>@tool
def check_pod_resources(pod_name: str, namespace: str = &quot;default&quot;) -&gt; str:
    &quot;&quot;&quot;Check CPU and memory usage of a pod.
    
    Use this when you need to see if a pod is consuming too many resources
    or hitting resource limits.
    
    Args:
        pod_name: Name of the pod to check
        namespace: Kubernetes namespace (default: default)
    
    Returns:
        String with CPU and memory usage, or error message
    &quot;&quot;&quot;
    # TODO: Run kubectl top pod command
    # TODO: Handle errors (not found, timeout, kubectl missing)
    # TODO: Return formatted output
    pass</p>
<h1>Test it</h1>
if __name__ == &quot;__main__&quot;:
    # Test with a pod that exists
    result = check_pod_resources(&quot;nginx-abc&quot;)
    print(result)
    
    # Test with pod that doesn&#039;t exist
    result = check_pod_resources(&quot;nonexistent-pod&quot;)
    print(result)</code></pre>
<h3><strong>Your Solution</strong></h3>
<pre><code class="language-python"># Write your code here</code></pre>
<details>
<summary>Show Solution</summary>
<pre><code class="language-python">from langchain.tools import tool
import subprocess
<p>@tool
def check_pod_resources(pod_name: str, namespace: str = &quot;default&quot;) -&gt; str:
    &quot;&quot;&quot;Check CPU and memory usage of a pod.
    
    Use this when you need to see if a pod is consuming too many resources
    or hitting resource limits.
    
    Args:
        pod_name: Name of the pod to check
        namespace: Kubernetes namespace (default: default)
    
    Returns:
        String with CPU and memory usage, or error message
    &quot;&quot;&quot;
    try:
        # Run kubectl top pod command
        result = subprocess.run(
            [&quot;kubectl&quot;, &quot;top&quot;, &quot;pod&quot;, pod_name, &quot;-n&quot;, namespace],
            capture_output=True,
            text=True,
            timeout=10
        )
        
        # Check if command succeeded
        if result.returncode != 0:
            if &quot;not found&quot; in result.stderr.lower():
                return f&quot;Error: Pod &#039;{pod_name}&#039; not found in namespace &#039;{namespace}&#039;&quot;
            elif &quot;metrics not available&quot; in result.stderr.lower():
                return f&quot;Error: Metrics not available. Is metrics-server installed?&quot;
            else:
                return f&quot;Error: {result.stderr.strip()}&quot;
        
        # Parse output
        # Format: NAME        CPU(cores)   MEMORY(bytes)
        lines = result.stdout.strip().split(&#039;\n&#039;)
        if len(lines) &lt; 2:
            return &quot;Error: Unexpected output format&quot;
        
        # Return the data line
        return f&quot;Pod resource usage:\n{lines[1]}&quot;
        
    except subprocess.TimeoutExpired:
        return &quot;Error: Command timed out after 10 seconds&quot;
    
    except FileNotFoundError:
        return &quot;Error: kubectl command not found. Is it installed?&quot;
    
    except Exception as e:
        return f&quot;Unexpected error: {str(e)}&quot;</p>
<h1>Test it</h1>
if __name__ == &quot;__main__&quot;:
    print(&quot;Test 1: Existing pod&quot;)
    result = check_pod_resources(&quot;coredns-5d78c9869d-abcde&quot;, &quot;kube-system&quot;)
    print(result)
    print()
    
    print(&quot;Test 2: Non-existent pod&quot;)
    result = check_pod_resources(&quot;nonexistent-pod&quot;)
    print(result)</code></pre>
</details>
<h3><strong>Bonus Challenge</strong></h3>
Modify to also show resource limits from `kubectl describe pod`.
<p>---</p>
<h2>ü§ñ <strong>Exercise 3: Build a Simple Agent (1 hour)</strong></h2>
<h3><strong>Objective</strong></h3>
Create a working ReAct agent with 3 tools.
<h3><strong>Task</strong></h3>
Build an agent that can:
1. Get pod status
2. Get pod logs (last 20 lines)
3. Describe a pod
4. Use memory to maintain context
<h3><strong>Starter Code</strong></h3>
<pre><code class="language-python">from langchain.agents import create_react_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferWindowMemory
from langchain.tools import tool
import subprocess
<h1>TODO: Define 3 tools (get_pod_status, get_pod_logs, describe_pod)</h1>
<h1>TODO: Create ReAct prompt template</h1>
<h1>TODO: Create agent</h1>
<h1>TODO: Create agent executor with memory</h1>
<h1>Test conversation</h1>
if __name__ == &quot;__main__&quot;:
    # Query 1
    result = agent_executor.invoke({&quot;input&quot;: &quot;Check status of pod nginx-abc&quot;})
    print(result[&quot;output&quot;])
    
    # Query 2 (agent should remember nginx-abc from context)
    result = agent_executor.invoke({&quot;input&quot;: &quot;What are the logs?&quot;})
    print(result[&quot;output&quot;])</code></pre>
<h3><strong>Your Solution</strong></h3>
<pre><code class="language-python"># Write your code here</code></pre>
<details>
<summary>Show Solution</summary>
<pre><code class="language-python">from langchain.agents import create_react_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferWindowMemory
from langchain.tools import tool
import subprocess
<h1>Define tools</h1>
@tool
def get_pod_status(pod_name: str, namespace: str = &quot;default&quot;) -&gt; str:
    &quot;&quot;&quot;Get status of a Kubernetes pod. Use this FIRST when diagnosing issues.&quot;&quot;&quot;
    try:
        result = subprocess.run(
            [&quot;kubectl&quot;, &quot;get&quot;, &quot;pod&quot;, pod_name, &quot;-n&quot;, namespace],
            capture_output=True, text=True, timeout=5
        )
        return result.stdout if result.returncode == 0 else f&quot;Error: {result.stderr}&quot;
    except Exception as e:
        return f&quot;Error: {str(e)}&quot;
<p>@tool
def get_pod_logs(pod_name: str, namespace: str = &quot;default&quot;, tail: int = 20) -&gt; str:
    &quot;&quot;&quot;Get pod logs. Use when pod is crashing to see error messages.&quot;&quot;&quot;
    try:
        result = subprocess.run(
            [&quot;kubectl&quot;, &quot;logs&quot;, pod_name, &quot;-n&quot;, namespace, f&quot;--tail={tail}&quot;],
            capture_output=True, text=True, timeout=10
        )
        return result.stdout if result.returncode == 0 else f&quot;Error: {result.stderr}&quot;
    except Exception as e:
        return f&quot;Error: {str(e)}&quot;</p>
<p>@tool
def describe_pod(pod_name: str, namespace: str = &quot;default&quot;) -&gt; str:
    &quot;&quot;&quot;Get detailed pod info and events. Use when status and logs aren&#039;t enough.&quot;&quot;&quot;
    try:
        result = subprocess.run(
            [&quot;kubectl&quot;, &quot;describe&quot;, &quot;pod&quot;, pod_name, &quot;-n&quot;, namespace],
            capture_output=True, text=True, timeout=10
        )
        return result.stdout if result.returncode == 0 else f&quot;Error: {result.stderr}&quot;
    except Exception as e:
        return f&quot;Error: {str(e)}&quot;</p>
<p>tools = [get_pod_status, get_pod_logs, describe_pod]</p>
<h1>Create prompt</h1>
react_prompt = &quot;&quot;&quot;
Answer the user&#039;s question using these tools: {tools}
<p>Tool names: {tool_names}</p>
<p>Use this format:</p>
<p>Question: the user&#039;s question
Thought: think about what to do
Action: the tool to use (one of [{tool_names}])
Action Input: the input to the tool
Observation: the result from the tool
... (repeat Thought/Action/Observation as needed)
Thought: I now know the final answer
Final Answer: the complete answer</p>
<p>Question: {input}
{agent_scratchpad}
&quot;&quot;&quot;</p>
<p>prompt = PromptTemplate.from_template(react_prompt)</p>
<h1>Create LLM</h1>
llm = ChatOpenAI(model=&quot;gpt-4&quot;, temperature=0.0)
<h1>Create agent</h1>
agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)
<h1>Create memory</h1>
memory = ConversationBufferWindowMemory(
    k=5,
    memory_key=&quot;chat_history&quot;,
    return_messages=True
)
<h1>Create agent executor</h1>
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    memory=memory,
    max_iterations=5,
    verbose=True,
    handle_parsing_errors=True
)
<h1>Test conversation</h1>
if __name__ == &quot;__main__&quot;:
    print(&quot;\n&quot; + &quot;=&quot;*50)
    print(&quot;Query 1: Check pod status&quot;)
    print(&quot;=&quot;*50)
    result = agent_executor.invoke({&quot;input&quot;: &quot;Check status of pod coredns-abc in kube-system&quot;})
    print(f&quot;\nResult: {result[&#039;output&#039;]}&quot;)
    
    print(&quot;\n&quot; + &quot;=&quot;*50)
    print(&quot;Query 2: Get logs (agent should remember the pod)&quot;)
    print(&quot;=&quot;*50)
    result = agent_executor.invoke({&quot;input&quot;: &quot;What are the logs?&quot;})
    print(f&quot;\nResult: {result[&#039;output&#039;]}&quot;)</code></pre>
</details>
<h3><strong>Bonus Challenge</strong></h3>
Add a 4th tool that checks resource usage and have the agent use it appropriately.
<p>---</p>
<h2>üíæ <strong>Exercise 4: Implement Memory (30 minutes)</strong></h2>
<h3><strong>Objective</strong></h3>
Understand different memory types by implementing them.
<h3><strong>Task</strong></h3>
Compare BufferMemory vs BufferWindowMemory:
1. Create both memory types
2. Add 10 message exchanges
3. Check what each remembers
<h3><strong>Starter Code</strong></h3>
<pre><code class="language-python">from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory
<p>def test_memory_types():
    &quot;&quot;&quot;Compare different memory types&quot;&quot;&quot;
    
    # TODO: Create BufferMemory
    buffer_memory = None
    
    # TODO: Create BufferWindowMemory with k=3
    window_memory = None
    
    # Add 5 exchanges to both
    exchanges = [
        (&quot;Check pod nginx-abc&quot;, &quot;Pod is Running&quot;),
        (&quot;Check pod redis-xyz&quot;, &quot;Pod is CrashLoopBackOff&quot;),
        (&quot;What are redis logs?&quot;, &quot;Error: Connection refused&quot;),
        (&quot;Check pod mongo-123&quot;, &quot;Pod is Pending&quot;),
        (&quot;What&#039;s mongo status?&quot;, &quot;Waiting for PVC&quot;)
    ]
    
    # TODO: Add all exchanges to both memories
    
    # TODO: Print what each memory remembers
    
if __name__ == &quot;__main__&quot;:
    test_memory_types()</code></pre></p>
<h3><strong>Your Solution</strong></h3>
<pre><code class="language-python"># Write your code here</code></pre>
<details>
<summary>Show Solution</summary>
<pre><code class="language-python">from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory
<p>def test_memory_types():
    &quot;&quot;&quot;Compare different memory types&quot;&quot;&quot;
    
    # Create BufferMemory (stores all)
    buffer_memory = ConversationBufferMemory()
    
    # Create BufferWindowMemory (stores last k=3)
    window_memory = ConversationBufferWindowMemory(k=3)
    
    # Add 5 exchanges to both
    exchanges = [
        (&quot;Check pod nginx-abc&quot;, &quot;Pod is Running&quot;),
        (&quot;Check pod redis-xyz&quot;, &quot;Pod is CrashLoopBackOff&quot;),
        (&quot;What are redis logs?&quot;, &quot;Error: Connection refused&quot;),
        (&quot;Check pod mongo-123&quot;, &quot;Pod is Pending&quot;),
        (&quot;What&#039;s mongo status?&quot;, &quot;Waiting for PVC&quot;)
    ]
    
    print(&quot;Adding 5 exchanges to both memories...\n&quot;)
    for user_input, ai_output in exchanges:
        buffer_memory.save_context(
            {&quot;input&quot;: user_input},
            {&quot;output&quot;: ai_output}
        )
        window_memory.save_context(
            {&quot;input&quot;: user_input},
            {&quot;output&quot;: ai_output}
        )
    
    # Check what each remembers
    print(&quot;=&quot;*60)
    print(&quot;BufferMemory (stores ALL):&quot;)
    print(&quot;=&quot;*60)
    buffer_vars = buffer_memory.load_memory_variables({})
    print(buffer_vars[&quot;history&quot;])
    print()
    
    print(&quot;=&quot;*60)
    print(&quot;BufferWindowMemory with k=3 (stores last 3 exchanges):&quot;)
    print(&quot;=&quot;*60)
    window_vars = window_memory.load_memory_variables({})
    print(window_vars[&quot;history&quot;])
    print()
    
    # Count messages
    buffer_lines = buffer_vars[&quot;history&quot;].count(&quot;\n&quot;)
    window_lines = window_vars[&quot;history&quot;].count(&quot;\n&quot;)
    
    print(f&quot;BufferMemory: {buffer_lines} lines (all 5 exchanges)&quot;)
    print(f&quot;WindowMemory: {window_lines} lines (last 3 exchanges)&quot;)
    print()
    print(&quot;Notice: WindowMemory dropped the first 2 exchanges (nginx and redis status)&quot;)</p>
<p>if __name__ == &quot;__main__&quot;:
    test_memory_types()</code></pre></p>
<strong>Expected Output</strong>:
<pre><code class="language-text">BufferMemory stores all 5 exchanges:
<li>Check pod nginx-abc ‚Üí Pod is Running</li>
<li>Check pod redis-xyz ‚Üí Pod is CrashLoopBackOff</li>
<li>What are redis logs? ‚Üí Error: Connection refused</li>
<li>Check pod mongo-123 ‚Üí Pod is Pending</li>
<li>What&#039;s mongo status? ‚Üí Waiting for PVC</li>
<p>WindowMemory (k=3) stores only last 3:
<li>What are redis logs? ‚Üí Error: Connection refused</li>
<li>Check pod mongo-123 ‚Üí Pod is Pending</li>
<li>What&#039;s mongo status? ‚Üí Waiting for PVC</li>
(First 2 exchanges dropped)</code></pre></p>
</details>
<p>---</p>
<h2>üß¨ <strong>Exercise 5: Practice Debugging (45 minutes)</strong></h2>
<h3><strong>Objective</strong></h3>
Learn to debug agent issues using verbose mode and logs.
<h3><strong>Task</strong></h3>
An agent is stuck in a loop. Fix it!
<h3><strong>Broken Code</strong></h3>
<pre><code class="language-python">from langchain.agents import create_react_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.tools import tool
<p>@tool
def get_info(query: str) -&gt; str:
    &quot;&quot;&quot;Get information&quot;&quot;&quot;  # ‚ö†Ô∏è Vague description!
    return f&quot;Info about {query}&quot;</p>
<p>@tool
def check_status(query: str) -&gt; str:
    &quot;&quot;&quot;Check status&quot;&quot;&quot;  # ‚ö†Ô∏è Vague description!
    return f&quot;Status of {query}&quot;</p>
<p>tools = [get_info, check_status]</p>
<p>prompt = PromptTemplate.from_template(&quot;&quot;&quot;
Answer: {input}</p>
<p>{agent_scratchpad}
&quot;&quot;&quot;)  # ‚ö†Ô∏è Missing format instructions!</p>
<p>llm = ChatOpenAI(temperature=1.0)  # ‚ö†Ô∏è High temperature!</p>
<p>agent = create_react_agent(llm, tools, prompt)</p>
<h1>‚ö†Ô∏è No max_iterations!</h1>
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
<h1>This will likely loop or fail</h1>
result = agent_executor.invoke({&quot;input&quot;: &quot;Check pod status&quot;})</code></pre>
<h3><strong>Your Task</strong></h3>
1. Identify all problems
2. Fix each one
3. Test that it works
<h3><strong>Your Solution</strong></h3>
<pre><code class="language-python"># Write your fixed code here</code></pre>
<details>
<summary>Show Solution</summary>
<strong>Problems identified</strong>:
1. ‚ùå Vague tool descriptions ‚Üí Agent doesn't know when to use them
2. ‚ùå Missing ReAct format in prompt ‚Üí Agent doesn't know how to structure output
3. ‚ùå High temperature (1.0) ‚Üí Random tool selection
4. ‚ùå No max_iterations ‚Üí Can loop forever
5. ‚ùå No error handling ‚Üí Will crash on parsing errors
<strong>Fixed code</strong>:
<pre><code class="language-python">from langchain.agents import create_react_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.tools import tool
<h1>‚úÖ Clear, specific tool descriptions</h1>
@tool
def get_pod_info(pod_name: str) -&gt; str:
    &quot;&quot;&quot;Get general information about a pod.
    
    Use this when you need to see pod configuration, labels, or metadata.
    NOT for status (use check_pod_status instead).
    
    Args:
        pod_name: Name of the pod
    &quot;&quot;&quot;
    return f&quot;Info: Pod {pod_name} has labels app=nginx, replicas=3&quot;
<p>@tool
def check_pod_status(pod_name: str) -&gt; str:
    &quot;&quot;&quot;Check the current status of a pod.
    
    Use this when you need to see if pod is Running, Pending, CrashLoopBackOff, etc.
    This should be your FIRST step when diagnosing pod issues.
    
    Args:
        pod_name: Name of the pod
    &quot;&quot;&quot;
    return f&quot;Status: Pod {pod_name} is Running&quot;</p>
<p>tools = [get_pod_info, check_pod_status]</p>
<h1>‚úÖ Complete ReAct prompt template</h1>
prompt = PromptTemplate.from_template(&quot;&quot;&quot;
Answer the user&#039;s question using these tools: {tools}
<p>Tool names: {tool_names}</p>
<p>Use this format:</p>
<p>Question: the user&#039;s question
Thought: think about what to do
Action: the tool to use (one of [{tool_names}])
Action Input: the input to the tool
Observation: the result from the tool
... (repeat as needed)
Thought: I now know the final answer
Final Answer: the answer</p>
<p>Question: {input}
{agent_scratchpad}
&quot;&quot;&quot;)</p>
<h1>‚úÖ Low temperature for deterministic behavior</h1>
llm = ChatOpenAI(model=&quot;gpt-4&quot;, temperature=0.0)
<p>agent = create_react_agent(llm, tools, prompt)</p>
<h1>‚úÖ Max iterations and error handling</h1>
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    max_iterations=5,  # ‚úÖ Prevent infinite loops
    early_stopping_method=&quot;generate&quot;,  # ‚úÖ Graceful exit
    handle_parsing_errors=True,  # ‚úÖ Handle format errors
    verbose=True
)
<h1>Now works correctly</h1>
result = agent_executor.invoke({&quot;input&quot;: &quot;Check status of pod nginx-abc&quot;})
print(f&quot;\nResult: {result[&#039;output&#039;]}&quot;)</code></pre>
</details>
<p>---</p>
<h2>üéØ <strong>Exercise 6: End-to-End System (1 hour)</strong></h2>
<h3><strong>Objective</strong></h3>
Build a complete minimal system combining everything learned.
<h3><strong>Task</strong></h3>
Create a FastAPI service with:
1. An endpoint `/diagnose` that takes a pod name
2. An agent with 3 tools
3. Memory for conversation
4. Error handling
5. Basic logging
<h3><strong>Starter Code</strong></h3>
<pre><code class="language-python">from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from langchain.agents import create_react_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferWindowMemory
import logging
<h1>Setup logging</h1>
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
<p>app = FastAPI()</p>
<h1>TODO: Define request/response models</h1>
class DiagnoseRequest(BaseModel):
    pass  # Fill this in
<p>class DiagnoseResponse(BaseModel):
    pass  # Fill this in</p>
<h1>TODO: Create agent with tools</h1>
<h1>TODO: Create endpoint</h1>
@app.post(&quot;/diagnose&quot;)
async def diagnose(request: DiagnoseRequest):
    pass  # Implement this
<p>if __name__ == &quot;__main__&quot;:
    import uvicorn
    uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=8000)</code></pre></p>
<h3><strong>Your Solution</strong></h3>
<pre><code class="language-python"># Write your code here</code></pre>
<details>
<summary>Show Solution</summary>
<pre><code class="language-python">from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from langchain.agents import create_react_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferWindowMemory
from langchain.tools import tool
import subprocess
import logging
import time
<h1>Setup logging</h1>
logging.basicConfig(
    level=logging.INFO,
    format=&#039;%(asctime)s - %(name)s - %(levelname)s - %(message)s&#039;
)
logger = logging.getLogger(__name__)
<p>app = FastAPI(title=&quot;K8s Diagnostic Agent&quot;)</p>
<h1>Request/Response models</h1>
class DiagnoseRequest(BaseModel):
    pod_name: str
    namespace: str = &quot;default&quot;
    session_id: str = &quot;default&quot;
<p>class DiagnoseResponse(BaseModel):
    success: bool
    diagnosis: str
    duration_ms: float
    error: str = None</p>
<h1>Define tools</h1>
@tool
def get_pod_status(pod_name: str, namespace: str = &quot;default&quot;) -&gt; str:
    &quot;&quot;&quot;Get pod status. Use FIRST when diagnosing.&quot;&quot;&quot;
    try:
        result = subprocess.run(
            [&quot;kubectl&quot;, &quot;get&quot;, &quot;pod&quot;, pod_name, &quot;-n&quot;, namespace],
            capture_output=True, text=True, timeout=5
        )
        return result.stdout if result.returncode == 0 else f&quot;Error: {result.stderr}&quot;
    except Exception as e:
        return f&quot;Error: {str(e)}&quot;
<p>@tool
def get_pod_logs(pod_name: str, namespace: str = &quot;default&quot;) -&gt; str:
    &quot;&quot;&quot;Get pod logs when pod is crashing.&quot;&quot;&quot;
    try:
        result = subprocess.run(
            [&quot;kubectl&quot;, &quot;logs&quot;, pod_name, &quot;-n&quot;, namespace, &quot;--tail=20&quot;],
            capture_output=True, text=True, timeout=10
        )
        return result.stdout if result.returncode == 0 else f&quot;Error: {result.stderr}&quot;
    except Exception as e:
        return f&quot;Error: {str(e)}&quot;</p>
<p>@tool
def describe_pod(pod_name: str, namespace: str = &quot;default&quot;) -&gt; str:
    &quot;&quot;&quot;Get detailed pod info when status/logs aren&#039;t enough.&quot;&quot;&quot;
    try:
        result = subprocess.run(
            [&quot;kubectl&quot;, &quot;describe&quot;, &quot;pod&quot;, pod_name, &quot;-n&quot;, namespace],
            capture_output=True, text=True, timeout=10
        )
        return result.stdout if result.returncode == 0 else f&quot;Error: {result.stderr}&quot;
    except Exception as e:
        return f&quot;Error: {str(e)}&quot;</p>
<p>tools = [get_pod_status, get_pod_logs, describe_pod]</p>
<h1>Create prompt</h1>
react_prompt = &quot;&quot;&quot;
You are a Kubernetes troubleshooting expert.
<p>Tools: {tools}
Tool names: {tool_names}</p>
<p>Format:
Question: {input}
Thought: [reasoning]
Action: [tool name]
Action Input: [tool input]
Observation: [result]
... (repeat)
Final Answer: [diagnosis with recommended fixes]</p>
<p>Question: {input}
{agent_scratchpad}
&quot;&quot;&quot;</p>
<p>prompt = PromptTemplate.from_template(react_prompt)</p>
<h1>Create LLM</h1>
llm = ChatOpenAI(model=&quot;gpt-4&quot;, temperature=0.0)
<h1>Create agent</h1>
agent = create_react_agent(llm, tools, prompt)
<h1>Global memory (in production, use Redis with session IDs)</h1>
memory = ConversationBufferWindowMemory(
    k=10,
    memory_key=&quot;chat_history&quot;,
    return_messages=True
)
<h1>Create agent executor</h1>
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    memory=memory,
    max_iterations=5,
    early_stopping_method=&quot;generate&quot;,
    handle_parsing_errors=True,
    verbose=True
)
<p>@app.post(&quot;/diagnose&quot;, response_model=DiagnoseResponse)
async def diagnose(request: DiagnoseRequest):
    &quot;&quot;&quot;Diagnose a Kubernetes pod&quot;&quot;&quot;
    start_time = time.time()
    
    logger.info(f&quot;Diagnosing pod: {request.pod_name} in namespace: {request.namespace}&quot;)
    
    try:
        # Build query
        query = f&quot;Diagnose pod {request.pod_name} in namespace {request.namespace}&quot;
        
        # Call agent
        result = agent_executor.invoke({&quot;input&quot;: query})
        
        duration = (time.time() - start_time) * 1000
        
        logger.info(f&quot;Diagnosis completed in {duration:.0f}ms&quot;)
        
        return DiagnoseResponse(
            success=True,
            diagnosis=result[&quot;output&quot;],
            duration_ms=duration
        )
        
    except Exception as e:
        duration = (time.time() - start_time) * 1000
        logger.error(f&quot;Error during diagnosis: {e}&quot;)
        
        return DiagnoseResponse(
            success=False,
            diagnosis=&quot;&quot;,
            duration_ms=duration,
            error=str(e)
        )</p>
<p>@app.get(&quot;/health&quot;)
async def health():
    &quot;&quot;&quot;Health check endpoint&quot;&quot;&quot;
    return {&quot;status&quot;: &quot;healthy&quot;}</p>
<p>if __name__ == &quot;__main__&quot;:
    import uvicorn
    uvicorn.run(app, host=&quot;0.0.0.0&quot;, port=8000)</code></pre></p>
<strong>Test it</strong>:
<pre><code class="language-bash"># Terminal 1: Start server
python app.py
<h1>Terminal 2: Test endpoint</h1>
curl -X POST &quot;http://localhost:8000/diagnose&quot; \
  -H &quot;Content-Type: application/json&quot; \
  -d &#039;{&quot;pod_name&quot;: &quot;nginx-abc&quot;, &quot;namespace&quot;: &quot;default&quot;}&#039;</code></pre>
</details>
<p>---</p>
<h2>üéì <strong>Self-Assessment</strong></h2>
<p>After completing these exercises, you should be able to:</p>
<p>‚úÖ Make basic LLM API calls  
‚úÖ Understand temperature effects  
‚úÖ Create tools with error handling  
‚úÖ Build a working ReAct agent  
‚úÖ Implement and compare memory types  
‚úÖ Debug agent issues  
‚úÖ Build a complete FastAPI service</p>
<strong>If you struggled with any</strong>:
<li>Review the relevant module</li>
<li>Try the exercise again</li>
<li>Check the solution and understand each line</li>
<p>---</p>
<h2>üöÄ <strong>Next Steps</strong></h2>
<p>You're now ready to:
1. ‚úÖ Build your K8s troubleshooting agent project
2. ‚úÖ Demo it to Hoang and the team
3. ‚úÖ Explain every technical decision confidently</p>
<strong>Before starting the project</strong>:
<li>Review QUICK_REFERENCE.md</li>
<li>Read INTERVIEW_DEMO_PREP.md</li>
<li>Practice explaining concepts out loud</li></ul>
<p>---</p>
<h2>üí° <strong>Tips for Success</strong></h2>
<p>1. <strong>Type, don't copy-paste</strong>: Muscle memory matters
2. <strong>Experiment</strong>: Change parameters, see what happens
3. <strong>Break things</strong>: Best way to learn is fixing errors
4. <strong>Take notes</strong>: Write down your "aha!" moments
5. <strong>Practice explaining</strong>: Teach concepts to rubber duck</p>
<p>---</p>
<strong>Congratulations on completing the hands-on exercises! You're ready to build! üéâ</strong>
<strong>Total learning time</strong>: ~10-12 hours (modules + exercises)  
<strong>Project time</strong>: ~8-12 hours (Level 1-3 implementation)  
<strong>Total</strong>: ~20-24 hours to master AI agent development!

    </div>
    

    <div class="module-content" id="module-14">
        <h1>Interview & Demo Preparation: K8s AI Agent Project</h1>
<strong>Talking points for presenting to Hoang and team</strong>
<p>---</p>
<h2>üéØ <strong>Opening Statement (30 seconds)</strong></h2>
<p>*"I built an AI-powered Kubernetes troubleshooting agent that uses the ReAct pattern to autonomously diagnose cluster issues. It combines GPT-4's reasoning with live kubectl commands through LangChain tools. The agent maintains conversation memory and can iteratively debug by calling tools, analyzing results, and deciding next steps. I deployed it as a FastAPI service in Kubernetes with RBAC for secure cluster access."*</p>
<p>---</p>
<h2>üìã <strong>Demo Flow (5-10 minutes)</strong></h2>
<h3><strong>1. Problem Statement (1 min)</strong></h3>
*"Traditional K8s troubleshooting requires manual kubectl commands and deep expertise. I wanted to create an intelligent assistant that could autonomously investigate issues and suggest fixes."*
<h3><strong>2. Architecture Overview (2 min)</strong></h3>
Show diagram and explain:
<pre><code class="language-text">User: &quot;Pod myapp-5678 is crashing&quot;
    ‚Üì
FastAPI ‚Üí LangChain ReAct Agent
    ‚Üì
Thought: &quot;I need to check pod status first&quot;
    ‚Üì
Action: GetPodStatus(pod_name=&quot;myapp-5678&quot;)
    ‚Üì
Observation: &quot;Pod is CrashLoopBackOff&quot;
    ‚Üì
Thought: &quot;Now I need logs to see why&quot;
    ‚Üì
Action: GetPodLogs(pod_name=&quot;myapp-5678&quot;)
    ‚Üì
Observation: &quot;Error: Connection to DB failed&quot;
    ‚Üì
Final Answer: &quot;Pod failing due to DB connection issue...&quot;</code></pre>
<strong>Key Points</strong>:
<ul><li>ReAct pattern enables autonomous reasoning</li>
<li>5 focused kubectl tools (not 20+)</li>
<li>Conversation memory for multi-turn debugging</li>
<li>Structured output parsing for reliability</li>
<h3><strong>3. Live Demo (3-5 min)</strong></h3>
<p>#### <strong>Scenario 1: Simple Diagnosis</strong>
<pre><code class="language-bash"># Start the agent
kubectl port-forward svc/k8s-agent 8000:8000</p>
<h1>Query</h1>
curl -X POST http://localhost:8000/agent \
  -d &#039;{&quot;query&quot;: &quot;Why is nginx-deployment-abc123 not starting?&quot;}&#039;</code></pre>
<strong>Expected Flow</strong>:
1. Agent checks pod status (Pending)
2. Agent describes pod (sees ImagePullBackOff)
3. Agent returns: "Pod can't pull image 'nginx:wrong-tag' - tag doesn't exist"
<strong>Talking Point</strong>: *"Notice the agent autonomously decided which tools to call. I didn't hardcode 'check status then describe pod' - it reasoned through the problem."*
<p>#### <strong>Scenario 2: Multi-Turn Debugging</strong>
<pre><code class="language-text">User: &quot;Help me debug cart-service-xyz&quot;
Agent: [checks status, finds CrashLoopBackOff]</p>
<p>User: &quot;What&#039;s in the logs?&quot;
Agent: [retrieves logs, finds OOMKilled]</p>
<p>User: &quot;What are the resource limits?&quot;
Agent: [checks resources, sees 128Mi limit]
Agent: &quot;Container exceeds 128Mi memory limit. Recommendation: Increase to 256Mi.&quot;</code></pre></p>
<strong>Talking Point</strong>: *"The agent maintains context across messages using BufferWindowMemory. It remembers we're debugging cart-service-xyz without me repeating it."*
<h3><strong>4. Code Walkthrough (2 min)</strong></h3>
<p>#### <strong>Show Tool Implementation</strong>
<pre><code class="language-python">@tool
def get_pod_status(pod_name: str, namespace: str = &quot;default&quot;) -&gt; str:
    &quot;&quot;&quot;Get the status of a Kubernetes pod.
    
    Use this tool when you need to check if a pod is Running, Pending, 
    CrashLoopBackOff, or in any other state.
    
    Args:
        pod_name: Name of the pod (required)
        namespace: Kubernetes namespace (default: default)
    
    Returns:
        String describing pod status and conditions
    &quot;&quot;&quot;
    try:
        result = subprocess.run(
            [&quot;kubectl&quot;, &quot;get&quot;, &quot;pod&quot;, pod_name, &quot;-n&quot;, namespace],
            capture_output=True, text=True, timeout=5
        )
        if result.returncode != 0:
            return f&quot;Error: Pod {pod_name} not found in namespace {namespace}&quot;
        return result.stdout
    except Exception as e:
        return f&quot;Error checking pod status: {str(e)}&quot;</code></pre></p>
<strong>Talking Points</strong>:
<li>‚úÖ Detailed description helps agent decide when to use tool</li>
<li>‚úÖ Error handling prevents crashes</li>
<li>‚úÖ Timeout prevents hanging</li>
<li>‚úÖ Returns string (not object) for LLM parsing</li>
<p>#### <strong>Show Agent Setup</strong>
<pre><code class="language-python">from langchain.agents import create_react_agent, AgentExecutor
from langchain.memory import ConversationBufferWindowMemory</p>
<p>memory = ConversationBufferWindowMemory(
    k=10,  # Last 10 exchanges
    memory_key=&quot;chat_history&quot;,
    return_messages=True
)</p>
<p>agent_executor = AgentExecutor(
    agent=agent,
    tools=[get_pod_status, get_pod_logs, describe_pod, 
           analyze_errors, check_resources],
    memory=memory,
    verbose=True,
    max_iterations=5,  # Prevent infinite loops
    handle_parsing_errors=True
)</code></pre></p>
<strong>Talking Points</strong>:
<li>‚úÖ max_iterations prevents runaway loops</li>
<li>‚úÖ handle_parsing_errors increases robustness</li>
<li>‚úÖ verbose=True for debugging (disable in prod)</li>
<li>‚úÖ BufferWindowMemory with k=10 balances context and tokens</li>
<p>---</p>
<h2>üó£Ô∏è <strong>Anticipated Questions & Answers</strong></h2>
<h3><strong>Q: Why ReAct instead of RAG?</strong></h3>
<strong>A</strong>: *"RAG is for retrieval-augmented generation - great when you have a large knowledge base of documents to search. My agent needs to interact with live cluster state, not retrieve static docs. ReAct pattern allows the agent to call tools (kubectl commands) based on what it discovers. That said, a production version could combine both: use RAG to retrieve runbook docs, and ReAct to execute diagnostic commands."*
<h3><strong>Q: Why only 5 tools? Why not cover all kubectl commands?</strong></h3>
<strong>A</strong>: *"Research shows agents perform worse with too many tools - they get confused deciding which to use. I analyzed common K8s issues and found 80% fall into 5 categories: status checks, logs, resource descriptions, error patterns, and resource limits. These 5 tools cover most scenarios. For edge cases, I can add specialized tools, but starting focused improves accuracy."*
<h3><strong>Q: How does the agent decide which tool to call?</strong></h3>
<strong>A</strong>: *"The LLM reads each tool's description and compares it to the current situation. For example, if the agent's thought is 'I need to check pod status', it scans all tool descriptions and finds GetPodStatus has 'Use this when checking if pod is Running, Pending, CrashLoopBackOff...' - that matches, so it calls that tool. This is why clear, detailed descriptions are critical."*
<h3><strong>Q: What if the agent hallucinates or gives wrong advice?</strong></h3>
<strong>A</strong>: *"Several safeguards: First, I use temperature=0.0 for deterministic outputs. Second, all tools validate inputs before executing. Third, I set max_iterations=5 to prevent loops. Fourth, I parse tool outputs into structured format. But yes, hallucinations can still happen - in production I'd add human-in-the-loop approval for risky actions like pod deletion or config changes. This v1 is read-only for safety."*
<h3><strong>Q: Why GitHub Copilot instead of Llama or GPT-4?</strong></h3>
<strong>A</strong>: *"Three options I considered:*
1. *GitHub Copilot: Access to our code repos, understands our deployment patterns. Best for code-aware troubleshooting.*
2. *GPT-4: Highest quality reasoning but expensive ($$$) and no code context.*
3. *Llama 3 70B: Free, open-source, good quality, but no code context and needs GPU.*
<p>*I chose Copilot for the demo since troubleshooting benefits from understanding our codebase. For cost-sensitive production, I'd evaluate Llama 3 with fine-tuning on our runbooks."*</p>
<h3><strong>Q: How does memory work?</strong></h3>
<strong>A</strong>: *"I use ConversationBufferWindowMemory which stores the last 10 message exchanges. When the agent receives a new query, it sees the last 10 back-and-forths, giving it context. For example:*
<p>*User: 'Debug cart-service-xyz'*  
*Agent: [checks status]*  
*User: 'What are the logs?'*  
*Agent: [knows we're still talking about cart-service-xyz]*</p>
<p>*I chose window memory over summary memory because debugging sessions are typically short (5-10 exchanges). Window memory is simpler and more predictable. For longer sessions, I'd use SummaryBufferMemory which summarizes old context to save tokens."*</p>
<h3><strong>Q: What about security? Can the agent delete pods?</strong></h3>
<strong>A</strong>: *"Security through layers:*
1. *RBAC: The service account only has read permissions - get, list, describe. No delete, create, or update.*
2. *Tool design: All 5 tools are read-only operations.*
3. *Future: For write operations, I'd add approval workflows or restrict to specific namespaces.*
<p>*This follows principle of least privilege. The agent can't break things because it literally doesn't have permission."*</p>
<h3><strong>Q: How would you improve this for production?</strong></h3>
<p>Great question - my roadmap has 3 phases:</p>
<strong>Phase 1 (Current)</strong>: Read-only diagnostics with 5 tools  
<strong>Phase 2</strong>: Add RAG for runbook retrieval, expand to 10 tools covering networking and storage  
<strong>Phase 3</strong>: Safe write operations with approval (restart pod, scale deployment)
<strong>Specific improvements</strong>:
1. <strong>Evaluation</strong>: Collect 100 real issues, measure agent accuracy, identify failures
2. <strong>Caching</strong>: Cache tool responses (pod status doesn't change every second)
3. <strong>Observability</strong>: Log agent decisions, track which tools are used most
4. <strong>Fine-tuning</strong>: Fine-tune Llama 3 on our runbooks to reduce cost vs Copilot
5. <strong>UI</strong>: Build Streamlit dashboard for non-technical users
6. <strong>Multi-cluster</strong>: Extend to work across dev/staging/prod environments
7. <strong>Proactive monitoring</strong>: Agent detects issues before users report them
<h3><strong>Q: How long did this take you to build?</strong></h3>
<strong>A</strong>: *"About 12 hours total:*
<li>*2 hours: Learning AI fundamentals (LLMs, ReAct, LangChain)*</li>
<li>*3 hours: Designing architecture and choosing components*</li>
<li>*4 hours: Implementing tools, agent, and FastAPI service*</li>
<li>*2 hours: K8s deployment, RBAC setup, and testing*</li>
<li>*1 hour: Documentation and this demo*</li>
<p>*The key was spending time upfront understanding the patterns rather than jumping into code. Once I grasped ReAct and tool calling, the implementation was straightforward."*</p>
<h3><strong>Q: What was the hardest part?</strong></h3>
<strong>A</strong>: *"Two challenges:*
<strong>1. Tool descriptions</strong>: My first attempt had vague descriptions like 'Get pod info'. The agent kept calling the wrong tools. I learned descriptions must be specific about WHEN to use the tool, not just WHAT it does. Added examples and use cases, which improved accuracy significantly.*
<strong>2. Token management</strong>: Early version crashed when debugging sessions exceeded context window. Switching to BufferWindowMemory and monitoring token usage fixed it. Learned to always consider token limits when designing agents."*
<p>---</p>
<h2>üéØ <strong>Key Talking Points to Memorize</strong></h2>
<h3><strong>Why ReAct?</strong></h3>
‚úÖ Enables tool calling for live data  
‚úÖ Autonomous reasoning  
‚úÖ Better than hardcoded if/else logic
<h3><strong>Design Decisions</strong></h3>
‚úÖ 5 focused tools > 20 generic tools (prevents confusion)  
‚úÖ Temperature 0.0 for deterministic tool selection  
‚úÖ max_iterations=5 prevents infinite loops  
‚úÖ Read-only tools for safety  
‚úÖ BufferWindowMemory for short debugging sessions
<h3><strong>Production Considerations</strong></h3>
‚úÖ Add RAG for runbook retrieval  
‚úÖ Implement approval workflows for write operations  
‚úÖ Cache tool responses  
‚úÖ Fine-tune on internal docs  
‚úÖ Add observability and metrics  
‚úÖ Evaluate accuracy on real issues
<p>---</p>
<h2>üöÄ <strong>Demo Success Checklist</strong></h2>
<strong>Before Demo</strong>:
<li>[ ] K8s cluster running (minikube start)</li>
<li>[ ] Agent deployed (kubectl apply -f deployment.yaml)</li>
<li>[ ] Create test pods with issues (ImagePullBackOff, CrashLoopBackOff, OOMKilled)</li>
<li>[ ] Test queries work end-to-end</li>
<li>[ ] Review all talking points</li>
<li>[ ] Prepare 2-3 example queries</li>
<strong>During Demo</strong>:
<li>[ ] Start with architecture diagram</li>
<li>[ ] Show live demo (don't just talk about it)</li>
<li>[ ] Walk through code (tools & agent setup)</li>
<li>[ ] Explain design decisions proactively</li>
<li>[ ] Mention production improvements</li>
<strong>After Demo</strong>:
<li>[ ] Ask for feedback: "What would you improve?"</li>
<li>[ ] Offer to add features: "I can add X if helpful"</li>
<li>[ ] Show eagerness to learn: "I'd love to extend this with RAG"</li></ul>
<p>---</p>
<h2>üí° <strong>Confidence Boosters</strong></h2>
<h3><strong>If You Get Stuck</strong></h3>
*"That's a great question - I want to make sure I give you an accurate answer. Let me think through that..."* [Take 5 seconds to compose thoughts]
<h3><strong>If You Don't Know</strong></h3>
*"I haven't implemented that yet, but here's how I'd approach it..."* [Show your thinking process]
<h3><strong>If Demo Breaks</strong></h3>
*"Interesting - this is a great example of why robust error handling is critical. Let me check the logs..."* [Turn it into a learning moment]
<h3><strong>To Show Enthusiasm</strong></h3>
*"I'm really excited about this pattern because..."*  
*"One thing I'd love to explore next is..."*  
*"This project taught me..."*
<p>---</p>
<h2>üéì <strong>Fast Review (5 min before demo)</strong></h2>
<strong>What is your agent?</strong>  
K8s troubleshooting assistant using ReAct pattern with 5 kubectl tools
<strong>Why ReAct?</strong>  
Needs to call tools to get live cluster data, not just reason with training data
<strong>Why 5 tools?</strong>  
Cover 80% of issues, more tools confuse agent
<strong>Why this memory?</strong>  
BufferWindowMemory - debugging sessions are short, needs recent context
<strong>Why Copilot?</strong>  
Code-aware troubleshooting, understands our repos
<strong>Security?</strong>  
RBAC read-only, all tools are safe operations
<strong>Production improvements?</strong>  
RAG, caching, fine-tuning, approval workflows, observability
<strong>Time to build?</strong>  
~12 hours (2 learning, 3 design, 4 implementation, 2 K8s, 1 docs)
<p>---</p>
<h2>üèÜ <strong>Closing Statement</strong></h2>
<p>*"This project demonstrates my ability to quickly learn and apply ML concepts to real problems. I understand the fundamentals - LLMs, agents, tool calling, memory management - and can explain every design decision. More importantly, I can iterate and improve: add RAG, expand tools, optimize for production. I'm excited to bring this fast-learning, first-principles approach to your ML team."*</p>
<p>---</p>
<strong>You've got this! üöÄ</strong> Remember: Confidence comes from understanding, and you've built that foundation. Good luck!

    </div>
    

    <div class="module-content" id="module-15">
        <h1>Quick Reference: AI/ML Concepts for K8s Agent Project</h1>
<strong>Use this as a cheat sheet during interviews and demos</strong>
<p>---</p>
<h2>ü§ñ <strong>LLMs (Large Language Models)</strong></h2>
<h3><strong>What They Are</strong></h3>
<ul><li>Neural networks trained on massive text data</li>
<li>Predict next token based on patterns</li>
<li>Parameters = "knobs" (more = smarter but slower)</li>
<h3><strong>Key Specs</strong></h3>
| Model | Parameters | Context Window | Cost | Speed |
|-------|------------|----------------|------|-------|
| GPT-4 | ~1.7T | 8K-128K | $$$ | Slow |
| Llama 3 70B | 70B | 8K | Free | Medium |
| Llama 3 7B | 7B | 8K | Free | Fast |
<h3><strong>Important Numbers</strong></h3>
<li>1 token ‚âà 0.75 words</li>
<li>8K tokens ‚âà 6K words ‚âà 10 pages</li>
<li>Temperature 0.0 = deterministic, 1.0 = creative</li>
<p>---</p>
<h2>‚öõÔ∏è <strong>ReAct Pattern</strong></h2>
<h3><strong>The Loop</strong></h3>
<pre><code class="language-text">Thought ‚Üí Action ‚Üí Observation ‚Üí Thought ‚Üí ...</code></pre>
<h3><strong>Why Use It</strong></h3>
<li>‚úÖ Can call tools to get real data</li>
<li>‚úÖ Can interact with systems (kubectl, APIs)</li>
<li>‚úÖ Can verify facts, not just guess</li>
<li>‚ùå More complex than simple prompting</li>
<li>‚ùå Multiple LLM calls (slower/expensive)</li>
<h3><strong>When to Use</strong></h3>
<li>‚úÖ Troubleshooting (need real cluster data)</li>
<li>‚úÖ Math/calculations (need calculator)</li>
<li>‚úÖ Current info (need search tool)</li>
<li>‚ùå Just explaining concepts (use simple prompt)</li>
<li>‚ùå Creative writing (no tools needed)</li>
<p>---</p>
<h2>üîß <strong>Tools/Function Calling</strong></h2>
<h3><strong>3 Essential Parts</strong></h3>
1. <strong>Name</strong>: What LLM calls it
2. <strong>Function</strong>: Python code that executes
3. <strong>Description</strong>: When/how to use it
<h3><strong>Good Description Template</strong></h3>
<pre><code class="language-text">[What it does in one sentence]
<p>Use this when:
<li>[Scenario 1]</li>
<li>[Scenario 2]</li></p>
<p>Input: [Format]
Output: [What it returns]</p>
<p>Example:
Input: [example]
Output: [result]</code></pre></p>
<h3><strong>Best Practices</strong></h3>
<li>‚úÖ 5-7 tools (not 20+)</li>
<li>‚úÖ Clear, specific descriptions</li>
<li>‚úÖ Handle all errors</li>
<li>‚úÖ Validate inputs</li>
<li>‚úÖ Return strings (not objects)</li>
<p>---</p>
<h2>üß† <strong>Memory Types</strong></h2>
<p>| Type | When to Use | Pros | Cons |
|------|-------------|------|------|
| <strong>BufferMemory</strong> | Demos, short sessions | Simple | Token overflow |
| <strong>BufferWindowMemory</strong> ‚≠ê | Multi-turn debugging | Predictable | Loses old context |
| <strong>SummaryMemory</strong> | Long sessions | Saves tokens | Extra LLM calls |
| <strong>SummaryBufferMemory</strong> | Production | Best balance | Complex |</p>
<h3><strong>Your Project: Use BufferWindowMemory</strong></h3>
<pre><code class="language-python">from langchain.memory import ConversationBufferWindowMemory
<p>memory = ConversationBufferWindowMemory(
    k=10,  # Last 10 exchanges
    memory_key=&quot;chat_history&quot;,
    return_messages=True
)</code></pre></p>
<p>---</p>
<h2>üìä <strong>RAG (Retrieval-Augmented Generation)</strong></h2>
<h3><strong>What It Is</strong></h3>
<pre><code class="language-text">User Query ‚Üí Retrieve Relevant Docs ‚Üí LLM + Docs ‚Üí Answer</code></pre>
<h3><strong>Components</strong></h3>
1. <strong>Embeddings</strong>: Convert text to vectors
2. <strong>Vector DB</strong>: Store/search embeddings (Chroma, Pinecone)
3. <strong>Retriever</strong>: Find relevant docs
4. <strong>Generator</strong>: LLM synthesizes answer
<h3><strong>When Needed</strong></h3>
<li>‚úÖ Large knowledge base (docs, runbooks)</li>
<li>‚úÖ Frequently updated info</li>
<li>‚úÖ Want to cite sources</li>
<li>‚ùå Simple Q&A (overkill)</li>
<li>‚ùå Your project v1 (add later)</li>
<p>---</p>
<h2>üèóÔ∏è <strong>Your K8s Agent Architecture</strong></h2>
<pre><code class="language-text">User Query
    ‚Üì
FastAPI Service (in K8s)
    ‚Üì
LangChain ReAct Agent
    ‚îú‚îÄ LLM (Copilot/Llama 3)
    ‚îú‚îÄ Memory (BufferWindowMemory)
    ‚îú‚îÄ Tools:
    ‚îÇ   ‚îú‚îÄ GetPodStatus
    ‚îÇ   ‚îú‚îÄ GetPodLogs
    ‚îÇ   ‚îú‚îÄ DescribePod
    ‚îÇ   ‚îú‚îÄ AnalyzeErrors
    ‚îÇ   ‚îî‚îÄ CheckResources
    ‚îî‚îÄ Output Parser (Pydantic)
    ‚Üì
Structured Response</code></pre>
<p>---</p>
<h2>üéØ <strong>Component Trade-offs</strong></h2>
<h3><strong>LLM Choice</strong></h3>
<p>#### <strong>GitHub Copilot (GPT-4)</strong>
<li>‚úÖ Best: Code understanding, your repos</li>
<li>‚úÖ Quality: Highest accuracy</li>
<li>‚ùå Cost: $$$ per call</li>
<li>‚ùå Vendor lock-in</li></p>
<p>#### <strong>Llama 3 70B</strong>
<li>‚úÖ Free and open source</li>
<li>‚úÖ Good quality (90% of GPT-4)</li>
<li>‚úÖ Can fine-tune</li>
<li>‚ùå No code context</li>
<li>‚ùå Need GPU to run</li></p>
<p>#### <strong>Llama 3 7B</strong>
<li>‚úÖ Fast inference</li>
<li>‚úÖ Runs on CPU</li>
<li>‚úÖ Free</li>
<li>‚ùå Lower quality</li>
<li>‚ùå Simpler reasoning</li></p>
<h3><strong>Agent Pattern</strong></h3>
<p>#### <strong>ReAct</strong>
<li>‚úÖ Can use tools</li>
<li>‚úÖ Autonomous decisions</li>
<li>‚úÖ Best for troubleshooting</li>
<li>‚ùå Multiple LLM calls</li>
<li>‚ùå Complex to debug</li></p>
<p>#### <strong>Chain-of-Thought</strong>
<li>‚úÖ Simple reasoning</li>
<li>‚úÖ One LLM call</li>
<li>‚ùå No tools</li>
<li>‚ùå Can't get real data</li></p>
<h3><strong>Memory Strategy</strong></h3>
<p>#### <strong>BufferWindowMemory (Your choice)</strong>
<li>‚úÖ Simple, predictable</li>
<li>‚úÖ Fixed token usage</li>
<li>‚úÖ Good for debugging sessions</li>
<li>‚ùå Loses old context</li>
<li>‚ùå Not ideal for very long sessions</li></p>
<p>#### <strong>SummaryBufferMemory</strong>
<li>‚úÖ Best token efficiency</li>
<li>‚úÖ Preserves key info</li>
<li>‚ùå Extra LLM calls</li>
<li>‚ùå More complex</li>
<li>‚ùå Overkill for demo</li></p>
<p>---</p>
<h2>üí¨ <strong>Interview Talking Points</strong></h2>
<h3><strong>Why This Architecture?</strong></h3>
<p>*"I chose ReAct pattern because we need to interact with live Kubernetes clusters - the agent must call kubectl commands to get real data. Chain-of-Thought wouldn't work because it only reasons with training data."*</p>
<h3><strong>Why These Tools?</strong></h3>
<p>*"I focused on 5 essential diagnostic tools rather than creating one for every kubectl command. More tools confuse the agent and hurt performance. These 5 cover 90% of common issues: status checks, logs, details, error patterns, and resources."*</p>
<h3><strong>Why This Memory?</strong></h3>
<p>*"I used ConversationBufferWindowMemory with k=10 because debugging sessions typically involve 5-10 exchanges. This prevents token overflow while maintaining recent context. For production, I'd upgrade to SummaryBufferMemory for longer sessions."*</p>
<h3><strong>Why Copilot?</strong></h3>
<p>*"GitHub Copilot has access to our code repositories, so it can analyze deployment YAMLs and suggest fixes specific to our stack. Alternative was Llama 3 which is free but lacks code context. Trade-off: cost vs accuracy."*</p>
<p>---</p>
<h2>üö® <strong>Common Pitfalls to Avoid</strong></h2>
<p>| Mistake | Why Bad | Fix |
|---------|---------|-----|
| Too many tools | Confuses agent | 5-7 focused tools max |
| Vague tool descriptions | Wrong tool selection | Detailed descriptions with examples |
| No error handling | Agent crashes | Try/except in all tools |
| High temperature | Inconsistent tool picks | Use 0.0-0.3 for agents |
| No max_iterations | Infinite loops | Set to 5-10 |
| Ignoring token limits | Context overflow | Monitor and manage memory |</p>
<p>---</p>
<h2>üìù <strong>Quick Decision Matrix</strong></h2>
<h3><strong>Should I add RAG?</strong></h3>
<li>Do you have >100 pages of docs? ‚Üí Yes</li>
<li>Is info frequently updated? ‚Üí Yes</li>
<li>Demo only, static K8s? ‚Üí No (add later)</li>
<h3><strong>Should I use GPT-4 or Llama?</strong></h3>
<li>Need code analysis? ‚Üí GPT-4/Copilot</li>
<li>Budget constrained? ‚Üí Llama 3</li>
<li>Demo simplicity? ‚Üí Llama 3 (local, no API keys)</li>
<h3><strong>Should I add more tools?</strong></h3>
<li>Is tool used >20% of time? ‚Üí Yes</li>
<li>Does it add unique capability? ‚Üí Yes</li>
<li>Just another view of same data? ‚Üí No</li>
<p>---</p>
<h2>üéì <strong>One-Sentence Explanations</strong></h2>
<strong>LLM</strong>: Neural network that predicts next token based on patterns in training data
<strong>Token</strong>: Smallest unit LLM processes, roughly 0.75 words
<strong>Context Window</strong>: Maximum tokens LLM can "remember" at once
<strong>ReAct</strong>: Pattern where LLM thinks, calls tools, observes results, and repeats
<strong>Tool Calling</strong>: Giving LLM ability to request function executions
<strong>Memory</strong>: Storing conversation history to maintain context
<strong>RAG</strong>: Retrieving relevant docs before generating answer
<strong>Embeddings</strong>: Converting text to numeric vectors for similarity search
<strong>Agent</strong>: LLM with tools and autonomy to solve tasks
<strong>Temperature</strong>: Controls randomness (0=deterministic, 1=creative)
<p>---</p>
<h2>üéØ <strong>Confidence Checklist</strong></h2>
<p>Before the demo, can you explain:
<li>‚úÖ Why you chose ReAct over chain-of-thought?</li>
<li>‚úÖ Why these specific 5 tools?</li>
<li>‚úÖ How token limits affect your agent?</li>
<li>‚úÖ Why you chose this memory type?</li>
<li>‚úÖ Trade-offs of Copilot vs Llama?</li>
<li>‚úÖ How the agent decides which tool to use?</li>
<li>‚úÖ What happens if agent gets stuck in a loop?</li>
<li>‚úÖ How you'd improve this in production?</li></ul></p>
<p>If yes to all ‚Üí <strong>You're ready!</strong> üöÄ</p>
    </div>
    
        </main>
    </div>

    <button class="scroll-top" id="scrollTop" onclick="scrollToTop()">‚Üë</button>

    <script>
        // Show first module by default
        document.addEventListener('DOMContentLoaded', () => {
            showModule('module-0');
        });

        function showModule(moduleId) {
            // Hide all modules
            document.querySelectorAll('.module-content').forEach(el => {
                el.classList.remove('active');
            });

            // Show selected module
            document.getElementById(moduleId).classList.add('active');

            // Update nav active state
            document.querySelectorAll('.nav-item').forEach(el => {
                el.classList.remove('active');
            });
            event.target.classList.add('active');

            // Scroll to top
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // Progress bar
        window.addEventListener('scroll', () => {
            const winScroll = document.body.scrollTop || document.documentElement.scrollTop;
            const height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
            const scrolled = (winScroll / height) * 100;
            document.getElementById('progressBar').style.transform = `scaleX(${scrolled / 100})`;

            // Show/hide scroll to top button
            const scrollTop = document.getElementById('scrollTop');
            if (winScroll > 300) {
                scrollTop.classList.add('visible');
            } else {
                scrollTop.classList.remove('visible');
            }
        });

        function scrollToTop() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // Keyboard navigation
        document.addEventListener('keydown', (e) => {
            const modules = Array.from(document.querySelectorAll('.module-content'));
            const activeModule = document.querySelector('.module-content.active');
            const currentIndex = modules.indexOf(activeModule);

            if (e.key === 'ArrowRight' && currentIndex < modules.length - 1) {
                showModule(`module-${currentIndex + 1}`);
            } else if (e.key === 'ArrowLeft' && currentIndex > 0) {
                showModule(`module-${currentIndex - 1}`);
            }
        });
    </script>
</body>
</html>