# Quick Reference: AI/ML Concepts for K8s Agent Project

**Use this as a cheat sheet during interviews and demos**

---

## ðŸ¤– **LLMs (Large Language Models)**

### **What They Are**
- Neural networks trained on massive text data
- Predict next token based on patterns
- Parameters = "knobs" (more = smarter but slower)

### **Key Specs**
| Model | Parameters | Context Window | Cost | Speed |
|-------|------------|----------------|------|-------|
| GPT-4 | ~1.7T | 8K-128K | $$$ | Slow |
| Llama 3 70B | 70B | 8K | Free | Medium |
| Llama 3 7B | 7B | 8K | Free | Fast |

### **Important Numbers**
- 1 token â‰ˆ 0.75 words
- 8K tokens â‰ˆ 6K words â‰ˆ 10 pages
- Temperature 0.0 = deterministic, 1.0 = creative

---

## âš›ï¸ **ReAct Pattern**

### **The Loop**
```
Thought â†’ Action â†’ Observation â†’ Thought â†’ ...
```

### **Why Use It**
- âœ… Can call tools to get real data
- âœ… Can interact with systems (kubectl, APIs)
- âœ… Can verify facts, not just guess
- âŒ More complex than simple prompting
- âŒ Multiple LLM calls (slower/expensive)

### **When to Use**
- âœ… Troubleshooting (need real cluster data)
- âœ… Math/calculations (need calculator)
- âœ… Current info (need search tool)
- âŒ Just explaining concepts (use simple prompt)
- âŒ Creative writing (no tools needed)

---

## ðŸ”§ **Tools/Function Calling**

### **3 Essential Parts**
1. **Name**: What LLM calls it
2. **Function**: Python code that executes
3. **Description**: When/how to use it

### **Good Description Template**
```
[What it does in one sentence]

Use this when:
- [Scenario 1]
- [Scenario 2]

Input: [Format]
Output: [What it returns]

Example:
Input: [example]
Output: [result]
```

### **Best Practices**
- âœ… 5-7 tools (not 20+)
- âœ… Clear, specific descriptions
- âœ… Handle all errors
- âœ… Validate inputs
- âœ… Return strings (not objects)

---

## ðŸ§  **Memory Types**

| Type | When to Use | Pros | Cons |
|------|-------------|------|------|
| **BufferMemory** | Demos, short sessions | Simple | Token overflow |
| **BufferWindowMemory** â­ | Multi-turn debugging | Predictable | Loses old context |
| **SummaryMemory** | Long sessions | Saves tokens | Extra LLM calls |
| **SummaryBufferMemory** | Production | Best balance | Complex |

### **Your Project: Use BufferWindowMemory**
```python
from langchain.memory import ConversationBufferWindowMemory

memory = ConversationBufferWindowMemory(
    k=10,  # Last 10 exchanges
    memory_key="chat_history",
    return_messages=True
)
```

---

## ðŸ“Š **RAG (Retrieval-Augmented Generation)**

### **What It Is**
```
User Query â†’ Retrieve Relevant Docs â†’ LLM + Docs â†’ Answer
```

### **Components**
1. **Embeddings**: Convert text to vectors
2. **Vector DB**: Store/search embeddings (Chroma, Pinecone)
3. **Retriever**: Find relevant docs
4. **Generator**: LLM synthesizes answer

### **When Needed**
- âœ… Large knowledge base (docs, runbooks)
- âœ… Frequently updated info
- âœ… Want to cite sources
- âŒ Simple Q&A (overkill)
- âŒ Your project v1 (add later)

---

## ðŸ—ï¸ **Your K8s Agent Architecture**

```
User Query
    â†“
FastAPI Service (in K8s)
    â†“
LangChain ReAct Agent
    â”œâ”€ LLM (Copilot/Llama 3)
    â”œâ”€ Memory (BufferWindowMemory)
    â”œâ”€ Tools:
    â”‚   â”œâ”€ GetPodStatus
    â”‚   â”œâ”€ GetPodLogs
    â”‚   â”œâ”€ DescribePod
    â”‚   â”œâ”€ AnalyzeErrors
    â”‚   â””â”€ CheckResources
    â””â”€ Output Parser (Pydantic)
    â†“
Structured Response
```

---

## ðŸŽ¯ **Component Trade-offs**

### **LLM Choice**

#### **GitHub Copilot (GPT-4)**
- âœ… Best: Code understanding, your repos
- âœ… Quality: Highest accuracy
- âŒ Cost: $$$ per call
- âŒ Vendor lock-in

#### **Llama 3 70B**
- âœ… Free and open source
- âœ… Good quality (90% of GPT-4)
- âœ… Can fine-tune
- âŒ No code context
- âŒ Need GPU to run

#### **Llama 3 7B**
- âœ… Fast inference
- âœ… Runs on CPU
- âœ… Free
- âŒ Lower quality
- âŒ Simpler reasoning

### **Agent Pattern**

#### **ReAct**
- âœ… Can use tools
- âœ… Autonomous decisions
- âœ… Best for troubleshooting
- âŒ Multiple LLM calls
- âŒ Complex to debug

#### **Chain-of-Thought**
- âœ… Simple reasoning
- âœ… One LLM call
- âŒ No tools
- âŒ Can't get real data

### **Memory Strategy**

#### **BufferWindowMemory (Your choice)**
- âœ… Simple, predictable
- âœ… Fixed token usage
- âœ… Good for debugging sessions
- âŒ Loses old context
- âŒ Not ideal for very long sessions

#### **SummaryBufferMemory**
- âœ… Best token efficiency
- âœ… Preserves key info
- âŒ Extra LLM calls
- âŒ More complex
- âŒ Overkill for demo

---

## ðŸ’¬ **Interview Talking Points**

### **Why This Architecture?**

*"I chose ReAct pattern because we need to interact with live Kubernetes clusters - the agent must call kubectl commands to get real data. Chain-of-Thought wouldn't work because it only reasons with training data."*

### **Why These Tools?**

*"I focused on 5 essential diagnostic tools rather than creating one for every kubectl command. More tools confuse the agent and hurt performance. These 5 cover 90% of common issues: status checks, logs, details, error patterns, and resources."*

### **Why This Memory?**

*"I used ConversationBufferWindowMemory with k=10 because debugging sessions typically involve 5-10 exchanges. This prevents token overflow while maintaining recent context. For production, I'd upgrade to SummaryBufferMemory for longer sessions."*

### **Why Copilot?**

*"GitHub Copilot has access to our code repositories, so it can analyze deployment YAMLs and suggest fixes specific to our stack. Alternative was Llama 3 which is free but lacks code context. Trade-off: cost vs accuracy."*

---

## ðŸš¨ **Common Pitfalls to Avoid**

| Mistake | Why Bad | Fix |
|---------|---------|-----|
| Too many tools | Confuses agent | 5-7 focused tools max |
| Vague tool descriptions | Wrong tool selection | Detailed descriptions with examples |
| No error handling | Agent crashes | Try/except in all tools |
| High temperature | Inconsistent tool picks | Use 0.0-0.3 for agents |
| No max_iterations | Infinite loops | Set to 5-10 |
| Ignoring token limits | Context overflow | Monitor and manage memory |

---

## ðŸ“ **Quick Decision Matrix**

### **Should I add RAG?**
- Do you have >100 pages of docs? â†’ Yes
- Is info frequently updated? â†’ Yes
- Demo only, static K8s? â†’ No (add later)

### **Should I use GPT-4 or Llama?**
- Need code analysis? â†’ GPT-4/Copilot
- Budget constrained? â†’ Llama 3
- Demo simplicity? â†’ Llama 3 (local, no API keys)

### **Should I add more tools?**
- Is tool used >20% of time? â†’ Yes
- Does it add unique capability? â†’ Yes
- Just another view of same data? â†’ No

---

## ðŸŽ“ **One-Sentence Explanations**

**LLM**: Neural network that predicts next token based on patterns in training data

**Token**: Smallest unit LLM processes, roughly 0.75 words

**Context Window**: Maximum tokens LLM can "remember" at once

**ReAct**: Pattern where LLM thinks, calls tools, observes results, and repeats

**Tool Calling**: Giving LLM ability to request function executions

**Memory**: Storing conversation history to maintain context

**RAG**: Retrieving relevant docs before generating answer

**Embeddings**: Converting text to numeric vectors for similarity search

**Agent**: LLM with tools and autonomy to solve tasks

**Temperature**: Controls randomness (0=deterministic, 1=creative)

---

## ðŸŽ¯ **Confidence Checklist**

Before the demo, can you explain:
- âœ… Why you chose ReAct over chain-of-thought?
- âœ… Why these specific 5 tools?
- âœ… How token limits affect your agent?
- âœ… Why you chose this memory type?
- âœ… Trade-offs of Copilot vs Llama?
- âœ… How the agent decides which tool to use?
- âœ… What happens if agent gets stuck in a loop?
- âœ… How you'd improve this in production?

If yes to all â†’ **You're ready!** ðŸš€
