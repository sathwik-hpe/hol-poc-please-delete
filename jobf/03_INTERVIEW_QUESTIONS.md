# Fidelity Senior Cloud Developer - Interview Questions & Answers

## ðŸ“ž Phone Screen Questions

### Q1: Walk me through your resume and relevant experience.

**Framework: Present-Past-Future**

**Answer**:
"Currently, I'm working as a [your current role] where I focus on cloud infrastructure automation and Kubernetes orchestration. I've built production-grade systems using Go and Python, deployed on AWS and Azure.

In my previous role at [company], I designed and implemented [specific achievement - e.g., 'a self-service platform for provisioning cloud resources that reduced deployment time by 70%']. I used Terraform for infrastructure-as-code and integrated it with CI/CD pipelines.

What excites me about the Cloud Network Developer role at Fidelity is the opportunity to work specifically on network automation in financial services. I've been deepening my expertise in AWS VPC architecture, security group automation, and policy-driven infrastructure. I'm particularly interested in building self-service capabilities that empower development teams while maintaining strict compliance controls - which aligns perfectly with what your team does."

---

### Q2: Why do you want to join Fidelity? Why this role?

**Answer**:
"Three main reasons:

**1. Technical Challenge**: This role is uniquely focused on network automation in the cloud, which is more specialized than general DevOps. I'm drawn to solving complex networking problems at scale - things like automated security group management, hybrid cloud connectivity, and policy-driven network provisioning.

**2. Financial Services Domain**: I want to work in an industry where reliability and security aren't just nice-to-haves - they're mandatory. The regulatory requirements in financial services force you to build better systems. That appeals to me.

**3. Team & Culture**: From my research and conversations, Fidelity's FAE organization is building cloud infrastructure the right way - infrastructure-as-code, GitOps, self-service platforms. The 'no manual console access' philosophy resonates with me. That's how modern cloud infrastructure should be managed.

Also, I'm excited about the mentorship aspect. I enjoy helping other engineers understand complex concepts and level up their cloud skills."

---

### Q3: What's your experience with Python?

**Answer (Be Honest, Then Demonstrate Learning)**:
"I have [X years] of Python experience, primarily focused on cloud automation and API development. Here's what I've built:

**1. Cloud Automation**: I've used Boto3 extensively to automate AWS operations - VPC provisioning, security group management, EC2 lifecycle. For example, I built a script that automatically right-sizes EC2 instances based on CloudWatch metrics, which saved 30% on compute costs.

**2. API Development**: I've built REST APIs using Flask and FastAPI for internal tools. For instance, I created a self-service API for developers to request and provision cloud resources, which integrated with our Terraform backend.

**3. DevOps Tooling**: I've written Python scripts for CI/CD pipelines - infrastructure validation, policy checks, deployment automation.

I follow Python best practices - type hints, error handling, logging, unit tests with pytest. I can discuss OOP principles and design patterns if you'd like specifics.

Currently, I'm deepening my Python skills by [specific example - e.g., 'rebuilding some of my Go microservices in Python to compare patterns' or 'contributing to open-source Python projects']."

---

### Q4: Describe your AWS/Azure experience.

**Answer**:
"I have hands-on experience with both AWS and Azure, with stronger depth in AWS:

**AWS** ([X years]):
- **Compute**: EC2, Lambda, ECS/EKS for container workloads
- **Networking**: VPC design, security groups, NACLs, Transit Gateway, VPC Peering
- **Storage**: S3, EBS, EFS
- **Databases**: RDS (PostgreSQL, MySQL), DynamoDB
- **DevOps**: CloudFormation, Systems Manager, CodePipeline
- **Monitoring**: CloudWatch, X-Ray, VPC Flow Logs

**Recent AWS Project**: [Specific example - e.g., 'Designed a multi-tier VPC architecture with public/private subnets across 3 AZs, implemented NAT Gateways for high availability, and automated the entire setup with Terraform']

**Azure** ([X years/months]):
- **Networking**: VNets, NSGs, VNet Peering, Service Endpoints
- **Compute**: VMs, Container Instances, AKS
- **Storage**: Blob Storage, Azure Files
- **DevOps**: Azure DevOps, ARM templates

I'm comfortable working in hybrid cloud environments and understand the networking considerations for connecting AWS and Azure to on-prem infrastructure."

---

### Q5: What's your experience with Terraform?

**Answer**:
"I've been using Terraform for [X years] as my primary IaC tool. Here's my experience:

**1. Module Development**: I've built reusable Terraform modules for common patterns - VPC provisioning, security groups, EKS clusters. I follow module best practices - input validation, comprehensive outputs, good documentation.

**2. State Management**: I've set up remote state backends with S3 + DynamoDB for locking, organized state files by environment and component, used workspaces for multi-environment deployments.

**3. CI/CD Integration**: I've integrated Terraform into CI/CD pipelines with automated validation, security scanning (tfsec, checkov), policy checks (Sentinel, OPA), and automated planning with manual approval for applies.

**4. Production Management**: I've managed production infrastructure with Terraform - performed upgrades, handled state migrations, recovered from state issues, implemented drift detection.

**Example**: [Specific achievement - e.g., 'I refactored monolithic Terraform code into modular, reusable components which reduced deployment time from 45 minutes to 10 minutes and made it possible for other teams to self-service their infrastructure']

I'm also familiar with CloudFormation and can compare the two if needed."

---

## ðŸ’» Technical Coding Interview

### Python Coding Challenges

#### Challenge 1: CIDR Subnet Calculator

**Question**: "Write a Python function that takes a VPC CIDR and subnet count, and returns evenly distributed subnet CIDRs."

```python
def calculate_subnets(vpc_cidr: str, num_subnets: int) -> list[str]:
    """
    Calculate subnet CIDRs from a VPC CIDR.
    
    Args:
        vpc_cidr: VPC CIDR block (e.g., "10.0.0.0/16")
        num_subnets: Number of subnets to create
    
    Returns:
        List of subnet CIDR blocks
    
    Example:
        >>> calculate_subnets("10.0.0.0/16", 4)
        ['10.0.0.0/18', '10.0.64.0/18', '10.0.128.0/18', '10.0.192.0/18']
    """
    import ipaddress
    import math
    
    # Parse VPC network
    vpc_network = ipaddress.ip_network(vpc_cidr)
    
    # Calculate required prefix length
    # If VPC is /16 and we need 4 subnets, we need /18 (4 = 2^2, so add 2 bits)
    additional_bits = math.ceil(math.log2(num_subnets))
    subnet_prefix_len = vpc_network.prefixlen + additional_bits
    
    # Validate
    if subnet_prefix_len > 28:  # AWS limit for subnets
        raise ValueError(f"Cannot create {num_subnets} subnets from {vpc_cidr}")
    
    # Generate subnets
    subnets = list(vpc_network.subnets(new_prefix=subnet_prefix_len))
    
    # Return requested number
    return [str(subnet) for subnet in subnets[:num_subnets]]


# Test cases
def test_calculate_subnets():
    # Test 1: /16 VPC with 4 subnets
    result = calculate_subnets("10.0.0.0/16", 4)
    assert len(result) == 4
    assert result[0] == "10.0.0.0/18"
    
    # Test 2: /24 VPC with 2 subnets
    result = calculate_subnets("192.168.1.0/24", 2)
    assert len(result) == 2
    assert result[0] == "192.168.1.0/25"
    assert result[1] == "192.168.1.128/25"
    
    # Test 3: Edge case - too many subnets
    try:
        calculate_subnets("10.0.0.0/24", 256)
        assert False, "Should have raised ValueError"
    except ValueError:
        pass
    
    print("All tests passed!")

if __name__ == "__main__":
    test_calculate_subnets()
    
    # Demo
    print("\nExample:")
    subnets = calculate_subnets("10.0.0.0/16", 6)
    for i, subnet in enumerate(subnets, 1):
        print(f"Subnet {i}: {subnet}")
```

**Follow-up Questions**:
- "How would you modify this to support IPv6?"
- "How would you distribute subnets across availability zones?"
- "How would you reserve some subnets for future use?"

---

#### Challenge 2: Security Group Rule Validator

**Question**: "Write a Python class that validates security group rules follow best practices."

```python
from typing import List, Dict, Optional
from dataclasses import dataclass
import ipaddress

@dataclass
class SecurityGroupRule:
    protocol: str  # tcp, udp, icmp, -1 (all)
    from_port: int
    to_port: int
    source_cidr: str
    description: Optional[str] = None

class SecurityGroupValidator:
    """Validate security group rules against best practices."""
    
    # Best practice checks
    DANGEROUS_CIDRS = ["0.0.0.0/0", "::/0"]
    DANGEROUS_PORTS = [22, 3389, 5432, 3306, 27017, 6379, 9200]  # SSH, RDP, DBs
    REQUIRED_DESCRIPTION = True
    
    def __init__(self):
        self.violations = []
    
    def validate(self, rules: List[SecurityGroupRule]) -> tuple[bool, List[str]]:
        """
        Validate security group rules.
        
        Returns:
            (is_valid, list_of_violations)
        """
        self.violations = []
        
        for i, rule in enumerate(rules):
            # Check 1: Description exists
            if self.REQUIRED_DESCRIPTION and not rule.description:
                self.violations.append(
                    f"Rule {i+1}: Missing description"
                )
            
            # Check 2: No wide-open dangerous ports
            if rule.source_cidr in self.DANGEROUS_CIDRS:
                if rule.from_port in self.DANGEROUS_PORTS or rule.to_port in self.DANGEROUS_PORTS:
                    self.violations.append(
                        f"Rule {i+1}: Port {rule.from_port}-{rule.to_port} "
                        f"should not be open to {rule.source_cidr}"
                    )
                elif rule.protocol == "-1":  # All traffic
                    self.violations.append(
                        f"Rule {i+1}: All traffic should not be open to 0.0.0.0/0"
                    )
            
            # Check 3: Valid CIDR
            try:
                ipaddress.ip_network(rule.source_cidr)
            except ValueError:
                self.violations.append(
                    f"Rule {i+1}: Invalid CIDR {rule.source_cidr}"
                )
            
            # Check 4: Port range validity
            if not (0 <= rule.from_port <= 65535 and 0 <= rule.to_port <= 65535):
                self.violations.append(
                    f"Rule {i+1}: Invalid port range {rule.from_port}-{rule.to_port}"
                )
            
            if rule.from_port > rule.to_port:
                self.violations.append(
                    f"Rule {i+1}: from_port ({rule.from_port}) > to_port ({rule.to_port})"
                )
        
        return len(self.violations) == 0, self.violations
    
    def suggest_fixes(self, rules: List[SecurityGroupRule]) -> List[str]:
        """Suggest how to fix violations."""
        suggestions = []
        
        for i, rule in enumerate(rules):
            if rule.source_cidr == "0.0.0.0/0" and rule.from_port in self.DANGEROUS_PORTS:
                suggestions.append(
                    f"Rule {i+1}: Consider using a more specific CIDR range "
                    f"or use AWS Systems Manager Session Manager instead of SSH"
                )
        
        return suggestions


# Test cases
def test_validator():
    validator = SecurityGroupValidator()
    
    # Test 1: Valid rules
    good_rules = [
        SecurityGroupRule("tcp", 443, 443, "0.0.0.0/0", "Allow HTTPS from internet"),
        SecurityGroupRule("tcp", 8080, 8080, "10.0.1.0/24", "Allow app traffic from web tier"),
    ]
    is_valid, violations = validator.validate(good_rules)
    assert is_valid, f"Expected valid, got violations: {violations}"
    
    # Test 2: SSH open to world (bad)
    bad_rules = [
        SecurityGroupRule("tcp", 22, 22, "0.0.0.0/0", "SSH from anywhere - BAD"),
    ]
    is_valid, violations = validator.validate(bad_rules)
    assert not is_valid
    assert "Port 22" in violations[0]
    
    # Test 3: Missing description
    no_desc_rules = [
        SecurityGroupRule("tcp", 80, 80, "0.0.0.0/0"),
    ]
    is_valid, violations = validator.validate(no_desc_rules)
    assert not is_valid
    assert "Missing description" in violations[0]
    
    # Test 4: Invalid CIDR
    invalid_cidr_rules = [
        SecurityGroupRule("tcp", 80, 80, "256.0.0.0/8", "Invalid CIDR"),
    ]
    is_valid, violations = validator.validate(invalid_cidr_rules)
    assert not is_valid
    assert "Invalid CIDR" in violations[0]
    
    print("All tests passed!")

if __name__ == "__main__":
    test_validator()
    
    # Demo
    print("\n=== Demo ===")
    validator = SecurityGroupValidator()
    
    rules = [
        SecurityGroupRule("tcp", 22, 22, "0.0.0.0/0", "SSH from world"),
        SecurityGroupRule("tcp", 3306, 3306, "0.0.0.0/0", "MySQL from world"),
        SecurityGroupRule("tcp", 443, 443, "0.0.0.0/0", "HTTPS from world"),
    ]
    
    is_valid, violations = validator.validate(rules)
    print(f"Valid: {is_valid}")
    print(f"\nViolations:")
    for v in violations:
        print(f"  - {v}")
    
    print(f"\nSuggestions:")
    for s in validator.suggest_fixes(rules):
        print(f"  - {s}")
```

**Follow-up Questions**:
- "How would you extend this to check for duplicate rules?"
- "How would you integrate this into a CI/CD pipeline?"
- "How would you handle security group references instead of CIDR blocks?"

---

#### Challenge 3: AWS Resource Lister with Boto3

**Question**: "Write a Python script to list all VPCs across all regions with their details."

```python
import boto3
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
import json

class AWSNetworkInventory:
    """Inventory AWS networking resources across all regions."""
    
    def __init__(self, profile_name: str = None):
        self.session = boto3.Session(profile_name=profile_name)
        self.ec2_client = self.session.client('ec2')
    
    def get_all_regions(self) -> List[str]:
        """Get list of all AWS regions."""
        response = self.ec2_client.describe_regions()
        return [region['RegionName'] for region in response['Regions']]
    
    def get_vpcs_in_region(self, region: str) -> List[Dict]:
        """Get all VPCs in a specific region."""
        try:
            ec2 = self.session.client('ec2', region_name=region)
            response = ec2.describe_vpcs()
            
            vpcs = []
            for vpc in response['Vpcs']:
                # Get VPC name from tags
                name = next(
                    (tag['Value'] for tag in vpc.get('Tags', []) if tag['Key'] == 'Name'),
                    'Unnamed'
                )
                
                # Get subnet count
                subnets_response = ec2.describe_subnets(
                    Filters=[{'Name': 'vpc-id', 'Values': [vpc['VpcId']]}]
                )
                subnet_count = len(subnets_response['Subnets'])
                
                # Get IGW attachment
                igw_response = ec2.describe_internet_gateways(
                    Filters=[{'Name': 'attachment.vpc-id', 'Values': [vpc['VpcId']]}]
                )
                has_igw = len(igw_response['InternetGateways']) > 0
                
                vpcs.append({
                    'region': region,
                    'vpc_id': vpc['VpcId'],
                    'name': name,
                    'cidr_block': vpc['CidrBlock'],
                    'is_default': vpc['IsDefault'],
                    'subnet_count': subnet_count,
                    'has_internet_gateway': has_igw,
                    'state': vpc['State']
                })
            
            return vpcs
        except Exception as e:
            print(f"Error in region {region}: {e}")
            return []
    
    def get_all_vpcs(self, max_workers: int = 10) -> List[Dict]:
        """Get all VPCs across all regions (parallel)."""
        regions = self.get_all_regions()
        all_vpcs = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_region = {
                executor.submit(self.get_vpcs_in_region, region): region
                for region in regions
            }
            
            for future in as_completed(future_to_region):
                region = future_to_region[future]
                try:
                    vpcs = future.result()
                    all_vpcs.extend(vpcs)
                    print(f"âœ“ {region}: {len(vpcs)} VPCs")
                except Exception as e:
                    print(f"âœ— {region}: {e}")
        
        return all_vpcs
    
    def generate_report(self, vpcs: List[Dict]) -> None:
        """Generate summary report."""
        print("\n" + "="*80)
        print("AWS VPC Inventory Report")
        print("="*80)
        
        print(f"\nTotal VPCs: {len(vpcs)}")
        
        # Group by region
        by_region = {}
        for vpc in vpcs:
            region = vpc['region']
            if region not in by_region:
                by_region[region] = []
            by_region[region].append(vpc)
        
        print(f"Regions with VPCs: {len(by_region)}")
        
        print("\n" + "-"*80)
        print(f"{'Region':<20} {'VPC ID':<25} {'Name':<20} {'CIDR':<18} {'Subnets':<8}")
        print("-"*80)
        
        for region in sorted(by_region.keys()):
            for vpc in by_region[region]:
                print(f"{region:<20} {vpc['vpc_id']:<25} {vpc['name']:<20} "
                      f"{vpc['cidr_block']:<18} {vpc['subnet_count']:<8}")
        
        # Statistics
        default_vpcs = [v for v in vpcs if v['is_default']]
        custom_vpcs = [v for v in vpcs if not v['is_default']]
        vpcs_with_igw = [v for v in vpcs if v['has_internet_gateway']]
        
        print("\n" + "-"*80)
        print("Statistics:")
        print(f"  Default VPCs: {len(default_vpcs)}")
        print(f"  Custom VPCs: {len(custom_vpcs)}")
        print(f"  VPCs with Internet Gateway: {len(vpcs_with_igw)}")
        
        # Export to JSON
        with open('vpc_inventory.json', 'w') as f:
            json.dump(vpcs, f, indent=2)
        print(f"\nâœ“ Exported to vpc_inventory.json")


# Usage
if __name__ == "__main__":
    print("Starting AWS VPC Inventory...")
    
    inventory = AWSNetworkInventory()
    vpcs = inventory.get_all_vpcs()
    inventory.generate_report(vpcs)
```

**Follow-up Questions**:
- "How would you add pagination for regions with many VPCs?"
- "How would you cache results to avoid repeated API calls?"
- "How would you extend this to include security groups and NACLs?"

---

## ðŸ—ï¸ System Design Interview

### Design Question 1: Self-Service Network Provisioning Platform

**Question**: "Design a self-service platform where developers can request and provision cloud networks (VPCs, subnets, security groups) through an API, while maintaining security and compliance controls."

**Expected Discussion Points**:

**1. Requirements Clarification** (5 min):
```
Functional Requirements:
- Create VPC with custom CIDR
- Create subnets (public/private)
- Create security groups with rules
- Support both AWS and Azure
- Audit logging
- Cost estimation

Non-Functional Requirements:
- Latency: < 30s for VPC creation
- Availability: 99.9%
- Compliance: SOX, PCI-DSS
- Security: Role-based access, approval workflows
- Scale: 1000+ VPCs, 100 requests/day
```

**2. High-Level Architecture** (10 min):
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Gateway   â”‚
â”‚   (Auth + Rate  â”‚
â”‚     Limiting)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Flask/FastAPI  â”‚â”€â”€â”€â”€â”€â–ºâ”‚  PostgreSQL  â”‚
â”‚     Backend     â”‚      â”‚  (Metadata)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â–ºâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚       â”‚   Redis      â”‚
         â”‚       â”‚  (Cache +    â”‚
         â”‚       â”‚   Queue)     â”‚
         â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â–ºâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚       â”‚  Terraform   â”‚
         â”‚       â”‚   Runner     â”‚
         â”‚       â”‚  (Container) â”‚
         â”‚       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚              â”‚
         â”‚              â–¼
         â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚       â”‚   AWS/Azure  â”‚
         â”‚       â”‚   Resources  â”‚
         â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â–ºâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚       â”‚   Policy     â”‚
         â”‚       â”‚   Engine     â”‚
         â”‚       â”‚    (OPA)     â”‚
         â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â””â”€â”€â”€â”€â”€â”€â–ºâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  Audit Log   â”‚
                 â”‚ (CloudWatch/ â”‚
                 â”‚  Log Analytics)
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**3. API Design** (5 min):
```python
# POST /api/v1/vpc
{
  "name": "app-prod-vpc",
  "cidr": "10.0.0.0/16",
  "cloud_provider": "aws",
  "region": "us-east-1",
  "subnets": [
    {"name": "web-public", "cidr": "10.0.1.0/24", "type": "public", "az": "us-east-1a"},
    {"name": "app-private", "cidr": "10.0.2.0/24", "type": "private", "az": "us-east-1a"},
    {"name": "db-private", "cidr": "10.0.3.0/24", "type": "private", "az": "us-east-1b"}
  ],
  "tags": {
    "environment": "production",
    "cost_center": "engineering"
  }
}

# Response
{
  "request_id": "req-12345",
  "status": "pending_approval",
  "estimated_cost": "$50/month",
  "approval_url": "/approvals/req-12345"
}

# GET /api/v1/vpc/{vpc_id}
{
  "vpc_id": "vpc-abc123",
  "name": "app-prod-vpc",
  "cidr": "10.0.0.0/16",
  "status": "active",
  "created_at": "2024-01-15T10:30:00Z",
  "created_by": "user@company.com",
  "subnets": [...],
  "security_groups": [...]
}
```

**4. Data Model** (5 min):
```sql
-- Requests table
CREATE TABLE network_requests (
    id UUID PRIMARY KEY,
    user_id VARCHAR(255),
    request_type VARCHAR(50), -- vpc, subnet, security_group
    cloud_provider VARCHAR(20), -- aws, azure
    region VARCHAR(50),
    configuration JSONB,
    status VARCHAR(50), -- pending, approved, provisioning, active, failed
    estimated_cost DECIMAL(10,2),
    created_at TIMESTAMP,
    approved_at TIMESTAMP,
    approved_by VARCHAR(255)
);

-- Resources table
CREATE TABLE provisioned_resources (
    id UUID PRIMARY KEY,
    request_id UUID REFERENCES network_requests(id),
    resource_type VARCHAR(50),
    resource_id VARCHAR(255), -- vpc-xxx, subnet-xxx
    cloud_resource_id VARCHAR(255),
    configuration JSONB,
    terraform_state_path VARCHAR(500),
    status VARCHAR(50),
    created_at TIMESTAMP
);

-- Audit log
CREATE TABLE audit_log (
    id BIGSERIAL PRIMARY KEY,
    timestamp TIMESTAMP,
    user_id VARCHAR(255),
    action VARCHAR(100),
    resource_type VARCHAR(50),
    resource_id VARCHAR(255),
    details JSONB,
    ip_address INET
);
```

**5. Deep Dive - Policy Engine** (10 min):
```python
# Policy checks with Open Policy Agent (OPA)

# policy.rego
package network.vpc

# Default deny
default allow = false

# Allow VPC creation if:
# 1. CIDR is within allowed ranges
# 2. Region is approved
# 3. User has permission
# 4. Cost is under budget

allow {
    input.action == "create_vpc"
    cidr_allowed
    region_allowed
    user_authorized
    under_budget
}

cidr_allowed {
    input.cidr_block == "10.0.0.0/8"
    # More specific checks...
}

region_allowed {
    input.region == data.approved_regions[_]
}

user_authorized {
    input.user.role == "network_admin"
}

under_budget {
    input.estimated_cost < data.team_budgets[input.team].monthly_limit
}

# Deny dangerous configurations
deny["SSH (port 22) cannot be open to 0.0.0.0/0"] {
    some rule in input.security_group_rules
    rule.port == 22
    rule.source_cidr == "0.0.0.0/0"
}
```

**6. Deep Dive - Terraform Execution** (10 min):
```python
# terraform_runner.py

class TerraformRunner:
    def __init__(self, request_id):
        self.request_id = request_id
        self.workspace_dir = f"/tmp/terraform/{request_id}"
    
    async def provision_vpc(self, config):
        # 1. Generate Terraform code
        tf_code = self.generate_terraform(config)
        
        # 2. Write to workspace
        self.write_terraform_files(tf_code)
        
        # 3. Initialize
        await self.run_command("terraform init")
        
        # 4. Validate
        await self.run_command("terraform validate")
        
        # 5. Plan
        plan_output = await self.run_command("terraform plan -out=tfplan")
        
        # 6. Security scan
        await self.run_command("tfsec .")
        await self.run_command("checkov -d .")
        
        # 7. Policy check
        policy_result = await self.check_policy(plan_output)
        if not policy_result.allowed:
            raise PolicyViolation(policy_result.violations)
        
        # 8. Apply
        await self.run_command("terraform apply tfplan")
        
        # 9. Save state to S3
        await self.save_state()
        
        # 10. Extract outputs
        outputs = await self.get_outputs()
        
        return outputs
    
    def generate_terraform(self, config):
        return f"""
terraform {{
  backend "s3" {{
    bucket = "terraform-state"
    key    = "networks/{self.request_id}/terraform.tfstate"
    region = "us-east-1"
  }}
}}

module "vpc" {{
  source = "git::https://github.com/company/terraform-modules.git//vpc"
  
  name       = "{config['name']}"
  cidr_block = "{config['cidr']}"
  region     = "{config['region']}"
  
  subnets = {json.dumps(config['subnets'])}
  
  tags = {json.dumps(config['tags'])}
}}
"""
```

**7. Scaling Considerations** (5 min):
```
Bottlenecks:
1. Terraform execution time (30-60s per VPC)
   â†’ Solution: Queue-based async processing

2. Concurrent Terraform runs
   â†’ Solution: Worker pool with rate limiting

3. State locking contention
   â†’ Solution: Separate state files per resource

4. API rate limits (AWS/Azure)
   â†’ Solution: Exponential backoff, circuit breaker

Scaling:
- Horizontal: Multiple API servers behind load balancer
- Workers: Multiple Terraform runner containers
- Database: Read replicas for queries
- Caching: Redis for frequently accessed data
```

**8. Monitoring & Observability** (5 min):
```python
# Metrics to track
- Request rate (requests/sec)
- Approval latency (time to approval)
- Provisioning latency (time to provision)
- Success rate (successful provisions / total)
- Error rate by type
- Cost per resource
- Resource utilization

# Alerts
- High error rate (>5%)
- Slow provisioning (>5 minutes)
- Policy violations
- Budget exceeded
- Failed Terraform applies

# Logging
- All API requests
- Policy decisions
- Terraform outputs
- State changes
- User actions
```

**Key Points to Emphasize**:
âœ… **Security**: Authentication, authorization, policy validation  
âœ… **Compliance**: Audit logging, approval workflows  
âœ… **Reliability**: Idempotent operations, state management  
âœ… **Scalability**: Async processing, queue-based architecture  
âœ… **Observability**: Metrics, logs, alerts

---

### Design Question 2: Multi-Region Network Architecture

**Question**: "Design a multi-region network architecture for a global application with requirements for disaster recovery, low latency, and data residency compliance."

**Expected Architecture**:
```
Primary Region (us-east-1):
- VPC: 10.1.0.0/16
- Public subnets: 10.1.1.0/24, 10.1.2.0/24
- Private subnets: 10.1.11.0/24, 10.1.12.0/24
- Database subnets: 10.1.21.0/24, 10.1.22.0/24

Secondary Region (us-west-2):
- VPC: 10.2.0.0/16
- Same subnet structure

Europe Region (eu-west-1):
- VPC: 10.3.0.0/16
- Data residency compliance (GDPR)

Connectivity:
- Transit Gateway in each region
- Inter-region VPC peering or Transit Gateway peering
- VPN to on-prem (with redundancy)
- Direct Connect for low-latency, high-bandwidth

DNS:
- Route53 with health checks
- Latency-based routing
- Failover routing for DR

Data Replication:
- RDS cross-region read replicas
- S3 cross-region replication
- DynamoDB global tables
```

---

## ðŸŽ¯ Behavioral Interview Questions

### STAR Method Template

**Situation**: Set the context (2-3 sentences)  
**Task**: What was your responsibility?  
**Action**: What steps did you take?  
**Result**: What was the outcome? (Quantify if possible)

---

### Q1: Tell me about a time you had to troubleshoot a complex networking issue in production.

**Example Answer**:

**Situation**: "In my previous role, we had a critical production issue where our application couldn't connect to the RDS database. This was affecting 50,000+ users and the outage had been ongoing for 20 minutes when I was paged."

**Task**: "As the on-call engineer, I needed to quickly identify the root cause and restore service within our 30-minute SLA."

**Action**: 
"I used a systematic troubleshooting approach:

1. **First 5 minutes**: I checked the obvious - RDS instance was running, application was running, no recent deployments. I enabled VPC Flow Logs to see traffic patterns.

2. **Next 5 minutes**: I analyzed the Flow Logs and saw REJECT records for traffic from the application subnet to the RDS subnet on port 3306. This pointed to network-level blocking.

3. **Next 5 minutes**: I checked security groups - they were correct. I then checked Network ACLs and found the issue: Someone had added a DENY rule that was blocking the database port. This was rule #50, and there was an ALLOW rule at #100, but NACLs process rules in order, so the DENY took precedence.

4. **Fix**: I removed the incorrect DENY rule and traffic immediately resumed."

**Result**: 
"Total resolution time was 25 minutes, just under our SLA. I then:
- Wrote a post-mortem documenting the incident
- Created automated tests to validate NACL rules in our Terraform pipeline
- Added monitoring to alert on NACL changes
- Conducted a team training on the difference between security groups (stateful) and NACLs (stateless)

Since then, we haven't had a similar incident, and our deployment process now catches these issues before they reach production."

---

### Q2: Describe a time when you had to learn a new technology quickly.

**Example Answer**:

**Situation**: "Six months ago, our team needed to migrate our infrastructure from manual AWS console management to infrastructure-as-code using Terraform. I had never used Terraform before, but I was chosen to lead the migration."

**Task**: "I needed to become proficient in Terraform within two weeks and create a migration plan for 200+ resources."

**Action**:
"I created a structured learning plan:

**Week 1 - Fundamentals**:
- Completed HashiCorp's official Terraform tutorials (8 hours)
- Read 'Terraform: Up & Running' book
- Built a personal project: Automated VPC setup with Terraform
- Joined Terraform community Slack

**Week 2 - Practical Application**:
- Analyzed our existing AWS infrastructure
- Created Terraform modules for common patterns (VPC, security groups, EC2)
- Imported existing resources into Terraform state (`terraform import`)
- Set up remote state backend with S3 + DynamoDB
- Created CI/CD pipeline for Terraform deployments

I also:
- Paired with a senior engineer who had Terraform experience
- Reviewed open-source Terraform modules to learn best practices
- Documented everything I learned in internal wiki"

**Result**:
"After two weeks, I successfully:
- Migrated 60% of our infrastructure to Terraform (starting with non-critical resources)
- Created reusable modules that reduced deployment time from hours to minutes
- Trained 5 other team members on Terraform
- Established best practices and coding standards

The migration project was completed in 8 weeks with zero downtime. Our infrastructure is now fully version-controlled, and new environments can be provisioned in under 10 minutes vs 2 days manually."

---

### Q3: Tell me about a time you disagreed with a technical decision.

**Example Answer**:

**Situation**: "In my previous role, our team was planning to implement a microservices architecture, and the lead architect proposed putting all services in a single VPC with shared security groups to 'simplify networking'."

**Task**: "I believed this approach would create security risks and operational challenges, but I needed to present my concerns constructively without undermining the architect."

**Action**:
"I took a collaborative approach:

1. **Research**: I documented the potential issues:
   - Blast radius: A security breach in one service could affect all services
   - Compliance: We couldn't isolate PCI-DSS workloads from non-sensitive workloads
   - Operational complexity: Security group rules would become unmanageable with 50+ services

2. **Propose Alternatives**: I created a proof-of-concept for a better design:
   - Separate VPCs per environment (dev, staging, prod)
   - Transit Gateway for inter-VPC communication
   - Service-specific security groups with least-privilege access
   - Network segmentation by trust level

3. **Present Data**: I scheduled a meeting with the architect and presented:
   - Security risks with examples from industry incidents
   - Compliance requirements we needed to meet
   - Operational benefits of network segmentation
   - Cost analysis (minimal additional cost)
   - Migration complexity (manageable with Terraform)

4. **Collaboration**: Instead of just criticizing, I offered to:
   - Build the POC and demonstrate it works
   - Create Terraform modules for the new design
   - Write migration documentation"

**Result**:
"The architect appreciated the thorough analysis and agreed to try my approach for one environment. After seeing it work well, we adopted it company-wide. 

The segmented architecture:
- Passed our SOC 2 audit with zero findings (vs 12 findings in previous audit)
- Reduced security incident blast radius (one compromised service didn't affect others)
- Made troubleshooting easier (clear network boundaries)
- Enabled us to meet PCI-DSS requirements for payment processing

The key lesson I learned was: Data + collaboration beats opinion. I could have just complained, but instead I invested time to build a better solution and demonstrate its value."

---

### Q4: Describe a time when you had to make a difficult decision with incomplete information.

**Example Answer**:

**Situation**: "During a major product launch, our monitoring showed increasing latency in our API, but we couldn't pinpoint the exact cause. The launch was scheduled to go live in 2 hours, and executive leadership was watching."

**Task**: "As the lead infrastructure engineer, I needed to decide: proceed with the launch, delay it (disappointing stakeholders), or implement a potential fix that might not work."

**Action**:
"I gathered the information I had:
- Latency increased from 100ms to 800ms in the past hour
- Database CPU was at 60% (not the bottleneck)
- Application logs showed no errors
- Network monitoring showed packet loss between app and database subnets
- Recent change: A teammate had modified NACL rules that morning

I made a decision framework:
1. If I delay: Lost revenue, disappointed stakeholders, but safe
2. If I proceed: Potential user impact, bad launch experience
3. If I implement fix: 70% confidence it would work based on symptoms

Decision: I chose option 3 with a rollback plan.

I:
- Reviewed the NACL changes (found missing return traffic rules for ephemeral ports)
- Fixed the NACL rules
- Tested in staging environment (latency normalized)
- Deployed to production with 30-minute monitoring window
- Had rollback plan ready (revert NACL changes)

I communicated to leadership:
- 'We found the likely cause and deployed a fix'
- 'Monitoring for 30 minutes before launch'
- 'If latency doesn't improve, we'll delay launch'"

**Result**:
"The fix worked. Latency returned to normal (95ms avg) within 10 minutes. We proceeded with the launch on time, and it was successful with zero user-facing issues.

After the incident:
- I implemented automated NACL validation in our Terraform pipeline
- Added monitoring for network-level metrics (packet loss, retransmissions)
- Created runbook for similar incidents
- Instituted a change freeze 24 hours before major launches

The key lesson: With incomplete information, structure your decision-making, prepare rollback plans, and communicate clearly with stakeholders."

---

### Q5: Tell me about a time you mentored or helped a junior engineer.

**Example Answer**:

**Situation**: "A junior engineer joined our team with strong coding skills but limited cloud experience. She was struggling to understand VPC architecture and felt overwhelmed."

**Task**: "I wanted to help her become productive quickly while building her confidence in cloud networking concepts."

**Action**:
"I created a structured mentoring program:

**Week 1 - Fundamentals**:
- Pair programming sessions: We built a VPC together from scratch using Terraform
- I explained each component (subnets, route tables, IGW, NAT Gateway) and why it exists
- Hands-on lab: She created her own VPC following a design document I provided

**Week 2 - Real-world application**:
- Code review: I reviewed her VPC implementation and explained best practices
- Troubleshooting practice: I intentionally broke a VPC configuration and had her debug it
- Architecture discussion: We reviewed our production VPC architecture and I explained design decisions

**Week 3 - Independence**:
- Gave her a small project: Create a VPC module for development environments
- I was available for questions but didn't hand-hold
- She presented her module to the team

**Ongoing**:
- Weekly 1:1s to discuss challenges and answer questions
- Shared resources: Articles, tutorials, AWS documentation
- Encouraged her to get AWS certification (I shared my study materials)"

**Result**:
"Within a month:
- She independently designed and implemented a VPC for a new project
- Her Terraform module became our standard template for dev environments
- She passed AWS Solutions Architect Associate certification
- She started mentoring another new hire using the same approach

Six months later, she's one of our strongest cloud engineers and has contributed significantly to our infrastructure automation efforts.

The key lesson: Effective mentoring combines hands-on practice, gradual independence, and ongoing support."

---

## ðŸ“š Questions to Ask Interviewer

### Technical/Role Questions:
1. "What does a typical day look like for this role? How much time is spent on development vs operations vs meetings?"

2. "What's the current state of your infrastructure-as-code adoption? What percentage of resources are managed with Terraform?"

3. "How do you handle network security in a self-service model? What guardrails are in place?"

4. "Can you describe your CI/CD pipeline for infrastructure changes? What does the approval process look like?"

5. "What's the biggest networking challenge the team is currently facing?"

6. "How do you balance self-service capabilities with security and compliance requirements?"

### Team/Culture Questions:
7. "How is the team structured? How many cloud network engineers are there?"

8. "What's the on-call rotation like? How often and what's the typical volume?"

9. "How does the team stay current with new AWS/Azure features and best practices?"

10. "What opportunities are there for learning and professional development?"

### Business/Impact Questions:
11. "What are the team's goals for the next 6-12 months?"

12. "How does this role contribute to Fidelity's cloud strategy?"

13. "What metrics does the team use to measure success?"

### Growth Questions:
14. "What does career progression look like for this role?"

15. "Are there opportunities to work on special projects or contribute to other teams?"

---

## ðŸŽ¯ Final Preparation Checklist

### Technical Skills:
- [ ] Can explain VPC architecture from memory
- [ ] Can write Terraform modules without docs
- [ ] Can debug network connectivity issues systematically
- [ ] Can design secure, scalable network architecture
- [ ] Familiar with Jenkins pipeline syntax
- [ ] Comfortable with Python Boto3

### Interview Preparation:
- [ ] Have 10+ STAR stories prepared
- [ ] Practiced system design questions (3-5 times)
- [ ] Reviewed your resume and can explain every point
- [ ] Researched Fidelity's tech stack and challenges
- [ ] Prepared questions for each interview round
- [ ] Know your salary expectations

### Portfolio:
- [ ] GitHub repo with Terraform modules
- [ ] Working demo of network provisioning API
- [ ] Blog post or documentation of a complex problem solved
- [ ] AWS/Azure certifications (if time permits)

---

**You're ready! Remember: Confidence comes from preparation. You've got this! ðŸš€**
